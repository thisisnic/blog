[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Recurse Center Return Statement\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nMonitoring R Package Updates with Github Actions\n\n\n\n\n\n\nR\n\n\nGitHub Actions\n\n\n\n\n\n\n\n\n\nDec 28, 2023\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nExplaining the C bits at the start of ‘Deep R Programming Ch14: Interfacing compiled code’\n\n\n\n\n\n\nR\n\n\nC\n\n\n\n\n\n\n\n\n\nDec 23, 2023\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nReflections on 2023\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nDebugging\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nThree Goats in a Raincoat\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nR package documentation - what makes a good example?\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\nNic Crane\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/debugging/index.html",
    "href": "posts/debugging/index.html",
    "title": "Debugging",
    "section": "",
    "text": "As a package maintainer, I’m constantly disappointed when folks mention Arrow bugs they’re aware of but haven’t reported. Not disappointed with the individual in question, but disappointed with the fact that we’re not at the point where we’ve created an environment in which folks are happy to just report bugs immediately. This is not an Arrow-specific problem, and I find myself behaving in exactly the same way with other open source projects. If I’m not entirely sure something is a bug, I’m not going to risk looking foolish publicly by complaining, but the irony of this is that I don’t judge users who make those mistakes with Arrow, as usually it means we need to improve our docs to be more clear, and that in itself is valuable feedback.\nI really love interacting with people who use the package, as without that interaction, development and maintenance can feel like shouting into the void. I like being able to solve problems that make other people’s lives easier, and I thrive off that social energy. I implemented bindings for dplyr::across() because someone commented on Twitter that they’d love to see it as a feature. Last night I got home from a friend’s birthday drinks, saw a user question on Stack Overflow which had an easy win, and within an hour had a pull request up which implemented a new function and fixed the particular issue. I am not promising this level of responsiveness in perpetuity, but I’m still at the point where I find this kind of thing exciting and energising.\nOne particular bug which has haunted me for the past 6 months, is a particularly irritating one whereby when working with a partitioned CSV dataset in arrow (I’m using lowercase to denote the R package, rather than Arrow the overall project), if the user manually specified a schema, the variable used for partitioning did not appear in the resulting dataset. This is a huge problem IMO - while we can sit here all day talking about the virtues of Parquet, in reality, a lot of our users will be using CSVs, and it’s issues like these that can rule out being able to use arrow at all in some cases.\nWhen I opened the original issue based on another user issue, I knew it was important, but felt a bit stuck. It wasn’t immediately obvious to anyone what the source of the error was. I’d assumed it must be a C++ error and flagged it as such, but nobody had taken a look at it, and I’m always hesitant to mindlessly tag folks on issues when I don’t feel like I’ve done the work to investigate (though to be fair, didn’t really know what “the work” should be in this case).\nI’d ended up assuming that this bit of functionality just didn’t work with CSV datasets, and had been working around it, until I was presenting about arrow at New York Open Statistical Programming Meetup, and someone asked about it again. I take 1 user question as representative of 99 other people with the same issue who aren’t being so vocal about it, and felt like it needed to be fixed. I am unashamed to admit that I occasionally have the taste for a bit of melodrama, and publicly declared it to a few of my fellow contributors as “my white whale”, and so set out to find the source, even if it required me to delve deep into the guts of Arrow’s C++ libraries, a task which can often send me down endless rabbit holes and chasing red herrings (this sentence has become quite the nature park…)\nMy original exploration didn’t result in much useful - the arrow package does some cool things with R6 objects to binding them to Arrow C++ objects, but accessing the inner properties of these bound objects would mean manually creating print methods for every single one of them, and when you don’t know in which class the problem lies, this becomes, frankly, a massive pain in the arse. I still didn’t have enough to go on to take it to an Arrow C++ contributor and ask for their help, but showing I’d done some of “the work” to at least make an effort myself.\nAnd then collaborative debugging saved me! I had a catchup with the fantastic Dane Pitkin, and I asked for his help just walking through the problem. Dane’s main contributions to Arrow have been to Python, though he has a ton of previous C++ experience, even if he isn’t a regular contributor to the Arrow C++ library. I walked through the problem with him, and the steps I’d taken so far to try to figure things out, and the fact that I still needed to figure out if the problem was in R or C++. Dane commented that the object bindings we’d been looking at had little surface area for the problem to be in R - most of them were straight-up mappings from the C++ object to an R6 object with no extension. This was my first big clue! I remembered that there’s a bit of open_dataset() where we do some manual reconfiguration of specified options, which involves a whole load of R code - something I’ll come back to later. Dane also suggested I check out Stack Overflow to see if people were complaining about the issue in C++ there. I was sceptical that I’d find anything - lots of these bugs are more often surfaced in the R and Python libraries - but realised that this wasn’t the dead end that I’d thought. It suddenly occurred to me that if I could reproduce the bug in PyArrow, then the problem must lie in the C++ library, but if I couldn’t, then the problem lay in the R code.\nFifteen minutes later, and I had confirmed it was an R problem. I happened to mention on Slack the problem I was having, steps I’d taken so far to investigate, and potential ideas to look at next, and ended up engaging in a bit more collaborative debugging, this time with the wonderful Dewey Dunnington, who mentioned more disparities between PyArrow and R in terms of how we construct schemas, which put me on the path of testing the schema values at different points in Dataset creation and able to rule that out. At that point, with a smaller problem space to explore, the only logical thing left to look into was the R code which sets up the options for the various Arrow classes, and I ended up spotting the rogue instantiation of CSVReadOptions which just needed to have the partitioning column excluded (it relates to the reading in of the individual files which make up the dataset, and so has no “knowledge” of the partitions, and so previously raised an error as it treated them as an extraneous column).\nOne pull request later, and the problem that I’d given myself a week to look at had been solved in less than a day! This is probably one of the most gratifying bugs I’d worked on all year; there was a user with a problem to solve, a bug which had been annoying me for ages, the chance to fall into the puzzle-like aspects of debugging, and some great opportunities for collaboration with folks whose help here I really appreciated. This is one of the things I enjoy most about being a software engineer; this process of starting off feeling entirely clueless about something, and having to work out where I need to be and how I’m going to get there, and then doing it. Actually, in the abstract, that’s probably one of the things I enjoy most about being a human :)"
  },
  {
    "objectID": "posts/2023wrapup/index.html",
    "href": "posts/2023wrapup/index.html",
    "title": "Reflections on 2023",
    "section": "",
    "text": "Here’s a summary of my highlights of 2023!"
  },
  {
    "objectID": "posts/2023wrapup/index.html#arrow-positconf-workshop",
    "href": "posts/2023wrapup/index.html#arrow-positconf-workshop",
    "title": "Reflections on 2023",
    "section": "Arrow posit::conf workshop",
    "text": "Arrow posit::conf workshop\nThe biggest professional achievement for me was having the privilege of being invited to co-present a workshop with Steph Hazlitt about Arrow at posit::conf. Steph is fantastic to work with, and while writing and delivering the workshop was hard work, it paid off in the end - we received really positive feedback from attendees. The experience reignited my passion for teaching, and desire to develop those skills. I’ve applied for Carpentries instructor training, which I’m still waiting to hear back about, and I’m excited to also be working with Steph on creating a 2 hour version of the workshop, which we’re planning on teaching at user events in 2024.\n\n\n\nHanging out with some awesome people at posit::conf 2023!"
  },
  {
    "objectID": "posts/2023wrapup/index.html#arrow-r-maintenance-bot",
    "href": "posts/2023wrapup/index.html#arrow-r-maintenance-bot",
    "title": "Reflections on 2023",
    "section": "Arrow R Maintenance bot",
    "text": "Arrow R Maintenance bot\nAnother project I enjoyed working on was creating the Arrow maintenance bot. It pulls data from the GitHub API, aggregates the relevant bits, and then posts it to a Zulip channel (though also works for Slack). It was an itch I’d wanted to scratch for a long time, as I’d previously got this information by clicking on various bookmarks every so often whenever I remembered. The output is shown in the screenshot below; basically, it summarises new bug tickets which need investigating, investigated bugs which need a fix, and open PRs which may need reviewing. \nThere are more aspects of arrow maintenance which I’d love to automate and pull together in dashboard form - I’ve been discussing this with a friend, and I hope we find the time to work on this in 2024!"
  },
  {
    "objectID": "posts/2023wrapup/index.html#arrow-maintenance-and-book",
    "href": "posts/2023wrapup/index.html#arrow-maintenance-and-book",
    "title": "Reflections on 2023",
    "section": "Arrow maintenance and book",
    "text": "Arrow maintenance and book\nIn January, I became the official package maintainer on CRAN. It took my some time to get my head around this; it’s a daunting responsibility, and in reality I’m part of a wider team, all of whom play an important part.\nThe amount of time I’ve been able to spend on Arrow has dropped significantly since I left Voltron Data, though I’m hoping to be able to spend more time this year on it. I feel like my focus will shift a little, away from code, and more towards encouraging new contributors, as well as teaching people how to use the package.\nMost excitingly though, is the book which I am co-authoring! This is taking up the main chunk of my Arrow-related time, but the aim is to have the manuscript submitted to the publishers by summer, if not sooner."
  },
  {
    "objectID": "posts/2023wrapup/index.html#c",
    "href": "posts/2023wrapup/index.html#c",
    "title": "Reflections on 2023",
    "section": "C++",
    "text": "C++\nHaving interacted with bits of C++ code while working on Arrow, I decided to spend time learning C++ more formally. I’ve generally found this tricky - frankly, higher-level languages seem to have more engaging content out there. I made a start on the Udacity C++ Nanodegree, which I completed over half of, but was limited on time and so paused this - I plan to pick it up again next year though. I also tried a more practical approach, and decided to make a PR to the Arrow C++ codebase - one of the PRs I am most proud of this year is this one, adding support for the Substrait cast expression. It was hard, though most of the complexity came from peripheral tasks, like working out how to run the code interactively in the debugger."
  },
  {
    "objectID": "posts/2023wrapup/index.html#joining-the-rladies-global-team",
    "href": "posts/2023wrapup/index.html#joining-the-rladies-global-team",
    "title": "Reflections on 2023",
    "section": "Joining the RLadies Global team",
    "text": "Joining the RLadies Global team\nI spent some time thinking about how I could engage more with the wider R community, and so I volunteered to join the RLadies global team, helping set up new chapters on meetup.com. I’ve not done much here yet, but am delighted to be part of the organisation, and hope to be able to help out more in 2024"
  },
  {
    "objectID": "posts/2023wrapup/index.html#career-break-and-plans-for-2024-recurse-center",
    "href": "posts/2023wrapup/index.html#career-break-and-plans-for-2024-recurse-center",
    "title": "Reflections on 2023",
    "section": "Career break and plans for 2024 (Recurse Center!)",
    "text": "Career break and plans for 2024 (Recurse Center!)\nLast, but absolutely not least, is my decision to take a career break. After leaving Voltron Data, I’d been working part-time in a contractor capacity, alongside working on the Arrow book, but I decided I wanted to narrow my focus and take some time out for me.\nI’m taking a break and spending the next couple of months focusing on writing. Once most of the book is done, I plan to spend more time on my open source work in general, which will involve teaching and talking about Arrow, as well as pushing forward a few side projects.\nThe most exciting news I’ve had this year is that I applied to the Recurse Center a few weeks ago. RC is like a writer’s retreat for programmers, and attendees spend 12 weeks together working on programming problems of their choice. I want to get to grips with R’s internals, and learn about the C API, as well as levelling up my C++ skills in general. I’m delighted to say that I was accepted to attend this morning, and will be joining the spring batch in late March! I’ve got a ton of ideas for projects I want to work on, but will have to see how much time I ultimately end up having.\nLonger-term - I don’t know. Ultimately, I’d love to get paid to work on open source again, but those opportunities are few and far between. I also have some professional goals I’d like to hit before I’m going to be in the right headspace to engage with this too - between writing the book and my plans for Recurse Center, I’ll be there pretty soon.\nThis year has been a real rollercoaster, but I’m extremely grateful to all my friends and mentors who have helped me during this time."
  },
  {
    "objectID": "posts/r-examples/index.html",
    "href": "posts/r-examples/index.html",
    "title": "R package documentation - what makes a good example?",
    "section": "",
    "text": "I’m currently working on adding to the documentation of the arrow R package, and I’ve started thinking about the qualities of good examples. Specifically, I’m referring to the examples included as part of function documentation. In my experience, the best way for me to achieve rapid familiarity with an R function I haven’t worked with before, or understand how to use a function about which I already understand the basics, is by having example code that I can run. In the book ‘R Packages’, Hadley Wickham remarks that examples are “a very important part of the documentation because many people look at the examples first” and rOpenSci recommend that each of a package’s exported functions should be accompanied by examples.\nIn this blog post, I will explore the things that I believe make for good function examples in documentation, focussing mainly on R."
  },
  {
    "objectID": "posts/r-examples/index.html#what-good-looks-like-in-r",
    "href": "posts/r-examples/index.html#what-good-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like in R",
    "text": "What good looks like in R\nI asked people on Twitter for their opinions of good R package documentation in general, and Jonathan Sinclair highlighted the ‘examples’ section from dplyr::case_when, the beginning of which is shown below.\n\n\n\n(image from: https://rdrr.io/cran/dplyr/man/case_when.html)\n\n\nI think Jonathan is spot on in his assessment. To paraphrase, the highlights for him are:\n\nthere is next to no prose or intro\nthere are plenty of comments, as needed, to explain the examples\nthere is a variety of different examples\nthere are examples of what not to do.\n\nThis kind of documentation appeals to my skim-reading self. If I’m trying to accomplish a task, sometimes I just want to run some code and see what happens to get an intuitive feel for what a function does. While I am fully prepared to slow down and read the rest of the documentation, a “quick win” motivates me to invest the additional effort. It tells me that the developers of this code have prioritised making things easy to understand and that the time I am investing will pay off.\nI’ve been skimming through the documentation of some tidyverse and related packages - as I consider many of these to be well documented and easy to read. Here are some things I’ve observed which I think one can do to make function examples look great:\n\ninclude the most basic usage of a function\nuse very simple toy datasets or standard in-built datasets\ndemonstrate non-obvious behaviours of a function\ndemonstrate different parameter values/combinations where relevant\ndemonstrate any unusual parameters\ndemonstrate on different objects if appropriate\nsometimes go beyond the use of an individual function to include common mini-workflows\ngroup documentation and examples for similar functions together\ninclude examples that may lead to unexpected results\ninclude comments to explain examples\nno examples for deprecated functions to discourage their use\nno unpredictable external dependencies - rvest::html_text manually creates HTML to demonstrate capabilities rather than scraping an external site\nsometimes showing the output when it adds to the example (e.g. tidyselect::starts_with() and many other examples from that package)\nexamples should be correct and run without error (unless intended to show erroneous output)"
  },
  {
    "objectID": "posts/r-examples/index.html#what-bad-looks-like-in-r",
    "href": "posts/r-examples/index.html#what-bad-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What bad looks like in R",
    "text": "What bad looks like in R\nI am not intending to “name and shame” any package authors who haven’t included examples for their functions. It may have been overlooked, there may be plenty of explanation elsewhere, or they may have felt that the code was not sufficiently complex to require examples. It might be true that it seems obvious what a function does, but that makes assumptions about the users of your code that might not hold."
  },
  {
    "objectID": "posts/r-examples/index.html#what-good-looks-like-generally",
    "href": "posts/r-examples/index.html#what-good-looks-like-generally",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like generally",
    "text": "What good looks like generally\nWhen reading through examples, one thing that struck me is that when I’m looking at Python docs in Jupyter Notebook (press shift + tab), I also see the output of running the examples.\n\nSimilarly, both examples and outputs are shown in the official docs for some libraries, for example, pandas.\n\nI think this is a helpful feature - less effort is required to see how a function works.\nIn R function documentation, runnable code is often included, but in most cases needs to be manually run by the reader to see the output. I’m torn as to whether this is good or not. On the one hand, it encourages you to run the code and get a more tangible feel for what it does and saves valuable space in the Viewer window in RStudio. On the other hand, it adds an extra manual step to your workflow and lengthens the time until that precious “quick win” of enlightenment when exploring a new function.\nYou get a lot closer to this on the website rdrr.io, which indexes R package documentation and allows examples to be run inline. However, examples are run one after the other without the original code being displayed. So in the case of multiple examples, you have to match up the output to which example it is from.\n\n\n\n(from https://rdrr.io/cran/dplyr/man/case_when.html)\n\n\nSome packages include output as comments within their examples. For instance, the tidyselect package; here’s an example from tidyselect::all_of:\n\n\n\n(from https://rdrr.io/cran/tidyselect/man/all_of.html)\n\n\nAll that said, while the ability to see the output of examples is a nice-to-have, I don’t think it’s essential to good function documentation. With any piece of documentation, it’s necessary to consider the purpose; at a minimum, examples exist to tell the reader how to use a function, and you don’t need to see the output to do that.\nSince I first wrote this, I found out that it is possible to easily run examples from help files by selecting them and then hitting Ctrl+Enter, the same as running code in the Source pane.\n\nAnother thing I wasn’t aware of - pkgdown - commonly used to automatically render docs for packages run examples and displays the output underneath. Check out the example below from the Arrow pkgdown site.\n\nIn conclusion, good examples make functions easier to work with and help readers of your documentation gain a deeper understanding of how a function works. While any examples are better than no examples, you can give your users the best chance of success when using your code with careful thought about the content of your documentation.\nHuge thanks to everyone who responded to my Twitter thread, and to my fantastic colleague Joris Van den Bossche for reading the first draft of this, and our conversations about how things are done in R and Python."
  },
  {
    "objectID": "projects/phd/index.html",
    "href": "projects/phd/index.html",
    "title": "PhD Thesis - Debiasing Reasoning: A Signal Detection Analysis",
    "section": "",
    "text": "Download here: PDF, 2.1MB\nThis thesis focuses on deductive reasoning and how the belief bias effect can be reduced or ameliorated.\nBelief bias is a phenomenon whereby the evaluation of the logical validity of an argument is skewed by the degree to which the reasoner believes the conclusion. There has been little research examining ways of reducing such bias and whether there is some sort of effective intervention which makes people reason more on the basis of logic. Traditional analyses of this data has focussed on simple measures of accuracy, typically deducting the number of incorrect answers from the number of correct answers to give an accuracy score. However, recent theoretical developments have shown that this approach fails to separate reasoning biases and response biases. A reasoning bias, is one which affects individuals’ ability to discriminate between valid and invalid arguments, whereas a response bias is simply the individual’s tendency to give a particular answer, independent of reasoning. A Signal Detection Theory (SDT) approach is used to calculate measures of reasoning accuracy and response bias. These measures are then analysed using mixed effects models.\nChapter 1 gives a general introduction to the topic, and outlines the content of subsequent chapters. In Chapter 2, I review the psychological literature around belief bias, the growth of the use of SDT models, and approaches to reducing bias. Chapter 3 covers the methodology, and includes a a thorough description of the calculation of the SDT measures, and an explanation of the mixed effects models I used to analyse these. Chapter 4 presents an experiment in which the effects of feedback on reducing belief bias is examined. In Chapter 5, the focus shifts in the direction of individual differences, and looks at the effect of different instructions given to participants, and Chapter 6 examines the effects of both feedback and specific training. Chapter 7 provides a general discussion of the implications of the previous three chapters."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks and Workshops",
    "section": "",
    "text": "Big Data in R with Arrow\n\n\n\n\n\n\nNic Crane\n\n\nSep 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiently Engineering Bigger Data with Arrow\n\n\n\n\n\n\nNic Crane\n\n\nAug 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat they forgot to teach you about becoming an open source contributor\n\n\n\n\n\n\nNic Crane\n\n\nJul 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe future’s Shiny: Pioneering genomic medicine in R\n\n\n\n\n\n\nNic Crane\n\n\nJan 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTen Steps To Becoming A Tidyverse Contributor\n\n\n\n\n\n\nNic Crane\n\n\nNov 27, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/rstudioconf2019/index.html",
    "href": "talks/rstudioconf2019/index.html",
    "title": "The future’s Shiny: Pioneering genomic medicine in R",
    "section": "",
    "text": "Shiny's expanding capabilities are rapidly transforming how it is used in an enterprise. This talk details the creation of a large-scale application, supporting hundreds of concurrent users, making use of the future and promises packages. The 100,000 genomes project is an ambitious exercise that follows on from the Human Genome Project - aiming to put the UK at the forefront of genomic medicine, with the NHS as the first health service in the world to offer precision medicine to patients with rare diseases and cancer. Data is at the heart of this project; not only the outputs of the genomic sequencing, but vast amounts of metadata used to track progress against the 100,000 genome target and the status and path of each case through the sample tracking pipeline. In order to make this data readily available to stakeholders, Shiny was used to create an application containing multiple interactive dashboards. A scaled-up version of the app is being rolled out in early 2019 to a much larger audience to support the National Genomics Informatics Service, with the challenge of creating a complex app capable of supporting so many users without grinding to a halt. In this talk, I will explain why Shiny was the obvious technology choice for this task, and discuss the design decisions which enabled this project’s success."
  },
  {
    "objectID": "talks/nyhackr/index.html",
    "href": "talks/nyhackr/index.html",
    "title": "Efficiently Engineering Bigger Data with Arrow",
    "section": "",
    "text": "Data analysis pipelines with larger-than-memory data are becoming more and more commonplace. There are often blurred lines between data science and data engineering, and knowing a bit of both is a sure-fire way to make your life easier when working with big datasets. In this talk, I will give an overview of the arrow R package and best practices for getting the most out of your data when working with bigger datasets. I’ll demo the dplyr interface to arrow, and give you some tips and tricks for getting the most out of arrow’s functionality, as well as applying data engineering principles to speed things up even more."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Scaling Up With R and Arrow\n\n\n\n\n\n\nNic Crane\n\n\nAug 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Thesis - Debiasing Reasoning: A Signal Detection Analysis\n\n\n\n\n\n\nNic Crane\n\n\nJul 22, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nic Crane, PhD",
    "section": "",
    "text": "Hi, I’m Nic! I’m a data scientist, software engineer, and R enthusiast. I am part of the team who maintain the arrow R package and co-author of Scaling Up with R and Arrow."
  },
  {
    "objectID": "talks/rstudioconf2022/index.html",
    "href": "talks/rstudioconf2022/index.html",
    "title": "What they forgot to teach you about becoming an open source contributor",
    "section": "",
    "text": "Getting involved in open source is an amazing learning experience and helps you grow your skills as a developer, but to a new contributor there are so many unknown factors about open source projects. In this talk, I’m going to discuss my journey from occasional open source contributor to full time project maintainer, and answer questions such as: what does it look like from the inside of an open-source project? What’s a good way to get involved, and really learn the internals of an R package? How can I navigate the social dynamics of an open source project? How do contributions look entirely different from the point of view of a contributor versus a maintainer?"
  },
  {
    "objectID": "talks/londonr/index.html",
    "href": "talks/londonr/index.html",
    "title": "Ten Steps To Becoming A Tidyverse Contributor",
    "section": "",
    "text": "A talk about getting involved in open source!\nSlides here"
  },
  {
    "objectID": "talks/positconf2023/index.html",
    "href": "talks/positconf2023/index.html",
    "title": "Big Data in R with Arrow",
    "section": "",
    "text": "A workshop on Arrow that Stephanie Hazlitt and I wrote and presented at posit::conf 2023.\nCourse homepage"
  },
  {
    "objectID": "talks/positconf2023/index.html#overview",
    "href": "talks/positconf2023/index.html#overview",
    "title": "Big Data in R with Arrow",
    "section": "Overview",
    "text": "Overview\nData analysis pipelines with larger-than-memory data are becoming more and more commonplace. In this workshop you will learn how to use Apache Arrow, a multi-language toolbox for working with larger-than-memory tabular data, to create seamless “big” data analysis pipelines with R.\nThe workshop will focus on using the the arrow R package—a mature R interface to Apache Arrow—to process larger-than-memory files and multi-file datasets with arrow using familiar dplyr syntax. You’ll learn to create and use interoperable data file formats like Parquet for efficient data storage and access, with data stored both on disk and in the cloud, and also how to exercise fine control over data types to avoid common large data pipeline problems. This workshop will provide a foundation for using Arrow, giving you access to a powerful suite of tools for performant analysis of larger-than-memory data in R.\nThis course is for you if you:\n\nwant to learn how to work with tabular data that is too large to fit in memory using existing R and tidyverse syntax implemented in Arrow\nwant to learn about Parquet and other file formats that are powerful alternatives to CSV files\nwant to learn how to engineer your tabular data storage for more performant access and analysis with Apache Arrow"
  },
  {
    "objectID": "projects/book/index.html",
    "href": "projects/book/index.html",
    "title": "Scaling Up With R and Arrow",
    "section": "",
    "text": "Published online for free at arrowrbook.com\nA book I co-authored with Jonathan Keane and Neal Richardson about working with larger-than-memory datasets in R with Arrow. The paper version will be published later this year."
  },
  {
    "objectID": "posts/rwatcher/index.html",
    "href": "posts/rwatcher/index.html",
    "title": "Monitoring R Package Updates with Github Actions",
    "section": "",
    "text": "As maintainer of the Arrow R package, there are a few packages I want to keep up to date with, so I can make sure that our bindings continue to be compatible with the latest versions of these packages. The packages I’m most interested in here are:\nI’m also interested in knowing when packages that folks use for vaguely similar purposes have been updated; I like to be up-to-date on this as sometimes people ask me about how things compare.\nPreviously, I’d occasionally caught glimpses of things via social media, but I wanted a more methodical approach, and so decided to write a GitHub Actions CRON job that does this. Now, when any of the packages on my list is updated, I receive an email that looks a little like this:\nIn this blog post, I’m going to walk through how I created this repository, and how you can do the same for your own packages."
  },
  {
    "objectID": "posts/rwatcher/index.html#how-it-all-works",
    "href": "posts/rwatcher/index.html#how-it-all-works",
    "title": "Monitoring R Package Updates with Github Actions",
    "section": "How it all works",
    "text": "How it all works\nThe repo itself it pretty simple in structure - it contains the GitHub Actions workflow, and a folder containing the changelogs for the packages I’m interested in.\n# tree\n.github\n└── workflows\n    └── compare_hashes.yml\nchangelogs\n├── data.table-NEWS.md\n├── dbplyr-NEWS.md\n├── dplyr-NEWS.md\n├── dtplyr-NEWS.md\n├── duckdb-NEWS.md\n├── duckplyr-NEWS.md\n├── lubridate-NEWS.md\n├── r-polars-NEWS.md\n├── stringr-NEWS.md\n└── tidypolars-NEWS.md\nThe workflow is triggered every day at 1am UTC, and runs a script that compares the hashes of the changelogs in my repo with the hashes of the changelogs in the package repos. If there’s a difference, the new changelog is saved to my repo and it sends me an email."
  },
  {
    "objectID": "posts/rwatcher/index.html#the-github-actions-workflow",
    "href": "posts/rwatcher/index.html#the-github-actions-workflow",
    "title": "Monitoring R Package Updates with Github Actions",
    "section": "The GitHub Actions Workflow",
    "text": "The GitHub Actions Workflow\nIn the next sections, I’ll walk through the GitHub Actions workflow, step by step, explaining what each bit does.\n\nScheduling\nThe start of the workflow looks like this:\nname: Check for package updates\n\non:\n  schedule:\n    # * is a special character in YAML so you have to quote this string\n    - cron:  '00 1 * * *'\nIt has a name, and the schedule is set to run every day at 1am UTC. The cron syntax is a bit weird, but there’s a handy crontab guru that can help you figure out what you need to put in here.\n\n\nJobs\nNext, we set up the different jobs we want to run. I want 1 job to run for each package.\nI used GitHub Copilot to help me with some of the syntax here; it was fantastic when I just added a comment describing what I’d like to be added, and then it filled it in. This wasn’t a perfect process as you have to know what to ask, and setting up the list of packages to work with was tricky, as I didn’t quite have the understanding of how matrices (which can run code in parallel) interacted with arrays (for specifying multiple inputs for each parallel job) as I hadn’t used this before. A bit of googling and a skim of StackOverflow and I got there pretty quickly though.\njobs:\n  compare-hashes:\n    name: ${{matrix.package.name}} updates\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    strategy:\n        matrix:\n          package: \n            [\n                { name: dbplyr, file: dbplyr-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/dbplyr/main/NEWS.md' }, \n                { name: lubridate, file: lubridate-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/lubridate/main/NEWS.md' },\n                { name: dplyr, file: dplyr-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/dplyr/main/NEWS.md'}, \n                { name: data.table, file: data.table-NEWS.md, url: 'https://raw.githubusercontent.com/Rdatatable/data.table/master/NEWS.md'},\n                { name: dtplyr, file: dtplyr-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/dtplyr/main/NEWS.md'},\n                { name: duckdb-r, file: duckdb-r-NEWS.md, url: 'https://raw.githubusercontent.com/duckdb/duckdb-r/main/NEWS.md'},\n                { name: r-polars, file: r-polars-NEWS.md, url: 'https://raw.githubusercontent.com/pola-rs/r-polars/main/NEWS.md'},\n                { name: stringr, file: stringr-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/stringr/main/NEWS.md'},\n                { name: duckplyr, file: duckplyr-NEWS.md, url: 'https://raw.githubusercontent.com/duckdblabs/duckplyr/main/NEWS.md'},\n                { name: tidypolars, file: tidypolars-NEWS.md, url: 'https://raw.githubusercontent.com/etiennebacher/tidypolars/main/NEWS.md'},\n            ]\nThe runs-on specifies the operating system to run the job on, and the permissions section allows the job to write to the repo. The strategy section is where we set up the matrix of packages to work with. Each package has a name, a file name, and a URL to the changelog. The name of the job is set to the name of the package.\n\n\nSteps\nNext, we set up the steps that we want to run. The first step is to check out the repo we are working in, and get the hash of the relevant changelog file I have stored in my repo. This is saved to the GITHUB_OUTPUT environment variable, which is a file that is shared between all the steps in the job.\n    steps:\n        - name: Checkout code\n          uses: actions/checkout@v2\n        - name: Get local file hash\n          id: local-hash\n          run: echo \"local_hash=$(md5sum changelogs/${{ matrix.package.file }} | cut -d ' ' -f 1)\" &gt;&gt; $GITHUB_OUTPUT\nNext, I want to get the hash of the latest version of the package’s changelog file. I do this by downloading the file, and then getting the hash of the downloaded file. This is also saved to the GITHUB_OUTPUT environment variable.\n        - name: Get remote file\n          id: remote-file\n          run: |\n                  mkdir tmp\n                  curl -s ${{ matrix.package.url }} &gt; tmp/${{ matrix.package.file }}#\n        - name: Get remote file hash\n          id: remote-hash\n          run: echo \"remote_hash=$(md5sum tmp/${{ matrix.package.file }} | cut -d ' ' -f 1)\" &gt;&gt; $GITHUB_OUTPUT\nFinally, I want to compare the hashes of the two files. If they’re different, I want to update the changelog in my repo. I do this by setting up a conditional step that only runs if the hashes are different. In this case, I’m setting the git config, copying the new changelog to my repo, and then committing and pushing the changes.\n        - name: Update changed files\n          if: ${{ steps.local-hash.outputs.local_hash != steps.remote-hash.outputs.remote_hash }}\n          run: |\n            echo \"Hashes do not match!\"   \n            git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n            git config --global user.name \"GHA Bot\"\n            cp tmp/${{ matrix.package.file }} changelogs/${{ matrix.package.file }}\n            git add changelogs/${{ matrix.package.file }}\n            git pull --ff-only\n            git commit -m \"Update ${{matrix.package.name}} changelog\"\n            git push\nFinally, I want to send an email notification if the changelog has been updated. I do this by setting up a conditional step that only runs if the hashes are different. I use the dawidd6/action-send-mail action to send the email. I set up a few secrets in my repo to store the email address and password, and then use those in the action. I also set up the subject and body of the email using the package name and URL.\nThe username and password are not my actual email address and password; instead, you can set up an app password for your email account, and use that instead, which is more secure.\n        - name: Send email notification\n          if: ${{ steps.local-hash.outputs.local_hash != steps.remote-hash.outputs.remote_hash }}\n          uses: dawidd6/action-send-mail@v3\n          with:\n            server_address: smtp.gmail.com\n            server_port: 465\n            username: ${{ secrets.MAIL_USERNAME }}\n            password: ${{ secrets.MAIL_PASSWORD }}\n            subject: \"${{matrix.package.name}} update\"\n            body: \"${{matrix.package.name}} has been updated! Please check the changelog at ${{matrix.package.url}}.\"\n            to: ${{ secrets.MAIL_RECIPIENT }}\n            from: ${{ secrets.MAIL_USERNAME}}\nAnd that’s it! The full repository can be found here."
  },
  {
    "objectID": "posts/rwatcher/index.html#conclusion",
    "href": "posts/rwatcher/index.html#conclusion",
    "title": "Monitoring R Package Updates with Github Actions",
    "section": "Conclusion",
    "text": "Conclusion\nI really enjoyed working on this and learning more about GitHub Actions. This has proved to be a useful tool, though there are a few improvements that could be made:\n\nsome packages update their changelog more frequently than others and so some of the updates I get feel a bit spammy. I could fix this by running my CRON job on a weekly rather than daily schedule.\nI don’t use this as much as I anticipated because some changes are really minor, and I tend to skim them and not pay too much attention. Again, a different CRON frequency could probably help here.\nI’m more interested in some packages than others. {dplyr}, {lubridate}, and {stringr} are the most important, whereas others are just a “nice to have” here. I could separate these out into different jobs, and run them on different schedules.\n\nAnyway, I’d love to hear your thoughts - how do you keep up to date with changes in R packages? Do you have any suggestions for improvements to this workflow? Let me know! Get in touch via Mastodon or LinkedIn."
  },
  {
    "objectID": "posts/goats/index.html",
    "href": "posts/goats/index.html",
    "title": "Three Goats in a Raincoat",
    "section": "",
    "text": "I am not a fan of the term “imposter syndrome.” It’s a lazy way of ignoring the complex and interconnected, often structural reasons that contribute to many people’s feelings, which manifests itself in a variety of deeply idiosyncratic ways. But I guess the tl;dr is that this is what this post is about.\nI am not someone who has “hobbies” as such; I have interests that I adopt with great enthusiasm, pursue obsessively for a short period of time, often declaring that I have found a lifelong fascination, and then drop once the initial feeling of “endless possibility” has worn off and something else shiny and new finds it way across my path. I suppose my hobby is “pursuing novelty.” The 2022-2023 edition of this came in the form of tabletop role-playing games, and earlier this year, I found myself sitting with some good friends, beers and snacks, working out how my character, “Ach!”, a small goat with a fondness for fainting at inappropriate moments could make their way into a human party undetected.\nThe solution for Ach! and friends was to don a disguise and stand on each others’ shoulders, and this image of 3 goats in a raincoat trying to sneak into a human party perfectly reflects how I feel about myself sometimes as an open source maintainer.\n(Here, I need to get round to commissioning an image of 3 goats in a raincoat standing on each other’s shoulders. The top one has black and red braids and is wearing a hat and fake beard/moustache)\nI am the current maintainer of the arrow R package, which in reality just means I am the person who is listed as such in the package DESCRIPTION file, and have the responsibility of clicking the link in the confirmation email from CRAN when someone submits the latest version of of the package to CRAN. Of course, I do more than just this; I triage bugs, submit patches, review PRs, and occasionally implement new features, but so do other people. While for career-serving purposes, I can technically claim to be “the” maintainer of the package, honestly, I am “a” maintainer and part of a team. This isn’t always reflected in other people’s language, and results in weird moments where people say things to me like, “ah, you wrote arrow!” and I have to respond with, “I absolutely did not, though I did write a lot of the bindings for dplyr::across!”, a topic which subsequently interests me more than it does them."
  },
  {
    "objectID": "posts/goats/index.html#the-tyranny-of-should",
    "href": "posts/goats/index.html#the-tyranny-of-should",
    "title": "Three Goats in a Raincoat",
    "section": "The tyranny of “should”",
    "text": "The tyranny of “should”\nI’ve spent much of this year suffering from the tyranny of “should” regarding my own knowledge and abilities, in a way that has been verging on ridiculous. A lot of it is the perfect storm of adjusting to new responsibility, the deeply unhelpful way in which my self-image when I was younger was built up around being “smart” and “knowing things”, and a knack for punching above my weight by being good at figuring out just enough about a topic to get shit done. I don’t know every single detail of object orientation in R, but I know enough to be able to create new bindings between C++ objects and R6 classes in the arrow R package and expose their methods via fields and active bindings. Despite having looked it up repeatedly over the past few years, I still cannot recall the difference between how things work with static and shared libraries, but I did manage to fix a bug requiring me to bump the version of the C++ date library that Arrow vendors, via inspecting old pull requests and helpful feedback I received on the PR I submitted.\nIt came to a head at this year’s posit::conf, where I had the privilege of co-teaching a workshop about Arrow with the wonderful Stephanie Hazlitt. We spent months preparing materials and practicing teaching to our laptop screens, but when the event itself rolled around, teaching it to a room just shy of 60 people was a different experience entirely. It was intimidating being in front of a room of eager learners, and any jokey asides I was usually capable of making went out of the window, and I just taught the materials fairly plainly, stumbling over my words a bit, and finding the moments of silence in the room as I was teaching pretty awful. Subsequent feedback from folks I trust enough to deliver at least some honesty alongside their validation has led me to conclude that the only person who really noticed or cared about this was me. I was doing my old trick of snatching defeat from the jaws of victory, and realised later that my self-expectations were totally unreasonable. While I used to be able to absolutely smash through delivering a well-practiced 2-day “introduction to R” course, and delighted in engaging the folks in the room and showing them what they (and R!) were capable of, it’s 5 years since I last did that on the regular. This was the first time these materials had seen the light of day, and at no less than posit::conf, daunting enough on its own. And honestly, this is all ego. We ran a good workshop, people were engaged, and we wrote some quality reusable materials. Yes, there’s room for improvement, but overall it was solid. Delivery of content is something I can (and will) work on. I’ve applied for The Carpentries instructor training, and have plans to get back into the swing of teaching by running workshops (on Arrow and other topics) at user events."
  },
  {
    "objectID": "posts/goats/index.html#the-power-of-i-dont-know-but-i-can-find-out",
    "href": "posts/goats/index.html#the-power-of-i-dont-know-but-i-can-find-out",
    "title": "Three Goats in a Raincoat",
    "section": "The power of “I don’t know but I can find out”",
    "text": "The power of “I don’t know but I can find out”\nThe second day of posit::conf was spent being a teaching assistant for the fantastic Andy Teucher, who was teaching “Fundamentals of Package Development”. I managed to escape a lot of this “should” nonsense here, so when one of the course attendees asked me about where to store CSV files to use in unit tests, I happily admitted I wasn’t sure but could find out, and together we had a look in “Writing R Packages”, where I couldn’t find the information, and then suggested the approach I usually take when working on arrow - look at what folks who often define best practices do in the packages that they maintain. We took a look at readr, found the files in the testthat directory, and concluded that unless there’s reason to want to do anything more complicated (like make those datasets available to end users), then that approach is fine. Not knowing the answer instantly wasn’t a hindrance to working out the necessary solution. Learning how to say “I don’t know, but let’s find out together” is one of the most important things I learned when I was first teaching people how to use R. A teacher doesn’t have to know everything, but should be able to help a learner find the information they need."
  },
  {
    "objectID": "posts/goats/index.html#the-power-of-community",
    "href": "posts/goats/index.html#the-power-of-community",
    "title": "Three Goats in a Raincoat",
    "section": "The power of community",
    "text": "The power of community\nThe rest of the conference week was pretty awesome. I was in the middle of a CRAN resubmission (well, actually a re-re-resubmission, but let’s not go there) which had got a bit hairy, and I needed some help tracking down the source of the bug. I spent some time briefly engaging in a bit of pair programming with Jonathan Keane and Neal Richardson, who’ve both been involved with Arrow for a lot longer than me, and I’m always taken aback by just how quickly they both get to the source of a problem any time I ask for help. Part of my “knowledge anxiety” manifests as me sometimes taking longer than I would like to process things when communicating verbally about technical topics, which leads to me getting really frustrated when I’m just not “getting” things in the moment that I know later will seem pretty obvious and uncomplicated. This happened when we were looking at the arrow bug, but I had this sudden moment of clarity upon realising…nobody cared except me. Jon and Neal are great collaborators and are always happy to explain things again if there’s time, and aren’t going to judge me for it.\nI also realised that there is no universal measure of what I should know or where I should be at. While I always want to know more about how things work, I don’t have to know everything and it’s a pretty unrealistic expectation for me to have of myself; sure I can (and often do!) go read a whole load of the codebase and related concepts, but that’ll never be the same as also leaning on other people’s years of experience and knowledge. The whole point of being part of a community is to be able to share both overlapping and divergent knowledge."
  },
  {
    "objectID": "posts/goats/index.html#progress-is-power",
    "href": "posts/goats/index.html#progress-is-power",
    "title": "Three Goats in a Raincoat",
    "section": "Progress is power",
    "text": "Progress is power\nI don’t think there is a universal solution to this problem of feeling like you “should” know more, but all I know is what has worked for me. Public work is important - social media, blog posts, talks, workshops, whatever. It can be scaled to wherever you’re at. There’s always something you’ll be surprised that you know but others don’t. And it creates a lovely feedback loop whereby a little bit of external validation can go a long way.\nLearning is also important - I did the first half of the Udacity C++ Nanodegree this year, and while a lot of what it taught me was that I never want to be a full-blown C++ developer (I’d rather make weird buildings and spaceships out of existing Lego pieces than become a polymer scientist just to create custom bricks), realising that I could learn C++ at that level if I really wanted to, was invaluable. Part of the course involved getting acquainted with cmake, and whilst I can’t claim expertise there, dabbling in a bit of the whats and whys helped make many related Arrow project issues seem less mystical.\nReassurance and validation are good and well, but in my experience, having tangible proof of the things that I do know is more effective."
  },
  {
    "objectID": "posts/goats/index.html#fake-it-til-you-make-it-is-deeply-problematic",
    "href": "posts/goats/index.html#fake-it-til-you-make-it-is-deeply-problematic",
    "title": "Three Goats in a Raincoat",
    "section": "“Fake it til you make it” is deeply problematic",
    "text": "“Fake it til you make it” is deeply problematic\nThe commonly received advice is “fake it ’til you make it”. Pretend you feel like you belong and are confident, until that becomes the case. I’ve spent a long time attempting this, and the problem is that it doesn’t actually work, because it doesn’t address the underlying issue. It’s only since I started to own the fact that I don’t feel comfortable in every environment or domain and working out what I need to do to feel more comfortable that I’ve felt my confidence growing.\nIn the past month, I’ve been a lot more open about how I’ve been feeling around this. It’s been tricky as I’ve been scared that my vulnerability just looks weakness that I shouldn’t show around other people, or just come off as moaning, but it’s had massive benefits. During the first couple of days of posit::conf this year, I suffered the worst ongoing anxiety I’ve had all year - I had a constant knot in my chest - but despite that managed to have a good time as everyone was very accepting. I can say without a doubt that pretending to be fine would have made things infinitely worse.\nThere was another point during the conference when I casually mentioning that I was feeling quite hopeful about a potential opportunity but not entirely sure if it was in the bag, as I suspected I might be up against someone whose background I find impressive. It was kindly pointed out to me that each of us bring different things to the table, a comment which has since set off a chain reaction of me starting to appreciate what I can do rather than what I can’t."
  },
  {
    "objectID": "posts/goats/index.html#the-cure-for-imposter-syndrome",
    "href": "posts/goats/index.html#the-cure-for-imposter-syndrome",
    "title": "Three Goats in a Raincoat",
    "section": "The cure for imposter syndrome",
    "text": "The cure for imposter syndrome\nThe most important thing I learned this year is that the cure for imposter syndrome isn’t persuading yourself that you aren’t just 3 goats in a raincoat pretending to be a person, but instead surrounding yourself with folks who wouldn’t really care if you were anyway because goats are cool and that’s a pretty awesome feat of acrobatics and balancing."
  },
  {
    "objectID": "posts/c_deepr/index.html",
    "href": "posts/c_deepr/index.html",
    "title": "Explaining the C bits at the start of ‘Deep R Programming Ch14: Interfacing compiled code’",
    "section": "",
    "text": "One of my goals for next year is to get a deeper understanding of R’s C API. I’m making a start on this by reading Chapter 14 of Deep R Programming by Marek Gagolewski: “Interfacing compiled code”. It’s a great resource, though the chapter preface states “we assume basic knowledge of the C language”. I do not have this knowledge My C knowledge is fairly limited, and so this blog post will pull out some of the details from that chapter, especially bits where I’ve had to go “wtf is that?!” or remind myself by googling. My current level of C knowledge at the moment is pretty minimal; the main things I know are:\n\nC is a compiled language (as opposed to an interpreted language) and C code needs compiling before it can be run\nA lot of R’s internals are based on C\nC is a statically-type language; this means that variable types are defined when the variable is created and cannot later change other than via explicit manual casting\n\nAnd that’s basically it! I imagine there are other bits I don’t realise I know which I’ve picked up from being an R package maintainer and dabbling in a few C++ tutorials, but I’ll try to explain everything as much as possible. OK, let’s do this!\nThe book chapter provides an example package with a simple C function implemented, and walks us through the code step-by-step. Great!\nSection 14.1.1 starts off with an example of a C function defined in src/cfunc.h. I guess the first thing to note is the location of the file - in the package’s src directory. This is where any compiled code needs to go, typically C or C++ code, or even Fortran if you’re really going old-school.\n\nHeader files and source files\nAnother thing to note here is the file name, which ends in .h. C code can be divided into header files (ending in .h) and source files (ending in .c). Header files contain the function declarations (including variable types) and other things like macros (named bits of code for the pre-processor to work with). They’re sometimes referred to as the interface - they contain information about functions’ inputs and outputs - including the argument names and types.\n\n\nOnce-only includes\nThe first couple of lines of code in the header file contain these lines:\n#ifndef __CFUNS_H\n#define __CFUNS_H\nand the final line is\n#endif\nWhat’s happening here is that, often we can end up with projects containing multiple files, some of which source each other, and include them. We don’t want to end up with duplication of the headers which have been included, otherwise the compiler will raise an error, so we put them in an #ifndef wrapper, and give them a name. Basically, what we’re saying to the preprocessor here is that if this name hasn’t already been defined, defined it and include this code, but if it’s already defined then skip this.\n\n\nIncludes\nThe third line in src/cfunc.h is:\n#include &lt;stddef.h&gt;\nThis allows for the inclusion of a file from the C standard library, which has a few different variables included. The key one for us here is size_t, which is commonly used for iterating over items in arrays - we need this as we’ll be including a for loop in the definition of our function.\n\n\nThe preprocessor\nAbove I casually mentioned the C preprocessor a couple of times without defining it. A succinct and perhaps naive summary is that there are multiple steps in the compilation of C code. One of these phases is preprocessing and it involves things like processing any additional files we’ve said we wanted to include, and replacing macros with their definitions.\n\n\nDeclarations\nSo, the declaration for the function in the example looks like this\ndouble my_c_sum(const double* x, size_t n);\nIn words, this means that:\n\nit is a function which returns an object of type double\nthe function name is my_c_sum\nthe first argument is called x\nx is a pointer to a variable of type double\nx is a const variable, which means it won’t be modified in the body of the function\nthe second argument is called n\nn is of type size_t\n\nThis concept of a pointer just means that x contains the memory address of the double that we pass in, rather than a copy of the values in it. This prevents us from copying the values unnecessarily.\n\n\nSource file\nSource files contain the body of the function, sometimes called the implementation.\nThe code in the book chapter goes on to show the content of the file src/cfuncs.c. The first line of this file is:\n#include \"cfuns.h\"\nThis is including the header file we discussed above. The rest of the code in the source file contains the definition of my_c_sum:\n/* computes the sum of all elements in an array x of size n */\ndouble my_c_sum(const double* x, size_t n)\n{\n    double s = 0.0;\n    for (size_t i = 0; i &lt; n; ++i) {\n        /* this code does not treat potential missing values specially\n           (they are kinds of NaNs); to fix this, add:\n        if (ISNA(x[i])) return NA_REAL;  // #include &lt;R.h&gt;  */\n        s += x[i];\n    }\n    return s;\n}\nThe function signature here is identical to how it is defined in the header file. The for loop uses the n argument which was passed in to represent the size of the array to loop through. In numerous other languages we’d calculate the size of the array in the body of the function, but in C you cannot have an array of unknown size, and so it must be passed in as a parameter. I think this is to do with how the C compiler allocates memory; more modern C does have the concept of variable-length arrays.\nThe chapter goes on to discuss further examples which then show how to include a wrapper which can be called by R. I won’t discuss this here, as the text there is all explained well, and the contents are more specific to R’s C API, and not specifically just C-related topics.\nHere’s a summary of the C-related topics mentioned here:\n\nheader files and source files\nincludes\nonce-only includes\nthe preprocessor\nvariable-length arrays\nconst variables\npointers\nstatically-typed languages\ncompiled and interpreted languages"
  },
  {
    "objectID": "posts/rc/index.html",
    "href": "posts/rc/index.html",
    "title": "Recurse Center Return Statement",
    "section": "",
    "text": "I spent 3 months earlier this year at Recurse Center in New York. Here’s a summary of the highlights of my time there!"
  },
  {
    "objectID": "posts/rc/index.html#what-is-recurse-center",
    "href": "posts/rc/index.html#what-is-recurse-center",
    "title": "Recurse Center Return Statement",
    "section": "What is Recurse Center?",
    "text": "What is Recurse Center?\nThe simplest explanation of what Recurse Center is is that it’s like a writer’s retreat but for programmers. Participants have to apply to get in, and there are 3 stages. The first is a written application, followed by a conversational interview, and then a pair programming interview. Generally, the intention is to assess whether you’ll be a good fit for RC. The selection criteria are stated explicitly on the website, but to summarise, they’re looking for self-motivated people who enjoy programming and are pleasant to be around. It’s not about being an amazing programmer; though I met some people with seriously impressive skills there, I also met newer programmers who were enthusiastic and engaged.\nRC can be attended remotely online or in person, but I opted for the in-person experience to get the most out of it. The hub, the in-person location is in Brooklyn, and takes up 2 floors of an office building. The 4th floor is the main floor where people code, socialise, do pair programming, and hang out in the kitchen, presentation space, or the meeting rooms. The 5th floor is the quiet floor where people can do more focused work without interruption.\nRC has a code of conduct, as well as 4 more general social rules:\n\nNo well-actually’s\nNo feigned surprise\nNo backseat driving\nNo subtle -isms\n\n\n\n\nA printed poster of RC’s social rules\n\n\nAs well as social rules about what not to do, there are 3 guiding principles, or self-directives, about what to do:\n\nWork at the edge of your abilities\nBuild your volitional muscles\nLearn generously\n\nThese explicit expectations lead to a fantastic learning environment where some of the more toxic elements that can sometimes be found in tech were avoided, and folks worked on project that really mattered to them and helped other people do the same.\nRecurse Center is a business, and makes its money from recruitment - RC works with partner companies looking for software engineers and similar roles, though admission to RC isn’t predicated on looking for employment via them, and is never pushed on people."
  },
  {
    "objectID": "posts/rc/index.html#why-did-i-go",
    "href": "posts/rc/index.html#why-did-i-go",
    "title": "Recurse Center Return Statement",
    "section": "Why did I go?",
    "text": "Why did I go?\nWhen I decided to go to RC, I was at the beginning of a career break. I had done some consultancy/contract work early on, but had decided that I needed a proper pause. I had begun working on the book on Arrow that I’m co-authoring, and splitting my attention between the two things wasn’t working well for me. I also wanted some focused learning time - I had a ton of side projects I’d been meaning to work on but had never gotten moving and I felt like this would be a great opportunity to push them forwards.\nMy secondary motivation was how I was feeling at the time. I was lacking confidence in some of my technical skills, and know from experience that the best way to tackle that is learn new things and get things done, to disprove some of the insecurities I was having.\n\n\n\nA cross-stitched piece on the wall at RC with the text “be gay find primes”"
  },
  {
    "objectID": "posts/rc/index.html#what-did-i-intend-to-work-on",
    "href": "posts/rc/index.html#what-did-i-intend-to-work-on",
    "title": "Recurse Center Return Statement",
    "section": "What did I intend to work on?",
    "text": "What did I intend to work on?\nBefore I went to RC, I had a huge list of potential things to work on. I didn’t plan on doing all of these, but this was my initial brain dump of everything I thought might be useful or interesting in future:\n\nRead “Writing R Extensions”\nDo the course Nand2Tetris to get a better foundation of “lower level stuff”\nLearn C\nLearn more about the finer details of how building R packages work\ncontribute to parsermd so it works for Quarto markdown documents\nfinish the Udacity C++ nanodegree I’d done some of a while ago\nlearn more about the C++ build process\nDo all of the C sessions of CS50\nDo some work on the Arrow book\n\n\n\n\nSome of the retro machines at the RC hub."
  },
  {
    "objectID": "posts/rc/index.html#what-did-i-actually-work-on",
    "href": "posts/rc/index.html#what-did-i-actually-work-on",
    "title": "Recurse Center Return Statement",
    "section": "What did I actually work on?",
    "text": "What did I actually work on?\nOne of the principles of RC is to use your volitional muscles and work on what interests you rather than things you think you “should” work on. There’s a balance to be struck here, but there was a great exercise early on where we were guided through questions to ask ourselves to work out what was going to be the best things to focus on.\nIn the end, I didn’t get through all of the things on my original list, but had a much more exciting time doing the following things:\n\nwent through most of Nand2Tetris\nplayed around with compilation and decompilation and how compiler flags change the code produced\nplayed with assembly code\nstarting going through the book Crafting Interpreters and plan to do more another time!\nspent lots of time pairing with other people on my and other people’s code and developing a love for pair programming\ndid a few presentations on Nand2Tetris and why it’s such a great course!\nwrote an R-based Quarto markdown parser which implements recursive descent parsing\ndelved into the Quarto codebase and developed a new version of the parser which takes advantage of pandoc instead of doing everything the hard way\n\nAnd alongside this:\n\ndid a ton of writing of the Arrow book I’m co-authoring\nfinished off interviewing for a job I’d applied for at Novartis which I’m super excited to start tomorrow!\nattended 2 days of Carpentries instructor training\nhad regular lessons with a public speaking tutor\nsubmitted and got accepted to co-present a talk to posit::conf\nhanded over arrow maintainership to the fantastic Jon Keane"
  },
  {
    "objectID": "posts/rc/index.html#what-did-i-learn",
    "href": "posts/rc/index.html#what-did-i-learn",
    "title": "Recurse Center Return Statement",
    "section": "What did I learn?",
    "text": "What did I learn?\n\nlow-level programming concepts felt out of reach for a long time; most docs are aimed at people who already understand the tech being described, there’s a lot of foundational computer science knowledge that is assumed that I didn’t have, and it’s hard to know where to find good learning resources. However, I learned that actually, it isn’t nearly as out of reach as I’d thought and all I really need is to find that one good resource, and have time to explore it. Nand2Tetris is an awesome course!\na solid community of peers is invaluable. I’ve been lucky enough in my career so far to mostly have either a community of peers or excellent mentors, but in future I’m going to aim to make sure I have both, whether that’s via a job role or engaging with wider programming communities more fully.\npair programming is a wonderful thing and is massively underutilised. Pairing shouldn’t only be utilised when there’s a bug, but be a regular practice. It helps share assumptions and norms,\nworking on the things I want to as well as the things I think I should work on is so key to productivity\nto listen to my gut when it comes to my own learning. I have had a frustration for years with public speaking where I know I’m not terrible at it, want to be better, but asking friends and colleagues for feedback often led to (lovely, kind, well-meaning) validation but not the actionable feedback I wanted. Working with a tutor has been amazing for helping me identify my strengths (I convey passion and excitement for my topic, and especially when I teach, really try to connect with my audience) and weaknesses (my natural speaking speed is about 3 times what it should be, and I could work on structuring information better within a talk)\nnot to mistake being limited for time to work on a problem with not being able to solve the problem\n\n\n\n\nSome craft projects which were on display at RC"
  },
  {
    "objectID": "posts/rc/index.html#conclusion",
    "href": "posts/rc/index.html#conclusion",
    "title": "Recurse Center Return Statement",
    "section": "Conclusion",
    "text": "Conclusion\nI can honestly describe the 3 months I spent at Recurse Center as the best solid 3 month period of my life. I learned a huge amount about, not only programming, but my own motivation. I made some amazing friends - my fellow RC participants and the faculty there are some of the coolest people I’ve ever met - and will be staying involved in the community in the longer term."
  }
]