[
  {
    "objectID": "posts/goats/index.html",
    "href": "posts/goats/index.html",
    "title": "Three Goats in a Raincoat",
    "section": "",
    "text": "I am not a fan of the term “imposter syndrome.” It’s a lazy way of ignoring the complex and interconnected, often structural reasons that contribute to many people’s feelings, which manifests itself in a variety of deeply idiosyncratic ways. But I guess the tl;dr is that this is what this post is about.\nI am not someone who has “hobbies” as such; I have interests that I adopt with great enthusiasm, pursue obsessively for a short period of time, often declaring that I have found a lifelong fascination, and then drop once the initial feeling of “endless possibility” has worn off and something else shiny and new finds it way across my path. I suppose my hobby is “pursuing novelty.” The 2022-2023 edition of this came in the form of tabletop role-playing games, and earlier this year, I found myself sitting with some good friends, beers and snacks, working out how my character, “Ach!”, a small goat with a fondness for fainting at inappropriate moments could make their way into a human party undetected.\nThe solution for Ach! and friends was to don a disguise and stand on each others’ shoulders, and this image of 3 goats in a raincoat trying to sneak into a human party perfectly reflects how I feel about myself sometimes as an open source maintainer.\n(Here, I need to get round to commissioning an image of 3 goats in a raincoat standing on each other’s shoulders. The top one has black and red braids and is wearing a hat and fake beard/moustache)\nI am the current maintainer of the arrow R package, which in reality just means I am the person who is listed as such in the package DESCRIPTION file, and have the responsibility of clicking the link in the confirmation email from CRAN when someone submits the latest version of of the package to CRAN. Of course, I do more than just this; I triage bugs, submit patches, review PRs, and occasionally implement new features, but so do other people. While for career-serving purposes, I can technically claim to be “the” maintainer of the package, honestly, I am “a” maintainer and part of a team. This isn’t always reflected in other people’s language, and results in weird moments where people say things to me like, “ah, you wrote arrow!” and I have to respond with, “I absolutely did not, though I did write a lot of the bindings for dplyr::across!”, a topic which subsequently interests me more than it does them."
  },
  {
    "objectID": "posts/goats/index.html#the-tyranny-of-should",
    "href": "posts/goats/index.html#the-tyranny-of-should",
    "title": "Three Goats in a Raincoat",
    "section": "The tyranny of “should”",
    "text": "The tyranny of “should”\nI’ve spent much of this year suffering from the tyranny of “should” regarding my own knowledge and abilities, in a way that has been verging on ridiculous. A lot of it is the perfect storm of adjusting to new responsibility, the deeply unhelpful way in which my self-image when I was younger was built up around being “smart” and “knowing things”, and a knack for punching above my weight by being good at figuring out just enough about a topic to get shit done. I don’t know every single detail of object orientation in R, but I know enough to be able to create new bindings between C++ objects and R6 classes in the arrow R package and expose their methods via fields and active bindings. Despite having looked it up repeatedly over the past few years, I still cannot recall the difference between how things work with static and shared libraries, but I did manage to fix a bug requiring me to bump the version of the C++ date library that Arrow vendors, via inspecting old pull requests and helpful feedback I received on the PR I submitted.\nIt came to a head at this year’s posit::conf, where I had the privilege of co-teaching a workshop about Arrow with the wonderful Stephanie Hazlitt. We spent months preparing materials and practicing teaching to our laptop screens, but when the event itself rolled around, teaching it to a room just shy of 60 people was a different experience entirely. It was intimidating being in front of a room of eager learners, and any jokey asides I was usually capable of making went out of the window, and I just taught the materials fairly plainly, stumbling over my words a bit, and finding the moments of silence in the room as I was teaching pretty awful. Subsequent feedback from folks I trust enough to deliver at least some honesty alongside their validation has led me to conclude that the only person who really noticed or cared about this was me. I was doing my old trick of snatching defeat from the jaws of victory, and realised later that my self-expectations were totally unreasonable. While I used to be able to absolutely smash through delivering a well-practiced 2-day “introduction to R” course, and delighted in engaging the folks in the room and showing them what they (and R!) were capable of, it’s 5 years since I last did that on the regular. This was the first time these materials had seen the light of day, and at no less than posit::conf, daunting enough on its own. And honestly, this is all ego. We ran a good workshop, people were engaged, and we wrote some quality reusable materials. Yes, there’s room for improvement, but overall it was solid. Delivery of content is something I can (and will) work on. I’ve applied for The Carpentries instructor training, and have plans to get back into the swing of teaching by running workshops (on Arrow and other topics) at user events."
  },
  {
    "objectID": "posts/goats/index.html#the-power-of-i-dont-know-but-i-can-find-out",
    "href": "posts/goats/index.html#the-power-of-i-dont-know-but-i-can-find-out",
    "title": "Three Goats in a Raincoat",
    "section": "The power of “I don’t know but I can find out”",
    "text": "The power of “I don’t know but I can find out”\nThe second day of posit::conf was spent being a teaching assistant for the fantastic Andy Teucher, who was teaching “Fundamentals of Package Development”. I managed to escape a lot of this “should” nonsense here, so when one of the course attendees asked me about where to store CSV files to use in unit tests, I happily admitted I wasn’t sure but could find out, and together we had a look in “Writing R Packages”, where I couldn’t find the information, and then suggested the approach I usually take when working on arrow - look at what folks who often define best practices do in the packages that they maintain. We took a look at readr, found the files in the testthat directory, and concluded that unless there’s reason to want to do anything more complicated (like make those datasets available to end users), then that approach is fine. Not knowing the answer instantly wasn’t a hindrance to working out the necessary solution. Learning how to say “I don’t know, but let’s find out together” is one of the most important things I learned when I was first teaching people how to use R. A teacher doesn’t have to know everything, but should be able to help a learner find the information they need."
  },
  {
    "objectID": "posts/goats/index.html#the-power-of-community",
    "href": "posts/goats/index.html#the-power-of-community",
    "title": "Three Goats in a Raincoat",
    "section": "The power of community",
    "text": "The power of community\nThe rest of the conference week was pretty awesome. I was in the middle of a CRAN resubmission (well, actually a re-re-resubmission, but let’s not go there) which had got a bit hairy, and I needed some help tracking down the source of the bug. I spent some time briefly engaging in a bit of pair programming with Jonathan Keane and Neal Richardson, who’ve both been involved with Arrow for a lot longer than me, and I’m always taken aback by just how quickly they both get to the source of a problem any time I ask for help. Part of my “knowledge anxiety” manifests as me sometimes taking longer than I would like to process things when communicating verbally about technical topics, which leads to me getting really frustrated when I’m just not “getting” things in the moment that I know later will seem pretty obvious and uncomplicated. This happened when we were looking at the arrow bug, but I had this sudden moment of clarity upon realising…nobody cared except me. Jon and Neal are great collaborators and are always happy to explain things again if there’s time, and aren’t going to judge me for it.\nI also realised that there is no universal measure of what I should know or where I should be at. While I always want to know more about how things work, I don’t have to know everything and it’s a pretty unrealistic expectation for me to have of myself; sure I can (and often do!) go read a whole load of the codebase and related concepts, but that’ll never be the same as also leaning on other people’s years of experience and knowledge. The whole point of being part of a community is to be able to share both overlapping and divergent knowledge."
  },
  {
    "objectID": "posts/goats/index.html#progress-is-power",
    "href": "posts/goats/index.html#progress-is-power",
    "title": "Three Goats in a Raincoat",
    "section": "Progress is power",
    "text": "Progress is power\nI don’t think there is a universal solution to this problem of feeling like you “should” know more, but all I know is what has worked for me. Public work is important - social media, blog posts, talks, workshops, whatever. It can be scaled to wherever you’re at. There’s always something you’ll be surprised that you know but others don’t. And it creates a lovely feedback loop whereby a little bit of external validation can go a long way.\nLearning is also important - I did the first half of the Udacity C++ Nanodegree this year, and while a lot of what it taught me was that I never want to be a full-blown C++ developer (I’d rather make weird buildings and spaceships out of existing Lego pieces than become a polymer scientist just to create custom bricks), realising that I could learn C++ at that level if I really wanted to, was invaluable. Part of the course involved getting acquainted with cmake, and whilst I can’t claim expertise there, dabbling in a bit of the whats and whys helped make many related Arrow project issues seem less mystical.\nReassurance and validation are good and well, but in my experience, having tangible proof of the things that I do know is more effective."
  },
  {
    "objectID": "posts/goats/index.html#fake-it-til-you-make-it-is-deeply-problematic",
    "href": "posts/goats/index.html#fake-it-til-you-make-it-is-deeply-problematic",
    "title": "Three Goats in a Raincoat",
    "section": "“Fake it til you make it” is deeply problematic",
    "text": "“Fake it til you make it” is deeply problematic\nThe commonly received advice is “fake it ’til you make it”. Pretend you feel like you belong and are confident, until that becomes the case. I’ve spent a long time attempting this, and the problem is that it doesn’t actually work, because it doesn’t address the underlying issue. It’s only since I started to own the fact that I don’t feel comfortable in every environment or domain and working out what I need to do to feel more comfortable that I’ve felt my confidence growing.\nIn the past month, I’ve been a lot more open about how I’ve been feeling around this. It’s been tricky as I’ve been scared that my vulnerability just looks weakness that I shouldn’t show around other people, or just come off as moaning, but it’s had massive benefits. During the first couple of days of posit::conf this year, I suffered the worst ongoing anxiety I’ve had all year - I had a constant knot in my chest - but despite that managed to have a good time as everyone was very accepting. I can say without a doubt that pretending to be fine would have made things infinitely worse.\nThere was another point during the conference when I casually mentioning that I was feeling quite hopeful about a potential opportunity but not entirely sure if it was in the bag, as I suspected I might be up against someone whose background I find impressive. It was kindly pointed out to me that each of us bring different things to the table, a comment which has since set off a chain reaction of me starting to appreciate what I can do rather than what I can’t."
  },
  {
    "objectID": "posts/goats/index.html#the-cure-for-imposter-syndrome",
    "href": "posts/goats/index.html#the-cure-for-imposter-syndrome",
    "title": "Three Goats in a Raincoat",
    "section": "The cure for imposter syndrome",
    "text": "The cure for imposter syndrome\nThe most important thing I learned this year is that the cure for imposter syndrome isn’t persuading yourself that you aren’t just 3 goats in a raincoat pretending to be a person, but instead surrounding yourself with folks who wouldn’t really care if you were anyway because goats are cool and that’s a pretty awesome feat of acrobatics and balancing."
  },
  {
    "objectID": "posts/r-examples/index.html",
    "href": "posts/r-examples/index.html",
    "title": "R package documentation - what makes a good example?",
    "section": "",
    "text": "I’m currently working on adding to the documentation of the arrow R package, and I’ve started thinking about the qualities of good examples. Specifically, I’m referring to the examples included as part of function documentation. In my experience, the best way for me to achieve rapid familiarity with an R function I haven’t worked with before, or understand how to use a function about which I already understand the basics, is by having example code that I can run. In the book ‘R Packages’, Hadley Wickham remarks that examples are “a very important part of the documentation because many people look at the examples first” and rOpenSci recommend that each of a package’s exported functions should be accompanied by examples.\nIn this blog post, I will explore the things that I believe make for good function examples in documentation, focussing mainly on R."
  },
  {
    "objectID": "posts/r-examples/index.html#what-good-looks-like-in-r",
    "href": "posts/r-examples/index.html#what-good-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like in R",
    "text": "What good looks like in R\nI asked people on Twitter for their opinions of good R package documentation in general, and Jonathan Sinclair highlighted the ‘examples’ section from dplyr::case_when, the beginning of which is shown below.\n\n\n\n(image from: https://rdrr.io/cran/dplyr/man/case_when.html)\n\n\nI think Jonathan is spot on in his assessment. To paraphrase, the highlights for him are:\n\nthere is next to no prose or intro\nthere are plenty of comments, as needed, to explain the examples\nthere is a variety of different examples\nthere are examples of what not to do.\n\nThis kind of documentation appeals to my skim-reading self. If I’m trying to accomplish a task, sometimes I just want to run some code and see what happens to get an intuitive feel for what a function does. While I am fully prepared to slow down and read the rest of the documentation, a “quick win” motivates me to invest the additional effort. It tells me that the developers of this code have prioritised making things easy to understand and that the time I am investing will pay off.\nI’ve been skimming through the documentation of some tidyverse and related packages - as I consider many of these to be well documented and easy to read. Here are some things I’ve observed which I think one can do to make function examples look great:\n\ninclude the most basic usage of a function\nuse very simple toy datasets or standard in-built datasets\ndemonstrate non-obvious behaviours of a function\ndemonstrate different parameter values/combinations where relevant\ndemonstrate any unusual parameters\ndemonstrate on different objects if appropriate\nsometimes go beyond the use of an individual function to include common mini-workflows\ngroup documentation and examples for similar functions together\ninclude examples that may lead to unexpected results\ninclude comments to explain examples\nno examples for deprecated functions to discourage their use\nno unpredictable external dependencies - rvest::html_text manually creates HTML to demonstrate capabilities rather than scraping an external site\nsometimes showing the output when it adds to the example (e.g. tidyselect::starts_with() and many other examples from that package)\nexamples should be correct and run without error (unless intended to show erroneous output)"
  },
  {
    "objectID": "posts/r-examples/index.html#what-bad-looks-like-in-r",
    "href": "posts/r-examples/index.html#what-bad-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What bad looks like in R",
    "text": "What bad looks like in R\nI am not intending to “name and shame” any package authors who haven’t included examples for their functions. It may have been overlooked, there may be plenty of explanation elsewhere, or they may have felt that the code was not sufficiently complex to require examples. It might be true that it seems obvious what a function does, but that makes assumptions about the users of your code that might not hold."
  },
  {
    "objectID": "posts/r-examples/index.html#what-good-looks-like-generally",
    "href": "posts/r-examples/index.html#what-good-looks-like-generally",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like generally",
    "text": "What good looks like generally\nWhen reading through examples, one thing that struck me is that when I’m looking at Python docs in Jupyter Notebook (press shift + tab), I also see the output of running the examples.\n\nSimilarly, both examples and outputs are shown in the official docs for some libraries, for example, pandas.\n\nI think this is a helpful feature - less effort is required to see how a function works.\nIn R function documentation, runnable code is often included, but in most cases needs to be manually run by the reader to see the output. I’m torn as to whether this is good or not. On the one hand, it encourages you to run the code and get a more tangible feel for what it does and saves valuable space in the Viewer window in RStudio. On the other hand, it adds an extra manual step to your workflow and lengthens the time until that precious “quick win” of enlightenment when exploring a new function.\nYou get a lot closer to this on the website rdrr.io, which indexes R package documentation and allows examples to be run inline. However, examples are run one after the other without the original code being displayed. So in the case of multiple examples, you have to match up the output to which example it is from.\n\n\n\n(from https://rdrr.io/cran/dplyr/man/case_when.html)\n\n\nSome packages include output as comments within their examples. For instance, the tidyselect package; here’s an example from tidyselect::all_of:\n\n\n\n(from https://rdrr.io/cran/tidyselect/man/all_of.html)\n\n\nAll that said, while the ability to see the output of examples is a nice-to-have, I don’t think it’s essential to good function documentation. With any piece of documentation, it’s necessary to consider the purpose; at a minimum, examples exist to tell the reader how to use a function, and you don’t need to see the output to do that.\nSince I first wrote this, I found out that it is possible to easily run examples from help files by selecting them and then hitting Ctrl+Enter, the same as running code in the Source pane.\n\nAnother thing I wasn’t aware of - pkgdown - commonly used to automatically render docs for packages run examples and displays the output underneath. Check out the example below from the Arrow pkgdown site.\n\nIn conclusion, good examples make functions easier to work with and help readers of your documentation gain a deeper understanding of how a function works. While any examples are better than no examples, you can give your users the best chance of success when using your code with careful thought about the content of your documentation.\nHuge thanks to everyone who responded to my Twitter thread, and to my fantastic colleague Joris Van den Bossche for reading the first draft of this, and our conversations about how things are done in R and Python."
  },
  {
    "objectID": "draft_posts/udfs/index.html",
    "href": "draft_posts/udfs/index.html",
    "title": "User-defined functions in arrow",
    "section": "",
    "text": "library(arrow)\nlibrary(dplyr)\n\nregister_scalar_function(\n  name = \"add_one\",\n  function(context, trip_distance) {\n    trip_distance + 1\n  },\n  in_type = schema(\n    trip_distance = float64()\n  ),\n  out_type = float64(),\n  auto_convert = TRUE\n)\n\n\nresults &lt;- bench::mark(\n  compute = open_dataset(\"~/data/nyc-taxi\") |&gt;\n    filter(year == 2019, month == 9) |&gt;\n    transmute(x = trip_distance + 1) |&gt;\n    collect(),\n  udf = open_dataset(\"~/data/nyc-taxi\") |&gt;\n    filter(year == 2019, month == 9) |&gt;\n    transmute(x = add_one(trip_distance)) |&gt;\n    collect(),\n  iterations = 10,\n  check = FALSE\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n\nresults\n\n# A tibble: 2 × 13\n  expression min        median     `itr/sec` mem_alloc  `gc/sec` n_itr  n_gc\n  &lt;bnch_xpr&gt; &lt;bench_tm&gt; &lt;bench_tm&gt;     &lt;dbl&gt; &lt;bnch_byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 &lt;language&gt; 0.6321745  0.6625475      1.49   6372712      0.744    10     5\n2 &lt;language&gt; 1.3555642  1.4408504      0.690 55933824      7.11     10   103\n# ℹ 5 more variables: total_time &lt;bench_tm&gt;, result &lt;list&gt;, memory &lt;list&gt;,\n#   time &lt;list&gt;, gc &lt;list&gt;\n\n\nThe time it took when using Arrow’s in-built compute function was about hald the time it took using a UDF. Crucially, there was significantly more memory allocated by R when using the UDF, as well as more garbage collections performed, which leads me to conclude that the UDF is being run after the results have been pulled back into R."
  },
  {
    "objectID": "talks/nyhackr/index.html",
    "href": "talks/nyhackr/index.html",
    "title": "Efficiently Engineering Bigger Data with Arrow",
    "section": "",
    "text": "Data analysis pipelines with larger-than-memory data are becoming more and more commonplace. There are often blurred lines between data science and data engineering, and knowing a bit of both is a sure-fire way to make your life easier when working with big datasets. In this talk, I will give an overview of the arrow R package and best practices for getting the most out of your data when working with bigger datasets. I’ll demo the dplyr interface to arrow, and give you some tips and tricks for getting the most out of arrow’s functionality, as well as applying data engineering principles to speed things up even more."
  },
  {
    "objectID": "talks/rstudioconf2019/index.html",
    "href": "talks/rstudioconf2019/index.html",
    "title": "The future’s Shiny: Pioneering genomic medicine in R",
    "section": "",
    "text": "Shiny's expanding capabilities are rapidly transforming how it is used in an enterprise. This talk details the creation of a large-scale application, supporting hundreds of concurrent users, making use of the future and promises packages. The 100,000 genomes project is an ambitious exercise that follows on from the Human Genome Project - aiming to put the UK at the forefront of genomic medicine, with the NHS as the first health service in the world to offer precision medicine to patients with rare diseases and cancer. Data is at the heart of this project; not only the outputs of the genomic sequencing, but vast amounts of metadata used to track progress against the 100,000 genome target and the status and path of each case through the sample tracking pipeline. In order to make this data readily available to stakeholders, Shiny was used to create an application containing multiple interactive dashboards. A scaled-up version of the app is being rolled out in early 2019 to a much larger audience to support the National Genomics Informatics Service, with the challenge of creating a complex app capable of supporting so many users without grinding to a halt. In this talk, I will explain why Shiny was the obvious technology choice for this task, and discuss the design decisions which enabled this project’s success."
  },
  {
    "objectID": "talks/rstudioconf2022/index.html",
    "href": "talks/rstudioconf2022/index.html",
    "title": "What they forgot to teach you about becoming an open source contributor",
    "section": "",
    "text": "Getting involved in open source is an amazing learning experience and helps you grow your skills as a developer, but to a new contributor there are so many unknown factors about open source projects. In this talk, I’m going to discuss my journey from occasional open source contributor to full time project maintainer, and answer questions such as: what does it look like from the inside of an open-source project? What’s a good way to get involved, and really learn the internals of an R package? How can I navigate the social dynamics of an open source project? How do contributions look entirely different from the point of view of a contributor versus a maintainer?"
  },
  {
    "objectID": "projects/phd/index.html",
    "href": "projects/phd/index.html",
    "title": "PhD Thesis - Debiasing Reasoning: A Signal Detection Analysis",
    "section": "",
    "text": "Download here: PDF, 2.1MB\nThis thesis focuses on deductive reasoning and how the belief bias effect can be reduced or ameliorated.\nBelief bias is a phenomenon whereby the evaluation of the logical validity of an argument is skewed by the degree to which the reasoner believes the conclusion. There has been little research examining ways of reducing such bias and whether there is some sort of effective intervention which makes people reason more on the basis of logic. Traditional analyses of this data has focussed on simple measures of accuracy, typically deducting the number of incorrect answers from the number of correct answers to give an accuracy score. However, recent theoretical developments have shown that this approach fails to separate reasoning biases and response biases. A reasoning bias, is one which affects individuals’ ability to discriminate between valid and invalid arguments, whereas a response bias is simply the individual’s tendency to give a particular answer, independent of reasoning. A Signal Detection Theory (SDT) approach is used to calculate measures of reasoning accuracy and response bias. These measures are then analysed using mixed effects models.\nChapter 1 gives a general introduction to the topic, and outlines the content of subsequent chapters. In Chapter 2, I review the psychological literature around belief bias, the growth of the use of SDT models, and approaches to reducing bias. Chapter 3 covers the methodology, and includes a a thorough description of the calculation of the SDT measures, and an explanation of the mixed effects models I used to analyse these. Chapter 4 presents an experiment in which the effects of feedback on reducing belief bias is examined. In Chapter 5, the focus shifts in the direction of individual differences, and looks at the effect of different instructions given to participants, and Chapter 6 examines the effects of both feedback and specific training. Chapter 7 provides a general discussion of the implications of the previous three chapters."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Reflections on 2023\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nNic Crane\n\n\n\n\n\n\n  \n\n\n\n\nDebugging\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nNic Crane\n\n\n\n\n\n\n  \n\n\n\n\nThree Goats in a Raincoat\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNic Crane\n\n\n\n\n\n\n  \n\n\n\n\nR package documentation - what makes a good example?\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\nNic Crane\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts/complexcodebases/index.html",
    "href": "drafts/complexcodebases/index.html",
    "title": "Getting to Grips with Complex Codebases",
    "section": "",
    "text": "Getting to grips with a large and complex codebase can be an intimidating task. When I first started working on Apache Arrow, I found it pretty overwhelming - the GitHub repo for the project contains the code for multiple Arrow implementations. Even when I narrowed things down to only the R package, there was still a lot to deal with. After some time I developed some strategies that I always use now when dealing with new codebases, and I’m writing this post to share these ideas."
  },
  {
    "objectID": "drafts/complexcodebases/index.html#make-notes.-lots-of-notes.",
    "href": "drafts/complexcodebases/index.html#make-notes.-lots-of-notes.",
    "title": "Getting to Grips with Complex Codebases",
    "section": "1. Make notes. Lots of notes.",
    "text": "1. Make notes. Lots of notes.\nWhen ingested large amounts of information, there will inevitably more than you can store and process at once, and so getting it down on paper can help. I find it useful to make notes with diagrams, or descriptions of things with indented descriptions to represent functions within functions. The most important thing here is to have no expectation that these notes be tidy or shareable. You can come back and make a revised version later, but your only priority in this raw version is helping the thinking process. I also find it helpful to write down questions I have - then, even if I disappear down a tangent or rabbit hole, I know how I got there and can easily loop back to my main train of thought later.\nA side-effect of this kind of writing is that it sometimes produces excellent docs. At no other point do you know less about this code, and so are the person most equipped to be teaching other newcomers and asking the same kinds of questions they’ll have. Don’t get too attached to your notes turning into docs though, as this can be a distraction, but do keep it in mind.\nI tend to start from the top, and try to find answers to questions like:\n\nwhat are the most important functions in this package? To answer this, I usually check out the README on the GitHub repo, the reference page on the pkgdown site, and any vignettes)\nwhat does this code look like at a high level? Where is the most complexity? What kinda of structures are used or passed around in the code?\n\nAs a bonus, if there is someone more knowledgeable around, you could get them to look over the cleaned-up version of your notes to give you feedback on how accurate they are, and if you’re missing anything. I only realistically do this about 5% of the time."
  },
  {
    "objectID": "drafts/complexcodebases/index.html#find-a-single-thing-to-fix-or-understand",
    "href": "drafts/complexcodebases/index.html#find-a-single-thing-to-fix-or-understand",
    "title": "Getting to Grips with Complex Codebases",
    "section": "2. Find a single thing to fix or understand",
    "text": "2. Find a single thing to fix or understand\nA top-down view has a certain amount of utility in getting a broad idea of the codebase, but it can still be overwhelming at this point. Finding a single bug to fix, issue to investigate, or even just concept to understand, can make it easier to zoom in on one place. Once you’ve worked this out, you can branch outwards to other areas of the code. When working on Arrow, I spent a lot of time focused on the bindings to dplyr functions, before I even looked at how the file-reading functionality worked. Getting some specialism in an area before expanding out can help build confidence too."
  },
  {
    "objectID": "drafts/complexcodebases/index.html#run-the-unit-tests",
    "href": "drafts/complexcodebases/index.html#run-the-unit-tests",
    "title": "Getting to Grips with Complex Codebases",
    "section": "3. Run the unit tests",
    "text": "3. Run the unit tests\nSometimes you’ll need to understand functions which don’t have many examples in the documentation, but are still crucial to understanding how the package works. In this case, it can be helpful to take a look at the unit tests for the functions. This can help you get a better idea of expected inputs and outputs, but also the kinds of things which result in errors from incorrect usage."
  },
  {
    "objectID": "drafts/complexcodebases/index.html#step-through-the-code-one-line-at-a-time",
    "href": "drafts/complexcodebases/index.html#step-through-the-code-one-line-at-a-time",
    "title": "Getting to Grips with Complex Codebases",
    "section": "4. Step through the code one line at a time",
    "text": "4. Step through the code one line at a time\nSo you found a function you want to understand better, ran some unit tests to get an idea of typical behaviour, but there’s still a lot more to learn, so now what? At this point I like to use the debugger to run through the code one line at a time. In R, there are a few ways of doing this - you can insert a call to browser() at the point you want to step into the debugger, or you can wrap the function call in debug() or debugonce() to start from the beginning. Here, I’ll step through the code line-by-line, often printing out the values of variables repeatedly to see how they change at each step. I also like to draw diagrams of what calls what, with some functions starting off as “black boxes” and then extra detail being added as I step through those. I’ll write plain language description of the functions, either as comments in the codebase, or written on paper with line numbers next to my notes. These descriptions tend to be indented, following the structure of the code, so I can start to build my own mental model of the shape of it."
  },
  {
    "objectID": "drafts/complexcodebases/index.html#using-analogy---finding-other-bits-of-code-that-do-the-same-thing",
    "href": "drafts/complexcodebases/index.html#using-analogy---finding-other-bits-of-code-that-do-the-same-thing",
    "title": "Getting to Grips with Complex Codebases",
    "section": "5. Using analogy - finding other bits of code that do the same thing",
    "text": "5. Using analogy - finding other bits of code that do the same thing\nUnderstanding every line of code can be helpful, but what if there’s just too much, or it’s in a completely unfamiliar language? One of my first PRs to the Arrow codebase was moving some C++ code from the Arrow R package to the Arrow C++ codebase. I’d never written a single line of C++ in my life, and didn’t really understand it in great depath. However, I could see similar things nearby, and focused on reading the variable names and seeing what went where. Another time anaology has come in useful has been looking at other PRs that do similar things."
  },
  {
    "objectID": "drafts/complexcodebases/index.html#just-make-a-pr",
    "href": "drafts/complexcodebases/index.html#just-make-a-pr",
    "title": "Getting to Grips with Complex Codebases",
    "section": "6. Just make a PR",
    "text": "6. Just make a PR\nCode review is a conversation. Sometimes I’ve tried everything I can think of, and made some progress, but ended up completely stuck. I’ve gotten comfortable with the idea that if I can show and explain my working (i.e. “I tried X because I thought Y, but not I’m not sure if Z is a better approach”), then that can be enough to get a reasonable PR together and wait for feedback. While it’s important to do the work of reading up and exploring the code first, there’s a point at which there are diminishing returns for pushing on when I’m entirely stuck. A pull request is a conversation, and presenting an initial approach which ends up being revised is no terrible thing.\nI’d love to hear from you - is there anything in this post you’re going to try? What other approaches do you recommend? Let me know on Mastodon or LinkedIn!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nic Crane, PhD",
    "section": "",
    "text": "Hi, I’m Nic! I’m a data scientist, software engineer, and R enthusiast. I am part of the team who maintain the arrow R package."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "PhD Thesis - Debiasing Reasoning: A Signal Detection Analysis\n\n\n\n\n\n\nNic Crane\n\n\nJul 22, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/positconf2023/index.html",
    "href": "talks/positconf2023/index.html",
    "title": "Big Data in R with Arrow",
    "section": "",
    "text": "A workshop on Arrow that Stephanie Hazlitt and I wrote and presented at posit::conf 2023.\nCourse homepage"
  },
  {
    "objectID": "talks/positconf2023/index.html#overview",
    "href": "talks/positconf2023/index.html#overview",
    "title": "Big Data in R with Arrow",
    "section": "Overview",
    "text": "Overview\nData analysis pipelines with larger-than-memory data are becoming more and more commonplace. In this workshop you will learn how to use Apache Arrow, a multi-language toolbox for working with larger-than-memory tabular data, to create seamless “big” data analysis pipelines with R.\nThe workshop will focus on using the the arrow R package—a mature R interface to Apache Arrow—to process larger-than-memory files and multi-file datasets with arrow using familiar dplyr syntax. You’ll learn to create and use interoperable data file formats like Parquet for efficient data storage and access, with data stored both on disk and in the cloud, and also how to exercise fine control over data types to avoid common large data pipeline problems. This workshop will provide a foundation for using Arrow, giving you access to a powerful suite of tools for performant analysis of larger-than-memory data in R.\nThis course is for you if you:\n\nwant to learn how to work with tabular data that is too large to fit in memory using existing R and tidyverse syntax implemented in Arrow\nwant to learn about Parquet and other file formats that are powerful alternatives to CSV files\nwant to learn how to engineer your tabular data storage for more performant access and analysis with Apache Arrow"
  },
  {
    "objectID": "talks/londonr/index.html",
    "href": "talks/londonr/index.html",
    "title": "Ten Steps To Becoming A Tidyverse Contributor",
    "section": "",
    "text": "A talk about getting involved in open source!\nSlides here"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks and Workshops",
    "section": "",
    "text": "Big Data in R with Arrow\n\n\n\n\n\n\nNic Crane\n\n\nSep 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiently Engineering Bigger Data with Arrow\n\n\n\n\n\n\nNic Crane\n\n\nAug 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat they forgot to teach you about becoming an open source contributor\n\n\n\n\n\n\nNic Crane\n\n\nJul 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe future’s Shiny: Pioneering genomic medicine in R\n\n\n\n\n\n\nNic Crane\n\n\nJan 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTen Steps To Becoming A Tidyverse Contributor\n\n\n\n\n\n\nNic Crane\n\n\nNov 27, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023wrapup/index.html",
    "href": "posts/2023wrapup/index.html",
    "title": "Reflections on 2023",
    "section": "",
    "text": "Here’s a summary of my highlights of 2023!"
  },
  {
    "objectID": "posts/2023wrapup/index.html#arrow-positconf-workshop",
    "href": "posts/2023wrapup/index.html#arrow-positconf-workshop",
    "title": "Reflections on 2023",
    "section": "Arrow posit::conf workshop",
    "text": "Arrow posit::conf workshop\nThe biggest professional achievement for me was having the privilege of being invited to co-present a workshop with Steph Hazlitt about Arrow at posit::conf. Steph is fantastic to work with, and while writing and delivering the workshop was hard work, it paid off in the end - we received really positive feedback from attendees. The experience reignited my passion for teaching, and desire to develop those skills. I’ve applied for Carpentries instructor training, which I’m still waiting to hear back about, and I’m excited to also be working with Steph on creating a 2 hour version of the workshop, which we’re planning on teaching at user events in 2024.\n\n\n\nHanging out with some awesome people at posit::conf 2023!"
  },
  {
    "objectID": "posts/2023wrapup/index.html#arrow-r-maintenance-bot",
    "href": "posts/2023wrapup/index.html#arrow-r-maintenance-bot",
    "title": "Reflections on 2023",
    "section": "Arrow R Maintenance bot",
    "text": "Arrow R Maintenance bot\nAnother project I enjoyed working on was creating the Arrow maintenance bot. It pulls data from the GitHub API, aggregates the relevant bits, and then posts it to a Zulip channel (though also works for Slack). It was an itch I’d wanted to scratch for a long time, as I’d previously got this information by clicking on various bookmarks every so often whenever I remembered. The output is shown in the screenshot below; basically, it summarises new bug tickets which need investigating, investigated bugs which need a fix, and open PRs which may need reviewing. \nThere are more aspects of arrow maintenance which I’d love to automate and pull together in dashboard form - I’ve been discussing this with a friend, and I hope we find the time to work on this in 2024!"
  },
  {
    "objectID": "posts/2023wrapup/index.html#arrow-maintenance-and-book",
    "href": "posts/2023wrapup/index.html#arrow-maintenance-and-book",
    "title": "Reflections on 2023",
    "section": "Arrow maintenance and book",
    "text": "Arrow maintenance and book\nIn January, I became the official package maintainer on CRAN. It took my some time to get my head around this; it’s a daunting responsibility, and in reality I’m part of a wider team, all of whom play an important part.\nThe amount of time I’ve been able to spend on Arrow has dropped significantly since I left Voltron Data, though I’m hoping to be able to spend more time this year on it. I feel like my focus will shift a little, away from code, and more towards encouraging new contributors, as well as teaching people how to use the package.\nMost excitingly though, is the book which I am co-authoring! This is taking up the main chunk of my Arrow-related time, but the aim is to have the manuscript submitted to the publishers by summer, if not sooner."
  },
  {
    "objectID": "posts/2023wrapup/index.html#c",
    "href": "posts/2023wrapup/index.html#c",
    "title": "Reflections on 2023",
    "section": "C++",
    "text": "C++\nHaving interacted with bits of C++ code while working on Arrow, I decided to spend time learning C++ more formally. I’ve generally found this tricky - frankly, higher-level languages seem to have more engaging content out there. I made a start on the Udacity C++ Nanodegree, which I completed over half of, but was limited on time and so paused this - I plan to pick it up again next year though. I also tried a more practical approach, and decided to make a PR to the Arrow C++ codebase - one of the PRs I am most proud of this year is this one, adding support for the Substrait cast expression. It was hard, though most of the complexity came from peripheral tasks, like working out how to run the code interactively in the debugger."
  },
  {
    "objectID": "posts/2023wrapup/index.html#joining-the-rladies-global-team",
    "href": "posts/2023wrapup/index.html#joining-the-rladies-global-team",
    "title": "Reflections on 2023",
    "section": "Joining the RLadies Global team",
    "text": "Joining the RLadies Global team\nI spent some time thinking about how I could engage more with the wider R community, and so I volunteered to join the RLadies global team, helping set up new chapters on meetup.com. I’ve not done much here yet, but am delighted to be part of the organisation, and hope to be able to help out more in 2024"
  },
  {
    "objectID": "posts/2023wrapup/index.html#career-break-and-plans-for-2024-recurse-center",
    "href": "posts/2023wrapup/index.html#career-break-and-plans-for-2024-recurse-center",
    "title": "Reflections on 2023",
    "section": "Career break and plans for 2024 (Recurse Center!)",
    "text": "Career break and plans for 2024 (Recurse Center!)\nLast, but absolutely not least, is my decision to take a career break. After leaving Voltron Data, I’d been working part-time in a contractor capacity, alongside working on the Arrow book, but I decided I wanted to narrow my focus and take some time out for me.\nI’m taking a break and spending the next couple of months focusing on writing. Once most of the book is done, I plan to spend more time on my open source work in general, which will involve teaching and talking about Arrow, as well as pushing forward a few side projects.\nThe most exciting news I’ve had this year is that I applied to the Recurse Center a few weeks ago. RC is like a writer’s retreat for programmers, and attendees spend 12 weeks together working on programming problems of their choice. I want to get to grips with R’s internals, and learn about the C API, as well as levelling up my C++ skills in general. I’m delighted to say that I was accepted to attend this morning, and will be joining the spring batch in late March! I’ve got a ton of ideas for projects I want to work on, but will have to see how much time I ultimately end up having.\nLonger-term - I don’t know. Ultimately, I’d love to get paid to work on open source again, but those opportunities are few and far between. I also have some professional goals I’d like to hit before I’m going to be in the right headspace to engage with this too - between writing the book and my plans for Recurse Center, I’ll be there pretty soon.\nThis year has been a real rollercoaster, but I’m extremely grateful to all my friends and mentors who have helped me during this time."
  },
  {
    "objectID": "posts/debugging/index.html",
    "href": "posts/debugging/index.html",
    "title": "Debugging",
    "section": "",
    "text": "As a package maintainer, I’m constantly disappointed when folks mention Arrow bugs they’re aware of but haven’t reported. Not disappointed with the individual in question, but disappointed with the fact that we’re not at the point where we’ve created an environment in which folks are happy to just report bugs immediately. This is not an Arrow-specific problem, and I find myself behaving in exactly the same way with other open source projects. If I’m not entirely sure something is a bug, I’m not going to risk looking foolish publicly by complaining, but the irony of this is that I don’t judge users who make those mistakes with Arrow, as usually it means we need to improve our docs to be more clear, and that in itself is valuable feedback.\nI really love interacting with people who use the package, as without that interaction, development and maintenance can feel like shouting into the void. I like being able to solve problems that make other people’s lives easier, and I thrive off that social energy. I implemented bindings for dplyr::across() because someone commented on Twitter that they’d love to see it as a feature. Last night I got home from a friend’s birthday drinks, saw a user question on Stack Overflow which had an easy win, and within an hour had a pull request up which implemented a new function and fixed the particular issue. I am not promising this level of responsiveness in perpetuity, but I’m still at the point where I find this kind of thing exciting and energising.\nOne particular bug which has haunted me for the past 6 months, is a particularly irritating one whereby when working with a partitioned CSV dataset in arrow (I’m using lowercase to denote the R package, rather than Arrow the overall project), if the user manually specified a schema, the variable used for partitioning did not appear in the resulting dataset. This is a huge problem IMO - while we can sit here all day talking about the virtues of Parquet, in reality, a lot of our users will be using CSVs, and it’s issues like these that can rule out being able to use arrow at all in some cases.\nWhen I opened the original issue based on another user issue, I knew it was important, but felt a bit stuck. It wasn’t immediately obvious to anyone what the source of the error was. I’d assumed it must be a C++ error and flagged it as such, but nobody had taken a look at it, and I’m always hesitant to mindlessly tag folks on issues when I don’t feel like I’ve done the work to investigate (though to be fair, didn’t really know what “the work” should be in this case).\nI’d ended up assuming that this bit of functionality just didn’t work with CSV datasets, and had been working around it, until I was presenting about arrow at New York Open Statistical Programming Meetup, and someone asked about it again. I take 1 user question as representative of 99 other people with the same issue who aren’t being so vocal about it, and felt like it needed to be fixed. I am unashamed to admit that I occasionally have the taste for a bit of melodrama, and publicly declared it to a few of my fellow contributors as “my white whale”, and so set out to find the source, even if it required me to delve deep into the guts of Arrow’s C++ libraries, a task which can often send me down endless rabbit holes and chasing red herrings (this sentence has become quite the nature park…)\nMy original exploration didn’t result in much useful - the arrow package does some cool things with R6 objects to binding them to Arrow C++ objects, but accessing the inner properties of these bound objects would mean manually creating print methods for every single one of them, and when you don’t know in which class the problem lies, this becomes, frankly, a massive pain in the arse. I still didn’t have enough to go on to take it to an Arrow C++ contributor and ask for their help, but showing I’d done some of “the work” to at least make an effort myself.\nAnd then collaborative debugging saved me! I had a catchup with the fantastic Dane Pitkin, and I asked for his help just walking through the problem. Dane’s main contributions to Arrow have been to Python, though he has a ton of previous C++ experience, even if he isn’t a regular contributor to the Arrow C++ library. I walked through the problem with him, and the steps I’d taken so far to try to figure things out, and the fact that I still needed to figure out if the problem was in R or C++. Dane commented that the object bindings we’d been looking at had little surface area for the problem to be in R - most of them were straight-up mappings from the C++ object to an R6 object with no extension. This was my first big clue! I remembered that there’s a bit of open_dataset() where we do some manual reconfiguration of specified options, which involves a whole load of R code - something I’ll come back to later. Dane also suggested I check out Stack Overflow to see if people were complaining about the issue in C++ there. I was sceptical that I’d find anything - lots of these bugs are more often surfaced in the R and Python libraries - but realised that this wasn’t the dead end that I’d thought. It suddenly occurred to me that if I could reproduce the bug in PyArrow, then the problem must lie in the C++ library, but if I couldn’t, then the problem lay in the R code.\nFifteen minutes later, and I had confirmed it was an R problem. I happened to mention on Slack the problem I was having, steps I’d taken so far to investigate, and potential ideas to look at next, and ended up engaging in a bit more collaborative debugging, this time with the wonderful Dewey Dunnington, who mentioned more disparities between PyArrow and R in terms of how we construct schemas, which put me on the path of testing the schema values at different points in Dataset creation and able to rule that out. At that point, with a smaller problem space to explore, the only logical thing left to look into was the R code which sets up the options for the various Arrow classes, and I ended up spotting the rogue instantiation of CSVReadOptions which just needed to have the partitioning column excluded (it relates to the reading in of the individual files which make up the dataset, and so has no “knowledge” of the partitions, and so previously raised an error as it treated them as an extraneous column).\nOne pull request later, and the problem that I’d given myself a week to look at had been solved in less than a day! This is probably one of the most gratifying bugs I’d worked on all year; there was a user with a problem to solve, a bug which had been annoying me for ages, the chance to fall into the puzzle-like aspects of debugging, and some great opportunities for collaboration with folks whose help here I really appreciated. This is one of the things I enjoy most about being a software engineer; this process of starting off feeling entirely clueless about something, and having to work out where I need to be and how I’m going to get there, and then doing it. Actually, in the abstract, that’s probably one of the things I enjoy most about being a human :)"
  }
]