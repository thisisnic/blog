[
  {
    "objectID": "posts/show-exec-plan/index.html",
    "href": "posts/show-exec-plan/index.html",
    "title": "Arrow New Feature Showcase: show_exec_plan()",
    "section": "",
    "text": "The arrow package allows you to take advantage of the power of the Acero execution engine for data manipulation and analysis. The code in arrow provides bindings to dplyr verbs and tidyverse functions, so that you can use these interfaces without having to understand the inner workings of Acero. But what if you actually want to know more?\nIn the latest release of arrow, version 9.0.0, the show_exec_plan() function is introduced. This function allows you to see the execution plan generated from your code. For example:\nThis functionality is similar to that of dplyr::explain() and you’ll get the same result whether you call arrow::show_exec_plan(), dplyr::explain() or dplyr::show_query() on an arrow_dplyr_query object.\nShout out to Dragoș Moldovan-Grünfeld who made the PR to implement this function!\nSee sections below for definitions of the terms mentioned above."
  },
  {
    "objectID": "posts/show-exec-plan/index.html#what-is-acero",
    "href": "posts/show-exec-plan/index.html#what-is-acero",
    "title": "Arrow New Feature Showcase: show_exec_plan()",
    "section": "What is Acero?",
    "text": "What is Acero?\nAcero is an Arrow-native query execution engine, developed as part of the Arrow C++ / R libraries."
  },
  {
    "objectID": "posts/show-exec-plan/index.html#what-is-a-query-execution-engine",
    "href": "posts/show-exec-plan/index.html#what-is-a-query-execution-engine",
    "title": "Arrow New Feature Showcase: show_exec_plan()",
    "section": "What is a query execution engine?",
    "text": "What is a query execution engine?\nA query execution engine is a piece of software which allows users to execute queries against a data source. It typically has multiple steps, which could include things like taking the query and parsing it into an algebraic format, re-ordering and optimising the query in order to run it in the most efficient way, and actually running the query."
  },
  {
    "objectID": "posts/show-exec-plan/index.html#what-is-an-execplan",
    "href": "posts/show-exec-plan/index.html#what-is-an-execplan",
    "title": "Arrow New Feature Showcase: show_exec_plan()",
    "section": "What is an ExecPlan?",
    "text": "What is an ExecPlan?\nQueries to be run on Acero are specified as execution plans aka ExecPlans. These are directed graphs which express what operations need to take place via nodes - called ExecNodes in this case. The graph for the ExecPlan in the code example earlier in this post looks like this:\nTableSourceNode -&gt; FilterNode -&gt; ProjectNode -&gt; SinkNode\nThe ExecNodes in the plan above are:\n\nTableSourceNode - used to input the data\nFilterNode - where we apply our filter to only retain rows where the value of mpg is greater than 20\nProjectNode - this then specifies which columns we want, including the new column wt_kg as a compute expression\nSinkNode - this provides the output"
  },
  {
    "objectID": "posts/shiny-confirm-popup/index.html",
    "href": "posts/shiny-confirm-popup/index.html",
    "title": "Pop-up to confirm action in Shiny",
    "section": "",
    "text": "It’s a gorgeous evening in Bath tonight, and my evening has been improved by finding a free table outside at the pub, a large glass of Sauvignon Blanc, and working out how to include a JavaScript confirmation box in Shiny. Here, I’m using Shiny dashboard, although this methodology will work fine with other UI layouts.\nIn the UI file, near the top of the dashboardBody, call tags$head and tags$script to give the source of the JavaScript file:\ntags$head(\n  tags$script(src = 'custom.js')\n),\nThen, in a file named custom.js as above, within the www folder, include the following code. Here, the button which the user clicks on has the inputId set to “submit”.\n$(document).on('shiny:inputchanged', function(event) {\n\n  if (event.name === 'submit') {\n\n    var r = confirm(\"Are you sure you want to continue?\");\n\n    if (r === false) {\n      event.preventDefault();\n    }\n\n  }\n\n});\nWant to know more about JavaScript events in Shiny? Visit this article on the RStudio website for a comprehensive guide: http://shiny.rstudio.com/articles/js-events.html"
  },
  {
    "objectID": "posts/goats/index.html",
    "href": "posts/goats/index.html",
    "title": "Three Goats in a Raincoat",
    "section": "",
    "text": "I am not a fan of the term “imposter syndrome.” It’s a lazy way of ignoring the complex and interconnected, often structural reasons that contribute to many people’s feelings, which manifests itself in a variety of deeply idiosyncratic ways. But I guess the tl;dr is that this is what this post is about.\nI am not someone who has “hobbies” as such; I have interests that I adopt with great enthusiasm, pursue obsessively for a short period of time, often declaring that I have found a lifelong fascination, and then drop once the initial feeling of “endless possibility” has worn off and something else shiny and new finds it way across my path. I suppose my hobby is “pursuing novelty.” The 2022-2023 edition of this came in the form of tabletop role-playing games, and earlier this year, I found myself sitting with some good friends, beers and snacks, working out how my character, “Ach!”, a small goat with a fondness for fainting at inappropriate moments could make their way into a human party undetected.\nThe solution for Ach! and friends was to don a disguise and stand on each others’ shoulders, and this image of 3 goats in a raincoat trying to sneak into a human party perfectly reflects how I feel about myself sometimes as an open source maintainer.\n(Here, I need to get round to commissioning an image of 3 goats in a raincoat standing on each other’s shoulders. The top one has black and red braids and is wearing a hat and fake beard/moustache)\nI am the current maintainer of the arrow R package, which in reality just means I am the person who is listed as such in the package DESCRIPTION file, and have the responsibility of clicking the link in the confirmation email from CRAN when someone submits the latest version of of the package to CRAN. Of course, I do more than just this; I triage bugs, submit patches, review PRs, and occasionally implement new features, but so do other people. While for career-serving purposes, I can technically claim to be “the” maintainer of the package, honestly, I am “a” maintainer and part of a team. This isn’t always reflected in other people’s language, and results in weird moments where people say things to me like, “ah, you wrote arrow!” and I have to respond with, “I absolutely did not, though I did write a lot of the bindings for dplyr::across!”, a topic which subsequently interests me more than it does them."
  },
  {
    "objectID": "posts/goats/index.html#the-tyranny-of-should",
    "href": "posts/goats/index.html#the-tyranny-of-should",
    "title": "Three Goats in a Raincoat",
    "section": "The tyranny of “should”",
    "text": "The tyranny of “should”\nI’ve spent much of this year suffering from the tyranny of “should” regarding my own knowledge and abilities, in a way that has been verging on ridiculous. A lot of it is the perfect storm of adjusting to new responsibility, the deeply unhelpful way in which my self-image when I was younger was built up around being “smart” and “knowing things”, and a knack for punching above my weight by being good at figuring out just enough about a topic to get shit done. I don’t know every single detail of object orientation in R, but I know enough to be able to create new bindings between C++ objects and R6 classes in the arrow R package and expose their methods via fields and active bindings. Despite having looked it up repeatedly over the past few years, I still cannot recall the difference between how things work with static and shared libraries, but I did manage to fix a bug requiring me to bump the version of the C++ date library that Arrow vendors, via inspecting old pull requests and helpful feedback I received on the PR I submitted.\nIt came to a head at this year’s posit::conf, where I had the privilege of co-teaching a workshop about Arrow with the wonderful Stephanie Hazlitt. We spent months preparing materials and practicing teaching to our laptop screens, but when the event itself rolled around, teaching it to a room just shy of 60 people was a different experience entirely. It was intimidating being in front of a room of eager learners, and any jokey asides I was usually capable of making went out of the window, and I just taught the materials fairly plainly, stumbling over my words a bit, and finding the moments of silence in the room as I was teaching pretty awful. Subsequent feedback from folks I trust enough to deliver at least some honesty alongside their validation has led me to conclude that the only person who really noticed or cared about this was me. I was doing my old trick of snatching defeat from the jaws of victory, and realised later that my self-expectations were totally unreasonable. While I used to be able to absolutely smash through delivering a well-practiced 2-day “introduction to R” course, and delighted in engaging the folks in the room and showing them what they (and R!) were capable of, it’s 5 years since I last did that on the regular. This was the first time these materials had seen the light of day, and at no less than posit::conf, daunting enough on its own. And honestly, this is all ego. We ran a good workshop, people were engaged, and we wrote some quality reusable materials. Yes, there’s room for improvement, but overall it was solid. Delivery of content is something I can (and will) work on. I’ve applied for The Carpentries instructor training, and have plans to get back into the swing of teaching by running workshops (on Arrow and other topics) at user events."
  },
  {
    "objectID": "posts/goats/index.html#the-power-of-i-dont-know-but-i-can-find-out",
    "href": "posts/goats/index.html#the-power-of-i-dont-know-but-i-can-find-out",
    "title": "Three Goats in a Raincoat",
    "section": "The power of “I don’t know but I can find out”",
    "text": "The power of “I don’t know but I can find out”\nThe second day of posit::conf was spent being a teaching assistant for the fantastic Andy Teucher, who was teaching “Fundamentals of Package Development”. I managed to escape a lot of this “should” nonsense here, so when one of the course attendees asked me about where to store CSV files to use in unit tests, I happily admitted I wasn’t sure but could find out, and together we had a look in “Writing R Packages”, where I couldn’t find the information, and then suggested the approach I usually take when working on arrow - look at what folks who often define best practices do in the packages that they maintain. We took a look at readr, found the files in the testthat directory, and concluded that unless there’s reason to want to do anything more complicated (like make those datasets available to end users), then that approach is fine. Not knowing the answer instantly wasn’t a hindrance to working out the necessary solution. Learning how to say “I don’t know, but let’s find out together” is one of the most important things I learned when I was first teaching people how to use R. A teacher doesn’t have to know everything, but should be able to help a learner find the information they need."
  },
  {
    "objectID": "posts/goats/index.html#the-power-of-community",
    "href": "posts/goats/index.html#the-power-of-community",
    "title": "Three Goats in a Raincoat",
    "section": "The power of community",
    "text": "The power of community\nThe rest of the conference week was pretty awesome. I was in the middle of a CRAN resubmission (well, actually a re-re-resubmission, but let’s not go there) which had got a bit hairy, and I needed some help tracking down the source of the bug. I spent some time briefly engaging in a bit of pair programming with Jonathan Keane and Neal Richardson, who’ve both been involved with Arrow for a lot longer than me, and I’m always taken aback by just how quickly they both get to the source of a problem any time I ask for help. Part of my “knowledge anxiety” manifests as me sometimes taking longer than I would like to process things when communicating verbally about technical topics, which leads to me getting really frustrated when I’m just not “getting” things in the moment that I know later will seem pretty obvious and uncomplicated. This happened when we were looking at the arrow bug, but I had this sudden moment of clarity upon realising…nobody cared except me. Jon and Neal are great collaborators and are always happy to explain things again if there’s time, and aren’t going to judge me for it.\nI also realised that there is no universal measure of what I should know or where I should be at. While I always want to know more about how things work, I don’t have to know everything and it’s a pretty unrealistic expectation for me to have of myself; sure I can (and often do!) go read a whole load of the codebase and related concepts, but that’ll never be the same as also leaning on other people’s years of experience and knowledge. The whole point of being part of a community is to be able to share both overlapping and divergent knowledge."
  },
  {
    "objectID": "posts/goats/index.html#progress-is-power",
    "href": "posts/goats/index.html#progress-is-power",
    "title": "Three Goats in a Raincoat",
    "section": "Progress is power",
    "text": "Progress is power\nI don’t think there is a universal solution to this problem of feeling like you “should” know more, but all I know is what has worked for me. Public work is important - social media, blog posts, talks, workshops, whatever. It can be scaled to wherever you’re at. There’s always something you’ll be surprised that you know but others don’t. And it creates a lovely feedback loop whereby a little bit of external validation can go a long way.\nLearning is also important - I did the first half of the Udacity C++ Nanodegree this year, and while a lot of what it taught me was that I never want to be a full-blown C++ developer (I’d rather make weird buildings and spaceships out of existing Lego pieces than become a polymer scientist just to create custom bricks), realising that I could learn C++ at that level if I really wanted to, was invaluable. Part of the course involved getting acquainted with cmake, and whilst I can’t claim expertise there, dabbling in a bit of the whats and whys helped make many related Arrow project issues seem less mystical.\nReassurance and validation are good and well, but in my experience, having tangible proof of the things that I do know is more effective."
  },
  {
    "objectID": "posts/goats/index.html#fake-it-til-you-make-it-is-deeply-problematic",
    "href": "posts/goats/index.html#fake-it-til-you-make-it-is-deeply-problematic",
    "title": "Three Goats in a Raincoat",
    "section": "“Fake it til you make it” is deeply problematic",
    "text": "“Fake it til you make it” is deeply problematic\nThe commonly received advice is “fake it ’til you make it”. Pretend you feel like you belong and are confident, until that becomes the case. I’ve spent a long time attempting this, and the problem is that it doesn’t actually work, because it doesn’t address the underlying issue. It’s only since I started to own the fact that I don’t feel comfortable in every environment or domain and working out what I need to do to feel more comfortable that I’ve felt my confidence growing.\nIn the past month, I’ve been a lot more open about how I’ve been feeling around this. It’s been tricky as I’ve been scared that my vulnerability just looks weakness that I shouldn’t show around other people, or just come off as moaning, but it’s had massive benefits. During the first couple of days of posit::conf this year, I suffered the worst ongoing anxiety I’ve had all year - I had a constant knot in my chest - but despite that managed to have a good time as everyone was very accepting. I can say without a doubt that pretending to be fine would have made things infinitely worse.\nThere was another point during the conference when I casually mentioning that I was feeling quite hopeful about a potential opportunity but not entirely sure if it was in the bag, as I suspected I might be up against someone whose background I find impressive. It was kindly pointed out to me that each of us bring different things to the table, a comment which has since set off a chain reaction of me starting to appreciate what I can do rather than what I can’t."
  },
  {
    "objectID": "posts/goats/index.html#the-cure-for-imposter-syndrome",
    "href": "posts/goats/index.html#the-cure-for-imposter-syndrome",
    "title": "Three Goats in a Raincoat",
    "section": "The cure for imposter syndrome",
    "text": "The cure for imposter syndrome\nThe most important thing I learned this year is that the cure for imposter syndrome isn’t persuading yourself that you aren’t just 3 goats in a raincoat pretending to be a person, but instead surrounding yourself with folks who wouldn’t really care if you were anyway because goats are cool and that’s a pretty awesome feat of acrobatics and balancing."
  },
  {
    "objectID": "posts/good-examples/index.html",
    "href": "posts/good-examples/index.html",
    "title": "R package documentation - what makes a good example?",
    "section": "",
    "text": "I’m currently working on adding to the documentation of the arrow R package, and I’ve started thinking about the qualities of good examples. Specifically, I’m referring to the examples included as part of function documentation. In my experience, the best way for me to achieve rapid familiarity with an R function I haven’t worked with before, or understand how to use a function about which I already understand the basics, is by having example code that I can run. In the book ‘R Packages’, Hadley Wickham remarks that examples are “a very important part of the documentation because many people look at the examples first” and rOpenSci recommend that each of a package’s exported functions should be accompanied by examples.\nIn this blog post, I will explore the things that I believe make for good function examples in documentation, focussing mainly on R."
  },
  {
    "objectID": "posts/good-examples/index.html#what-good-looks-like-in-r",
    "href": "posts/good-examples/index.html#what-good-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like in R",
    "text": "What good looks like in R\nI asked people on Twitter for their opinions of good R package documentation in general, and Jonathan Sinclair highlighted the ‘examples’ section from dplyr::case_when, the beginning of which is shown below.\n\nI think Jonathan is spot on in his assessment. To paraphrase, the highlights for him are:\n\nthere is next to no prose or intro\nthere are plenty of comments, as needed, to explain the examples\nthere is a variety of different examples\nthere are examples of what not to do.\n\nThis kind of documentation appeals to my skim-reading self. If I’m trying to accomplish a task, sometimes I just want to run some code and see what happens to get an intuitive feel for what a function does. While I am fully prepared to slow down and read the rest of the documentation, a “quick win” motivates me to invest the additional effort. It tells me that the developers of this code have prioritised making things easy to understand and that the time I am investing will pay off.\nI’ve been skimming through the documentation of some tidyverse and related packages - as I consider many of these to be well documented and easy to read. Here are some things I’ve observed which I think one can do to make function examples look great:\n\ninclude the most basic usage of a function\nuse very simple toy datasets or standard in-built datasets\ndemonstrate non-obvious behaviours of a function\ndemonstrate different parameter values/combinations where relevant\ndemonstrate any unusual parameters\ndemonstrate on different objects if appropriate\nsometimes go beyond the use of an individual function to include common mini-workflows\ngroup documentation and examples for similar functions together\ninclude examples that may lead to unexpected results\ninclude comments to explain examples\nno examples for deprecated functions to discourage their use\nno unpredictable external dependencies - rvest::html_text manually creates HTML to demonstrate capabilities rather than scraping an external site\nsometimes showing the output when it adds to the example (e.g. tidyselect::starts_with() and many other examples from that package)\nexamples should be correct and run without error (unless intended to show erroneous output)"
  },
  {
    "objectID": "posts/good-examples/index.html#what-bad-looks-like-in-r",
    "href": "posts/good-examples/index.html#what-bad-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What bad looks like in R",
    "text": "What bad looks like in R\nI am not intending to “name and shame” any package authors who haven’t included examples for their functions. It may have been overlooked, there may be plenty of explanation elsewhere, or they may have felt that the code was not sufficiently complex to require examples. It might be true that it seems obvious what a function does, but that makes assumptions about the users of your code that might not hold."
  },
  {
    "objectID": "posts/good-examples/index.html#what-good-looks-like-generally",
    "href": "posts/good-examples/index.html#what-good-looks-like-generally",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like generally",
    "text": "What good looks like generally\nWhen reading through examples, one thing that struck me is that when I’m looking at Python docs in Jupyter Notebook (press shift + tab), I also see the output of running the examples.\n\nSimilarly, both examples and outputs are shown in the official docs for some libraries, for example, pandas.\n\nI think this is a helpful feature - less effort is required to see how a function works.\nIn R function documentation, runnable code is often included, but in most cases needs to be manually run by the reader to see the output. I’m torn as to whether this is good or not. On the one hand, it encourages you to run the code and get a more tangible feel for what it does and saves valuable space in the Viewer window in RStudio. On the other hand, it adds an extra manual step to your workflow and lengthens the time until that precious “quick win” of enlightenment when exploring a new function.\nYou get a lot closer to this on the website rdrr.io, which indexes R package documentation and allows examples to be run inline. However, examples are run one after the other without the original code being displayed. So in the case of multiple examples, you have to match up the output to which example it is from.\n\nSome packages include output as comments within their examples. For instance, the tidyselect package; here’s an example from tidyselect::all_of:\n\nAll that said, while the ability to see the output of examples is a nice-to-have, I don’t think it’s essential to good function documentation. With any piece of documentation, it’s necessary to consider the purpose; at a minimum, examples exist to tell the reader how to use a function, and you don’t need to see the output to do that.\nSince I first wrote this, I found out that it is possible to easily run examples from help files by selecting them and then hitting Ctrl+Enter, the same as running code in the Source pane.\n\nAnother thing I wasn’t aware of - pkgdown - commonly used to automatically render docs for packages run examples and displays the output underneath. Check out the example below from the Arrow pkgdown site.\n\nIn conclusion, good examples make functions easier to work with and help readers of your documentation gain a deeper understanding of how a function works. While any examples are better than no examples, you can give your users the best chance of success when using your code with careful thought about the content of your documentation.\nHuge thanks to everyone who responded to my Twitter thread, and to my fantastic colleague Joris Van den Bossche for reading the first draft of this, and our conversations about how things are done in R and Python."
  },
  {
    "objectID": "posts/r-examples/index.html",
    "href": "posts/r-examples/index.html",
    "title": "R package documentation - what makes a good example?",
    "section": "",
    "text": "I’m currently working on adding to the documentation of the arrow R package, and I’ve started thinking about the qualities of good examples. Specifically, I’m referring to the examples included as part of function documentation. In my experience, the best way for me to achieve rapid familiarity with an R function I haven’t worked with before, or understand how to use a function about which I already understand the basics, is by having example code that I can run. In the book ‘R Packages’, Hadley Wickham remarks that examples are “a very important part of the documentation because many people look at the examples first” and rOpenSci recommend that each of a package’s exported functions should be accompanied by examples.\nIn this blog post, I will explore the things that I believe make for good function examples in documentation, focussing mainly on R."
  },
  {
    "objectID": "posts/r-examples/index.html#what-good-looks-like-in-r",
    "href": "posts/r-examples/index.html#what-good-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like in R",
    "text": "What good looks like in R\nI asked people on Twitter for their opinions of good R package documentation in general, and Jonathan Sinclair highlighted the ‘examples’ section from dplyr::case_when, the beginning of which is shown below.\n\n\n\n(image from: https://rdrr.io/cran/dplyr/man/case_when.html)\n\n\nI think Jonathan is spot on in his assessment. To paraphrase, the highlights for him are:\n\nthere is next to no prose or intro\nthere are plenty of comments, as needed, to explain the examples\nthere is a variety of different examples\nthere are examples of what not to do.\n\nThis kind of documentation appeals to my skim-reading self. If I’m trying to accomplish a task, sometimes I just want to run some code and see what happens to get an intuitive feel for what a function does. While I am fully prepared to slow down and read the rest of the documentation, a “quick win” motivates me to invest the additional effort. It tells me that the developers of this code have prioritised making things easy to understand and that the time I am investing will pay off.\nI’ve been skimming through the documentation of some tidyverse and related packages - as I consider many of these to be well documented and easy to read. Here are some things I’ve observed which I think one can do to make function examples look great:\n\ninclude the most basic usage of a function\nuse very simple toy datasets or standard in-built datasets\ndemonstrate non-obvious behaviours of a function\ndemonstrate different parameter values/combinations where relevant\ndemonstrate any unusual parameters\ndemonstrate on different objects if appropriate\nsometimes go beyond the use of an individual function to include common mini-workflows\ngroup documentation and examples for similar functions together\ninclude examples that may lead to unexpected results\ninclude comments to explain examples\nno examples for deprecated functions to discourage their use\nno unpredictable external dependencies - rvest::html_text manually creates HTML to demonstrate capabilities rather than scraping an external site\nsometimes showing the output when it adds to the example (e.g. tidyselect::starts_with() and many other examples from that package)\nexamples should be correct and run without error (unless intended to show erroneous output)"
  },
  {
    "objectID": "posts/r-examples/index.html#what-bad-looks-like-in-r",
    "href": "posts/r-examples/index.html#what-bad-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What bad looks like in R",
    "text": "What bad looks like in R\nI am not intending to “name and shame” any package authors who haven’t included examples for their functions. It may have been overlooked, there may be plenty of explanation elsewhere, or they may have felt that the code was not sufficiently complex to require examples. It might be true that it seems obvious what a function does, but that makes assumptions about the users of your code that might not hold."
  },
  {
    "objectID": "posts/r-examples/index.html#what-good-looks-like-generally",
    "href": "posts/r-examples/index.html#what-good-looks-like-generally",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like generally",
    "text": "What good looks like generally\nWhen reading through examples, one thing that struck me is that when I’m looking at Python docs in Jupyter Notebook (press shift + tab), I also see the output of running the examples.\n\nSimilarly, both examples and outputs are shown in the official docs for some libraries, for example, pandas.\n\nI think this is a helpful feature - less effort is required to see how a function works.\nIn R function documentation, runnable code is often included, but in most cases needs to be manually run by the reader to see the output. I’m torn as to whether this is good or not. On the one hand, it encourages you to run the code and get a more tangible feel for what it does and saves valuable space in the Viewer window in RStudio. On the other hand, it adds an extra manual step to your workflow and lengthens the time until that precious “quick win” of enlightenment when exploring a new function.\nYou get a lot closer to this on the website rdrr.io, which indexes R package documentation and allows examples to be run inline. However, examples are run one after the other without the original code being displayed. So in the case of multiple examples, you have to match up the output to which example it is from.\n\n\n\n(from https://rdrr.io/cran/dplyr/man/case_when.html)\n\n\nSome packages include output as comments within their examples. For instance, the tidyselect package; here’s an example from tidyselect::all_of:\n\n\n\n(from https://rdrr.io/cran/tidyselect/man/all_of.html)\n\n\nAll that said, while the ability to see the output of examples is a nice-to-have, I don’t think it’s essential to good function documentation. With any piece of documentation, it’s necessary to consider the purpose; at a minimum, examples exist to tell the reader how to use a function, and you don’t need to see the output to do that.\nSince I first wrote this, I found out that it is possible to easily run examples from help files by selecting them and then hitting Ctrl+Enter, the same as running code in the Source pane.\n\nAnother thing I wasn’t aware of - pkgdown - commonly used to automatically render docs for packages run examples and displays the output underneath. Check out the example below from the Arrow pkgdown site.\n\nIn conclusion, good examples make functions easier to work with and help readers of your documentation gain a deeper understanding of how a function works. While any examples are better than no examples, you can give your users the best chance of success when using your code with careful thought about the content of your documentation.\nHuge thanks to everyone who responded to my Twitter thread, and to my fantastic colleague Joris Van den Bossche for reading the first draft of this, and our conversations about how things are done in R and Python."
  },
  {
    "objectID": "posts/pkgdown-version-selector/index.html",
    "href": "posts/pkgdown-version-selector/index.html",
    "title": "Customising pkgdown with a version selector",
    "section": "",
    "text": "One of the things I’ve been working on lately is implementing a version selector for the Arrow pkgdown site, so that users can browse to previous versions of the documentation. In this post, I discuss the problem and the implemented solution."
  },
  {
    "objectID": "posts/pkgdown-version-selector/index.html#the-problem",
    "href": "posts/pkgdown-version-selector/index.html#the-problem",
    "title": "Customising pkgdown with a version selector",
    "section": "The problem",
    "text": "The problem\nI was excited to work on this feature as its importance is clear to me from my consulting days; I’ve worked with many organisations who have had to lock down versions of R packages they allow employees to install, and so upgrading to the latest version isn’t an option. Arrow is growing and developing as a project, both in terms of internal functionality and nudges to improve package UX, and so the latest docs version may be inaccurate if users are working with a version of the package from 1 year ago, given we do major releases every three months."
  },
  {
    "objectID": "posts/pkgdown-version-selector/index.html#does-this-already-exist-in-pkgdown",
    "href": "posts/pkgdown-version-selector/index.html#does-this-already-exist-in-pkgdown",
    "title": "Customising pkgdown with a version selector",
    "section": "Does this already exist in pkgdown?",
    "text": "Does this already exist in pkgdown?\nIt should be noted that pkgdown does already support having documentation for dev and release versions of packages, which is sufficient for most cases, but it doesn’t currently support more versions than that. This has been discussed in an issue, and I’d considered trying to do something which I could submit as a PR to pkgdown, but it’s complicated by having to make a number of design decisions which would work at a general level (and not just specific to Arrow), and, frankly, not having the resources to invest the time and effort required to come up with this kind of solution.\nInstead, at the suggestion of a colleague, I opted for a simpler solution - using custom JavaScript to override the version badge on the page and replace it with a dropdown selector."
  },
  {
    "objectID": "posts/pkgdown-version-selector/index.html#the-solution",
    "href": "posts/pkgdown-version-selector/index.html#the-solution",
    "title": "Customising pkgdown with a version selector",
    "section": "The solution",
    "text": "The solution\nArrow already has the CI set up to deploy different versions of the docs to specific URLs - the task here is just to point pkgdown to the right place. The docs are deployed to the following URLs:\n\nhttps://arrow.apache.org/docs/r/ - the released version\nhttps://arrow.apache.org/docs/dev/r/ - the dev version\nhttps://arrow.apache.org/docs/[arrow release version number]/r/ - a previous version\n\nWhen creating a pkgdown site, you can display the package version in the navbar. Here’s how this looks on an older version of the Arrow R package docs:\n\nThe plan was to use jQuery to grab this element from the page and replace it with a dropdown that has the current version selected but allows the user to navigate to another version.\n\nVersioning numbering\nOne of the constraints that I had to work with is that the deployed URL version number might not match up to the R package version number. This is because the deployed URL reflects the overall Arrow project release number, but we can release revisions to the R package independently. This might sound a bit confusing, so let’s break it down with an example. Keep this in mind - the Apache Arrow project does an overall release; this includes, amongst other things, the Arrow C++ implementation, upon which the R package is based. The R package is subsequently sent to CRAN. If we find any bugs in the R packages, we can (independently of the overall project’s releases) submit updates of the R package to CRAN. In the timeline below, I refer to the releases from the main project as “Apache Arrow [version]” and R package submissions to CRAN as “arrow pkg [version]”.\n\n2021-04-26 Apache Arrow 4.0.0 is released\n2021-04-26 arrow pkg 4.0.0 is on CRAN\n2021-05-10 arrow pkg 4.0.0.1 is on CRAN\n2021-05-26 Apache Arrow 4.0.1 is released\n2021-05-28 arrow pkg 4.0.1 is on CRAN\n\nI’ve deliberately picked a release that had a higher number of updates than usual, both from the main project and on CRAN, but hopefully, you can see from this how the different version numbers correspond with one another.\nAnyway, the deployed docs only go to the minor version number - so in this case, https://arrow.apache.org/docs/4.0/r/. If we were looking at the docs on 11th April 2021, the pkgdown site would reflect the latest released version of the R package, so 4.0.0.1. If we were looking at that same URL on 29th May 2021, the pkgdown site would then display the latest released version, which would be 4.0.1, as you can see in the example below.\n\nThe way I solved this mismatch was to use the solution already used in the main docs - maintain a JSON file mapping from the URL of the deployed docs to the R package version. I did initially think that this was a little overcomplicated and thought about doing it all with JavaScript, but I realised that to do this, I’d literally need to load each URL, extract the R package version from the version badge at the top, and then construct the selector - which is definitely much less efficient than loading a JSON file full of mappings. Instead, I created a JSON file containing key/value pairs; “name” - the displayed name of the package version, and “version” - the part of the URL following “https://arrow.apache.org/docs/”.\nHere’s the latest (at the time of writing) version of that JSON file:\n[\n    {\n        \"name\": \"7.0.0.9000 (dev)\",\n        \"version\": \"dev/\"\n    },\n    {\n        \"name\": \"7.0.0 (release)\",\n        \"version\": \"\"\n    },\n    {\n        \"name\": \"6.0.1\",\n        \"version\": \"6.0/\"\n    },\n    {\n        \"name\": \"5.0.0\",\n        \"version\": \"5.0/\"\n    },\n    {\n        \"name\": \"4.0.1\",\n        \"version\": \"4.0/\"\n    },\n    {\n        \"name\": \"3.0.0\",\n        \"version\": \"3.0/\"\n    },\n    {\n        \"name\": \"2.0.0\",\n        \"version\": \"2.0/\"\n    },\n    {\n        \"name\": \"1.0.1\",\n        \"version\": \"1.0/\"\n    }\n]\nThis JSON file then just needs updating with every release - I also updated our release scripts to do that, though I won’t go into detail about that here.\n\n\nOverriding the version element with a dropdown\nThe JavaScript code I used to create the dropdown is shown below; it’s pasted from the most recent (at the time of writing) version of the code. To summarise, it:\n\nsets up function $pathStart, the bit of the URL before /docs/\nsets up function $pathEnd, the bit of the URL after /docs/\nfetches the JSON file mapping R package versions to URLs\ncreates the appropriate links based both on the desired version number and the current page (i.e. so if I’m browsing the documentation for a function, I’ll be redirected to the selected version’s documentation for that function, rather than the main docs page for that version)\nsets up a function that, when an item is selected, quickly checks that the page selected exists and, if not, redirects the user to the main docs page for that version (i.e. if I am browsing a function’s docs in the current version and navigate to a version where that function didn’t exist)\ncreates the selector and objects and replaces the “version” span with the dropdown instead\n\nJust for transparency, I’ll note that the idea for the function to check the page exists and redirect is based on a colleague’s implementation of this in the main docs, which were based on something else, though I did have to implement it myself here to make it work with my code. Yay for open source and not reinventing the wheel :)\n$(document).ready(function () {\n\n  /**\n   * This replaces the package version number in the docs with a\n   * dropdown where you can select the version of the docs to view.\n   */\n\n     // Get the start of the path which includes the version number or \"dev\"\n     // where applicable and add the \"/docs/\" suffix\n    $pathStart = function(){\n      return window.location.origin + \"/docs/\";\n    }\n\n    // Get the end of the path after the version number or \"dev\" if present\n    $pathEnd  = function(){\n      var current_path = window.location.pathname;\n      return current_path.match(\"(?&lt;=\\/r).*\");\n    }\n\n    // Load JSON file mapping between docs version and R package version\n    $.getJSON(\"https://arrow.apache.org/docs/r/versions.json\", function( data ) {\n      // get the current page's version number:\n      var displayed_version = $('.version').text();\n      // Create a dropdown selector and add the appropriate attributes\n      const sel = document.createElement(\"select\");\n      sel.name = \"version-selector\";\n      sel.id = \"version-selector\";\n      sel.classList.add(\"navbar-default\");\n      // When the selected value is changed, take the user to the version\n      // of the page they are browsing in the selected version\n      sel.onchange = check_page_exists_and_redirect;\n\n      // For each of the items in the JSON object (name/version pairs)\n      $.each( data, function( key, val ) {\n        // Add a new option to the dropdown selector\n        const opt = document.createElement(\"option\");\n        // Set the path based on the 'version' field\n        opt.value = $pathStart() + val.version + \"r\" + $pathEnd();\n        // Set the currently selected item based on the major and minor version numbers\n        opt.selected = val.name.match(\"[0-9.]*\")[0] === displayed_version;\n        // Set the displayed text based on the 'name' field\n        opt.text = val.name;\n        // Add to the selector\n        sel.append(opt);\n      });\n\n      // Replace the HTML \"version\" component with the new selector\n      $(\"span.version\").replaceWith(sel);\n    });\n});\nHere’s the code which checks if the page exists, and if not, redirect to the main page:\nfunction check_page_exists_and_redirect(event) {\n\n    /**\n       * When a user uses the version dropdown in the docs, check if the page\n       * they are currently browsing exists in that version of the docs.\n       * If yes, take them there; if no, take them to the main docs page.\n       */\n\n    const path_to_try = event.target.value;\n\n    const base_path = path_to_try.match(\"(.*\\/r\\/)?\")[0];\n    let tryUrl = path_to_try;\n    $.ajax({\n        type: 'HEAD',\n        url: tryUrl,\n        success: function() {\n            location.href = tryUrl;\n        }\n    }).fail(function() {\n        location.href = base_path;\n    });\n    return false;\n}"
  },
  {
    "objectID": "posts/pkgdown-version-selector/index.html#testing",
    "href": "posts/pkgdown-version-selector/index.html#testing",
    "title": "Customising pkgdown with a version selector",
    "section": "Testing",
    "text": "Testing\nI considered testing this locally, but it was tricky because the code for breaking up the URL didn’t quite work properly when running this on localhost. I could have tried to fix that, but I’d also have to simulate having multiple versions of deployed docs, and ultimately it was much simpler to test on the live docs instead.\n\nLocal overrides\nOriginally I tested this all by opening up the developer tools in a web browser on the live site and pasting the JS above into the console. This was fine, though later on, when deploying some fixes to a couple of minor bugs, I learned about local overrides. Basically, this is a feature of developer tools that allows you to specify JS to load when the page loads instead of whatever is deployed on the site (or perhaps as well as?) - this was invaluable when testing a fix to a bug that only appeared when browsing between pages.\nThe bugs found in the initial implementation are discussed below.\n\n\nIssue 1 - cached HTML\nOne problem was that for one user when they browsed to a page that contained the version selector (let’s call it Page A), navigated to another page (let’s call it Page B), and then hit “back”, the selected/displayed version was the version from Page B and not Page A, as expected, given we were at the URL for Page A.\nThis felt like a caching problem to me, and whilst I couldn’t replicate it locally, I had a bit of a search and found similar StackOverflow questions which indicated it was to do with this, so I added this bit of JS code which basically checks if the page is loading from a cache and if so, reloads it.\n$(window).bind(\"pageshow\", function(event) {\n  if (event.originalEvent.persisted) {\n    window.location.reload()\n  }\n});\nWeirdly, this seemed to affect users on Chrome on macOS but not on Chromium on my Linux machine - perhaps the Chrome/Chromium defaults are a bit different, or we just had different versions with different things going on. I asked a colleague who could replicate the bug on his MacOS machine to test my solution using local overrides as mentioned above, and this seems to have fixed it quite nicely.\n\n\nIssue 2 - not showing on Safari on macOS\nAnother colleague found that the dropdown selector wasn’t showing up at all on Safari, and they could only see the original ‘version’ tag but no dropdown. It took a while to work this one out, and eventually, I asked a colleague on MacOS to dump out the output from the JS console.\nThe problem lay in the $pathEnd variable, defined as:\n$pathEnd  = function(){\n  var current_path = window.location.pathname;\n  return current_path.match(\"(?&lt;=\\/r).*\");\n}\nThe issue here is that part of the regular expression is using a positive look-behind - basically saying “look ‘/r’ and return everything after it”. As someone who doesn’t use regex often, I felt very smug putting together something “clever” like this, but these kinds of expressions aren’t supported by all browsers, so in the end, I simplified it to a regex that finds the entire chunk, “/r” and all, and then just returns everything after the second character.\n$pathEnd  = function(){\n  var current_path = window.location.pathname;\n  // Can't do this via a positive look-behind or we lose Safari compatibility\n  return current_path.match(\"\\/r.*\")[0].substr(2);\n}"
  },
  {
    "objectID": "posts/pkgdown-version-selector/index.html#conclusion",
    "href": "posts/pkgdown-version-selector/index.html#conclusion",
    "title": "Customising pkgdown with a version selector",
    "section": "Conclusion",
    "text": "Conclusion\nAwesome, thanks for sticking with me as I walked through that. This was honestly a bit of a PITA to do as I’m not the best JS developer, and the bugs were a pain to solve without being able to easily replicate them with my browsers/OS (I’m sure there are tools out there to do it but honestly just having a quick call with a colleague was the simplest solution!) I would have loved to have done something where I ended up submitting a PR to pkgdown to make this a more general feature, but this solution is super-custom and isn’t set up that way. That said, this was a super interesting problem to solve, and it was good fun working out how to fit all the pieces together!"
  },
  {
    "objectID": "posts/ai-tooling-open-source/index.html",
    "href": "posts/ai-tooling-open-source/index.html",
    "title": "AI Tooling and Open Source",
    "section": "",
    "text": "As one of the maintainers of Apache Arrow, lately I’ve been noticing an increase in AI-generated pull requests. In this blog post, I discuss how AI tooling affects open source, the actions maintainers have been taking to address the less positive aspects, and emerging policies that open source projects are implementing around the topic of AI-generated pull requests."
  },
  {
    "objectID": "posts/ai-tooling-open-source/index.html#attitudes-to-ai-tools",
    "href": "posts/ai-tooling-open-source/index.html#attitudes-to-ai-tools",
    "title": "AI Tooling and Open Source",
    "section": "Attitudes to AI tools",
    "text": "Attitudes to AI tools\nEarlier reactions to the use of AI tools in some open source projects have been quite negative, and there are lots of examples of open source projects explicitly banning the use of AI tools in generating contributions, for example, Gentoo Linux in 2024 banned all AI contributions.\nI have some sympathy for this perspective, and there are certainly good and valid reasons around complications relating to licensing and AI-generated code that I believe open source developers need to be cautious of. The ASF have published some high-level guidelines on the use of generative tooling that capture the risk of replicating copyrighted materials.\nThat said though, I also believe that the evolution of these tools means that condemning them outright is throwing the metaphorical baby out with the bathwater. Entire features created with AI absolutely can be problematic from a licensing perspective, but much of the maintenance work required for existing codebases is less fraught. In my experience, fixes and changes generated based on the existing codebase and reasoning around it rarely end up replicating other codebases, and these tools are a net win for maintainers when used well.\nAs the sophistication of tools has increased, I found that at some point in 2025, the balance tipped for me from relatively poor results towards them becoming net useful. I also believe a lot of developers are in this place where many of us are actively using these tools but fewer are openly admitting it, due to the scepticism out there.\nWhile there is some validity in the more cautious perspectives, it’s unrealistic to pretend that they aren’t being used pretty widely at this point."
  },
  {
    "objectID": "posts/ai-tooling-open-source/index.html#the-problem-of-ai-generated-prs-in-open-source",
    "href": "posts/ai-tooling-open-source/index.html#the-problem-of-ai-generated-prs-in-open-source",
    "title": "AI Tooling and Open Source",
    "section": "The problem of AI-generated PRs in open source",
    "text": "The problem of AI-generated PRs in open source\nThere is a certain class of AI-generated pull request that is becoming problematic. There have a distinct pattern to them:\n\na pull request is submitted by a contributor who has never previously contributed to the codebase\ncode changes are often accompanied by verbose and unnecessary comments\nthe scope of the changes is overly broad scope\nunit tests are either excessive or non-existent\nthe code itself doesn’t match the style or conventions of the other code\nthe contributor doesn’t ask any questions about anything they’re unsure about\n\nWhen I see a pull request with these characteristics, I tend to assume that the contributor wanted to contribute to the codebase for whatever reason, decided to use AI to generate the code, but then didn’t engage much if at all with the output before submitting the PR.\nTo a certain extent I get it. Some of my first open source pull requests were absolutely done for the sake of something that I viewed as a real achievement and not a genuine investment in a codebase, because I didn’t really understand how open source worked at that point. I was fortunate to end up interacting with patient people who took their time to guide me through the process and help me learn, for which I’m grateful.\nAnd I’ve made a fair share of embarrassing mistakes in my own use of AI tools. In my earlier explorations, I accidentally allowed Claude to push to an upstream branch instead of my own fork, and proposed poorly thought-through changes that I didn’t actually understand well enough to take responsibility for when other maintainers responded with thoughtful questions. The realisation of this potential for harm made me drastically alter my approach, but I had to make the mistake to learn the lesson.\nAll of that said though, most open source maintainers are pressed for time to work on the project. It’s great working with new contributors, both for the human connection and because it improves project health by increasing the number of people who are able to contribute, but there is very little value in a maintainer reviewing AI-generated code which the author has either abandoned or will just continue to use AI to respond to the provided suggestions.\nThoughtful use of AI tools requires multiple iterations and changes, and questioning of the tooling as to the reasoning behind changes, and taking ownership of the changes through the understanding gained during the process."
  },
  {
    "objectID": "posts/ai-tooling-open-source/index.html#taking-it-to-the-mailing-list",
    "href": "posts/ai-tooling-open-source/index.html#taking-it-to-the-mailing-list",
    "title": "AI Tooling and Open Source",
    "section": "Taking it to the mailing list",
    "text": "Taking it to the mailing list\nAfter noticing the increase in AI generated PRs, I wanted to validate the preferred approach to handling these PRs with the wider Arrow community, and so I started a discussion on the Arrow developer mailing list about defining a set of community-agreed guidelines for AI-generated PRs.\nIt takes a chunk of emotional energy trying to respond to a PR with the empathy and welcoming nature that we want to show towards new contributors, while simultaneously not investing too much time and energy in a pull request which may well be abandoned. Conversely though, if the contributor really is interested in engaging, we don’t want to drive them away.\nIn reality, we can’t necessarily tell when people make thoughtful use of AI tooling, and so my own personal motivation for suggesting this guidance was to have an official source that we can link to during reviews of PRs which appear to be AI generated.\nGiven all the negativity I’ve seen online around the use of AI tooling, and also my own use of such tools, I was a little nervous to see what the conversation would look like when I opened the discussion on the Apache Arrow developer mailing list.\nIt was great to see that, as ever, the Apache Arrow developer community engaged in a thoughtful discussion in which folks suggested additions to my suggested phrasing of policy, and chimed in with their own experiences. This clearly isn’t a novel or new idea, and Andrew Lamb pointed me straight to some excellent guidelines which already existed in the DataFusion project. (For context, DataFusion is a query engine built on Arrow, used to be kept in the former Arrow monorepo, and is part of the Arrow community). I borrowed some ideas from the DataFusion guidelines as well as the contributions from other folks on the mailing list. The key recommendations we ended up with were:\n\n\nOnly submit a PR if you are able to debug and own the changes yourself - review all generated code to understand every detail\nMatch the style and conventions used in the rest of the codebase, including PR titles and descriptions\nBe upfront about AI usage and summarise what was AI-generated\nIf there are parts you don’t fully understand, leave comments on your own PR explaining what steps you took to verify correctness\nWatch for AI’s tendency to generate overly verbose comments, unnecessary test cases, and incorrect fixes\nBreak down large PRs into smaller ones to make review easier\n\n\nSince we had this discussion, the PR that we merged was linked to by a couple of other projects and after following the links back I found even more open source projects which had already had engaged in similar discussions or recent policy updates, for example, Jupyter, Zulip, LLVM, and Python."
  },
  {
    "objectID": "posts/ai-tooling-open-source/index.html#other-challenges",
    "href": "posts/ai-tooling-open-source/index.html#other-challenges",
    "title": "AI Tooling and Open Source",
    "section": "Other challenges",
    "text": "Other challenges\nPull requests aren’t the only challenge that open source is seeing in terms of AI tool usage; multiple projects have also recently experienced an uptick in security reports. Some of these are valid, but others appear to constitute things that would be considered bugs or edge cases. While deploying fixes for these may genuinely improve the codebase, some examples appear to include strategic phrasing to depict bugs as security incidents, and feel a little disingenuous.\nThis is becoming more prevalent, as shown by the curl project recently closing its bug bounty program, and similar issues experienced by Apache Log4j.\nThe irony of the spurious reporting of alleged potential Denial-of-Service attacks is delightfully captured by curl author Daniel Stenberg in the abstract of his upcoming FOSDEM talk:\n\n“Sloppy humans causing Denial-of-Service attacks by overloading maintainers with quickly produced almost-real-looking rubbish”.\n\nAI is having a significant effect on open source development, some of it positive, but it’s certainly not without real challenges as well."
  },
  {
    "objectID": "posts/tidyverse-contributor/index.html",
    "href": "posts/tidyverse-contributor/index.html",
    "title": "Ten Steps to Becoming a Tidyverse Contributor",
    "section": "",
    "text": "This year has been a crazy whirlwind for me – I moved job once, moved house 3 times, co-authored a course on Data Camp, got invited by a former colleague to assist with a workshop at rstudio::conf 2019, was accepted to present at the very same conference, and became a minor contributor to the tidyverse. In this blog post I’ll talk about reasons for contributing and then walk through the steps I took to help guide others.\nA disclaimer before I start: I am no expert here and am talking about something I have done once. My aim here is to clarify the process and hopefully make it clear to other members of the R community that this isn’t as hard or intimidating as it might sound.\nIt all started with the fantastic talk by Mara Averick at the 2018 RStudio Conference. Mara talked about different ways of getting involved and made me realise that contributing, specifically code, was both something I wanted to do, and something that was within reach.\nA while later, I saw this tweet by Alex Hayes, who was doing an internship at RStudio, and couldn’t turn down the opportunity. Massive thanks once again to both Alex and Mara!\n\nI’d never thought about getting involved in contributing code because of my perception of some of the toxic culture of open source, and this perception isn’t totally unwarranted.\nThis year, however, I’ve seen a lot of evidence to suggest that this is changing, and, regardless, the R community, is way ahead of the curve in terms of being welcoming, friendly, and willing to help newbies of all varieties to get involved. I do not believe for even a split second that my concerns apply to the R community in 2018.\nThere are a lot of good reasons to contribute to open-source; half of the respondents to the Open Source Survey in 2017 said that their open source contributions helped them get their current job. My personal reasons for getting involved, however, were more about a) getting acquainted with some top-notch R code and hopefully levelling up my skills as a result, b) interacting with more of the R community, and c) helping lessen my imposter syndrome. I think it’s time for an inspirational quote now:\n\n“When I started my transition into data science, I said yes to pretty much every opportunity that came my way, even if it felt slightly beyond my skill set or experience level…I said yes to many of these things when it felt like I wasn’t sure if I was ready.” - Julia Silge (Interview with And Comfort, October 2018)\n\nSo, without further ado, let’s take a look at the ten steps to becoming a tidyverse contributor. Please note that there may be nuances and details that I have missed here, but this should provide a rough guide to anyone who’s thinking about getting involved and wants to know a bit more about exactly what that looks like.\nJust to be clear; these steps apply to contributing to R packages in general and not just the tidyverse. However, the tidyverse team have been especially good for encouraging people to get involved.\n\nStep 1 - Decide How to Contribute\nI want to emphasise at this point that contributing code is not the only way to get involved; the reason why I’m focussing on this in this blog post is because a) it’s what I did, and b) it’s the method of contribution I feel like people know the least about.\nHowever, as Mara discusses in her talk (no really, just go watch it now, seriously!), there are other equally valid and important ways of getting involved.\n\n\nStep 2 - Learn!\nWhilst there is some pre-requisite knowledge required to contribute code to R packages, it’s not as much as I expected. You absolutely should be comfortable with working with and writing functions in R. Beyond that, some familiarity with git is needed – things like clone, commit, and push, though doing it all via RStudio is fine. You’ll need to know the basics of R package building – when I say basics, what I mean is that you’re not writing a package from scratch, so don’t need to know every little detail. However, you should have a good idea of the directory structure of an R package, how to write tests and documentation, and how to build the page. Finally, you’ll need to know how to create a pull request on GitHub, which, once you’ve conquered the other items on the list, fairly uncomplicated. I’ve linked to resources for all of these things at the end of this post.\n\n\nStep 3 - Find an Issue\nThe next step is finding an issue to work on. Luckily, many issues are tagged on GitHub with “beginner friendly”, “help wanted”, or “good first issue” to help point you in the right direction.\nIn the GIF below, I show how to get from the CRAN page for a package – broom in this case – to a potential issue to work on. I first click on the URL next to “BugReports” to get taken to the GitHub issues page. Next I find a relevant tag – “beginner friendly” in this case. Once I click on the tag, I’m shown all issues with that tag. I then pick one, and read the description to see if it’s something I think would be good to look at getting involved with.\n\n\n\nStep 4 - Ask if You Can Help\nDon’t charge straight into trying to write code! You need to ask for help – it’s always good to check that someone else isn’t already working on this issue, and ask to get involved. You don’t want to spend ages working on something and then find out someone else has submitted a pull request first! Try to be as specific as possible – unlike my comment here, ellessenne says exactly what they’ll be looking at!\n\n\n\nStep 5 - Fork the Repo\nForking the repo means making a copy of it which is saved to your GitHub account. Click on the button as shown below.\n\n\n\nStep 6 - Open as New Project in RStudio & Make a Branch\nCreate a new project (“from version control”) in RStudio using the URL for your copy of the repo - the location for the URL is shown below.\n\nOnce you’re in this project, make a new branch, making sure “sync with remote” is checked.\n\n\n\nStep 7 - Familiarise Yourself with the Code\nPlay with the code and see if you can reproduce the issue, and work out the cause. I tend to run through things one line at a time, not moving on til I have a pretty good idea what each line is meant to be doing.\n\n\nStep 8 - Write Code\nOnce you’ve worked out how to fix the issue, it’s time to write your code! Conform to the coding style used in the rest of the code, be concise but clear, and don’t introduce any new dependencies (if this is needed, it should be discussed first). Don’t forget to update any function parameter documentation.\nThe image below shows a diff of the file I edited before and after the changes I made. There is very little changed – it’s best to work on small issues to build trust with the package maintainer over time.\n\n\n\nStep 9 - Submit PR and Wait\nOnce I’d submitted my pull request, I waited with the patience of your average toddler that’s been sat down in front of a plate of marshmallows and told not to eat any. Look, I was excited – I was making my first tidyverse pull request! I nearly lost sight of the fact that people have lives, jobs, and frankly, other things to be doing with their time. Ultimately, I’m glad I didn’t nudge as I feel like it would have felt rude. There were a few changes and additions needed, but eventually my changes got merged into the master branch.\n\n\n\nStep 10 - Celebrate! Then Encourage Others to Get Involved and Share Your Experience\n\nI was pleasantly surprised by the fact that I already had most of the background knowledge required, and all I really needed was some inspiration to get involved and the patient Alex Hayes being awesome at guiding me through the process.\nI’d like to end by emphasising that, although I’ve been using R on and off for 8 years, and as part of my job for 3, I believe I could have contributed sooner if I’d known more about the process. A lot is still unclear to me but I’m learning more by talking to people. If you’re unsure, watch the repos and see the conversations and how people go about different things. Look at the individual commits people make. Get a feel for what’s going on.\nI’m still so new to this, and still agonise on phrasing every time I open an issue or pull request, but I think that as long as you’re polite, accept feedback well, and don’t try to get too far ahead of yourself, it’s probably going to be a really positive experience for you. The R community is made up of a lot of really awesome people.\n\n\nResources\nTalk by Mara Averick on contributing: https://www.rstudio.com/resources/videos/contributing-to-tidyverse-packages/\nGuide to contributing code to the tidyverse: https://www.tidyverse.org/articles/2017/08/contributing/\nrOpenSci contributing guide: https://ropensci.github.io/dev_guide/contributingguide.html\nRepo walking through making a pull request: https://github.com/firstcontributions/first-contributions"
  },
  {
    "objectID": "posts/open-source-apprenticeship/index.html",
    "href": "posts/open-source-apprenticeship/index.html",
    "title": "Open Source Developer Apprenticeship - 9 months in",
    "section": "",
    "text": "I’ve been meaning to blog for ages but have been so wrapped up in project things and getting nerd-sniped. Here’s a bit about the last six months.\n\nOpen-ended role\n\nI’m doing a better job of adapting to the environment of total freedom within the role. My team has quarterly initiatives to complete, but otherwise, my job is just to “be an open-source maintainer” and do whatever that entails, with colleagues occasionally asking me to get involved with specific pieces of work. At first, this was deeply disorienting (and certainly I’m not the only one who feels this) - I want to be good at this and to be good at it, I need something to measure myself again, and super-explicit expectations and concrete standards for everything are crucial in this, right? Actually…no. Like everyone else, I have areas in which I’m stronger and weaker, and this environment allows me to play to my strengths, work on my weaknesses when it’s a good time to do so, and figure out where I fit in the wider team. If something isn’t getting done which falls within my remit, I’ll either figure it out myself or hear about it, and it’ll get sorted. If I’m doing something catastrophically wrong, I’m sure someone will let me know and help me work out how to fix it. Beyond that, I need to take ownership of asking for feedback, which is weirdly daunting - even from colleagues that I get on really well with - but learning to bite the bullet and do it is important.\nOn a practical level, I’ve found that keeping a spreadsheet of things I’m working on and things I intend to work on soon, with checklists of things I need to check each day, has been an effective and straightforward way of organising my time and priorities on a daily/weekly basis. And when I find the time, I plan on making something - perhaps a Shiny dashboard, Slack bot, or API - to help me keep track of answers to questions like “have any PRs been approved but not merged?”, “are there any new R PRs which haven’t yet been reviewed?” and “are there any new GitHub Issues without replies?”\n\n\nLearning and documentation\n\nIt’s been a delight to find out that learning new things and producing documentation can be deeply intertwined processes, and I’ve noticed a three-stage process that seems to work for me.\n\nIngestion. This is where I write a ton of scruffy biro notes, draw diagrams, and sometimes even transcribe chunks of lectures verbatim when the topic is particularly far from my current knowledge. These usually get blu-tacked to my office wall to use as reference materials.\nConsolidation. I take my notes from the ingestion stage, chuck away the irrelevant bits and condense them down to a smaller reference doc. This could be another piece of A4 stuck on the wall or a Google Doc. This is when I start to get excited that things are making a lot more sense.\nDocumentation. At some point, I realise that the thing I’ve learned is useful to someone else, and now I have to write it up properly. This is a weird bit of the process as I can’t tell you how long something is going to take or how I know when it’s done - it just kind of happens. I can’t say I love this bit once I’m three rounds of edits and reshuffling in, but once it’s done and I have some content, it’s pretty satisfying.\n\n\n\nC++\n\nWhen I first started, I felt like I wanted to learn everything, and very earnestly started picking up books on C++ and proclaiming I was going to learn the language. Since then, I’ve dabbled a bit but not taken it much further, and more recently, I’ve decided to change the scope of those ambitions. If programming in R and Python are like building things out of Lego bricks, C++ is more akin to becoming a polymer scientist and learning how to build the bricks. Worthwhile and interesting, sure, but not something I can get excited about in the same way. All that said, however, I’m still looking at learning “enough C++ to be useful in an R context”; I guess the equivalent of knowing enough about manufacturing to build my own custom bricks from time to time.\n\n\nThe SO non-incident\n\nThere was something that happened around month 5, which was really important at the time, and I’ve wanted to write about it. I’d replied to my second Arrow question on Stack Overflow - a little intimidating at the time, interacting in a different domain with different social norms compared to internally at Voltron Data or within the Apache Arrow project. Anyway, my (accurate and correct!) answer got deleted, and upon first glance, it looked like it had happened unfairly (which historically has disproportionately happened to people who aren’t your stereotypical dev). Some of my colleagues took this really seriously and were trying to see what they could do about it. Later, it transpired that this was not the case; I’d accidentally transgressed some rule relating to linking to an external source without pasting the relevant bit of content into my answer, and I submitted a new answer, which was not deleted. But, my colleagues’ initial reaction to what basically appeared to be discriminatory behaviour was…everything. I feel grateful to work with such excellent humans, and this whole incident made me realise it’s time to replace my mindset of “some people will treat you like this and best avoid those places/interactions if you don’t want to deal with that BS” with a new attitude of “you absolutely belong here, and people have got your back”.\n\n\nBeing an open-source maintainer\n\nWorking for Voltron Data and being a maintainer on Apache Arrow is one of the coolest jobs I’ve had in my life, if I’m honest, and I still have moments where I go “huh, I’m paid to work on open-source? Wow!” Don’t get me wrong, there are boring bits where I can spend more hours than I’d like tracking down an elusive bug than only shows up on a certain CI job or resolving some horrible merge conflicts after a rebase. But, these are balanced with things like working with new contributors on their first PR, writing documentation that turns implicit knowledge into explicit writing that can be shared, learning so so many new things, and submitting PRs for things that I personally care about on the project. My highlight reel so far includes:\n\nSubmitting PRs to other projects - having actual problems I wanted to solve, even if it was just adding extra documentation or fixing typos (as opposed to trying to find things to do/fix as I was more invested in the idea of being a contributor), was really gratifying.\nGetting added to the list of authors on the arrow R package - I’d submitted a ton of PRs for that release but wasn’t expecting to get added to the author list, so it was a great surprise.\nBecoming a committer on the Apache Arrow project - being given the power to merge PRs was a little intimidating at first, but again, this privilege/responsibility was something I’d thought was further away, so it was a super exciting moment when it happened and it was nice to be recognised as “demonstrating a sustained commitment to the project”.\nBeing heavily involved in publishing the first versions of the Apache Arrow cookbooks - I’m one of the main authors on the R cookbook, and whilst there’s a load of changes I’d like to see, getting the first version out was a real achievement.\nUpdating the R package docs - I’m passionate about documentation and did a lot of work for this release adding diagrams and rephrasing the (already great) docs to try to make them simpler to understand and reach a wider audience of users.\nPublishing docreview on CRAN - I desperately need to make time to work on this side project, but getting the initial version of it together was instrumental in helping me work on the arrow docs and learn a few things about being a maintainer and starting new projects.\nGetting involved in a lot of CI work - our CI is pretty complex, and this is an area that I’ve found challenging, but I love the detective work involved in working out how to maintain and extend our builds.\nIn general, building confidence as a developer. This is an ongoing process, and I’m not there yet, but recently I’ve started to notice a real shift in (a somewhat unconscious) mindset from “this is really challenging, how will I ever get this” to “I’m sure I’ll figure it out somehow”.\nHonestly, working with such a badass group of smart, hard-working, considerate people. I regularly find myself observing how people handle certain situations or approaches they take to solving problems and trying to mirror that behaviour.\n\n(Note that around July 2021, Ursa Computing (i.e. where I originally started my apprenticeship) became part of a new company, Voltron Data, but for the sake of simplicity here, I refer to where I work as Voltron Data, even when talking about things that happened prior to July 2021.)"
  },
  {
    "objectID": "posts/scraping-rstudioconf-2018-abstracts/index.html",
    "href": "posts/scraping-rstudioconf-2018-abstracts/index.html",
    "title": "Scraping rstudio::conf 2018 Abstracts",
    "section": "",
    "text": "RStudio Conference 2019 takes place in January 2019, and this week RStudio put out a call for contributed talks and e-posters. Though I was eager to browse previous years’ abstracts for inspiration, I couldn’t find them all in one place, and so I decided to use one of my favourite R packages, rvest, to do some web scraping to grab the content.\nMy main aim was to find all of the abstracts for the contributed talks only from 2018.\nAs this ended up being an unusually long blog post, here’s a table containing links to the videos and abstract for contributed talks. A detailed walk through of the code used to create it can be found below.\nAs ever, give me a shout on Twitter if you have any comments or questions!\nHere’s a walkthrough of the code, and the full list of abstracts."
  },
  {
    "objectID": "posts/scraping-rstudioconf-2018-abstracts/index.html#selector-gadget",
    "href": "posts/scraping-rstudioconf-2018-abstracts/index.html#selector-gadget",
    "title": "Scraping rstudio::conf 2018 Abstracts",
    "section": "Selector Gadget",
    "text": "Selector Gadget\nSelector Gadget is a tool which takes a lot of effort out of webscraping. For an in-depth description, check out this rvest vignette. In short, once Selector Gadget is loaded, I click on any unselected components to pick which ones I need (in green), look at what else is highlighted (in yellow) and then click on these to deselect them (now in red) and carry on selecting and/or deselecting components until only the ones I want are in green or yellow.\n\nThen, I copy the text out of the box, in the case, “.talk-title”. This is the CSS selector needed to grab those components, and I supply this as the css argument to html_nodes(). As I just want the inner text, I pipe the result of this into html_text().\ntalks &lt;- my_html %&gt;%\n  html_nodes(\".talk-title\") %&gt;%\n  html_text()\n\nhead(talks)\n## [1] \"\\nTo the Tidyverse and Beyond: Challenges for the Future in Data Science\\n\\n\"\n## [2] \"\\nTo the Tidyverse and Beyond: Challenges for the Future in Data Science\\n\\n\"\n## [3] \"\\nTidy spatial data analysis\\n\\n\"\n## [4] \"Scaling Shiny apps with async programming\"\n## [5] \"\\nRStudio 1.1 new features\\n\\n\"\n## [6] \"\\nThe future of time series and financial analysis in the tidyverse\\n\\n\"\nI now repeat this process to get the presenter names. Handily, the CSS selector for these is “.presenter”!\nauthors &lt;- my_html %&gt;%\n  html_nodes(\".presenter\") %&gt;%\n  html_text()\n\nhead(authors)\n## [1] \"Dianne Cook\"   \"Dianne Cook\"   \"Edzer Pebesma\" \"Joe Cheng\"\n## [5] \"Kevin Ushey\"   \"Davis Vaughan\"\nNow I’ve got my talks and authors, I’m next going to combine them in a data_frame. Once this is done, I’m also going to tidy them up by removing all the newline characters (“”) from the titles.\nlibrary(dplyr)\nlibrary(stringr)\nschedule &lt;- data_frame(talks, authors) %&gt;%\n  mutate(talks = str_remove_all(talks, \"\\n\")) %&gt;%\n  distinct()\n\nhead(schedule)\n## # A tibble: 6 x 2\n##   talks                                                        authors\n##   &lt;chr&gt;                                                        &lt;chr&gt;\n## 1 To the Tidyverse and Beyond: Challenges for the Future in D~ Dianne Cook\n## 2 Tidy spatial data analysis                                   Edzer Pebe~\n## 3 Scaling Shiny apps with async programming                    Joe Cheng\n## 4 RStudio 1.1 new features                                     Kevin Ushey\n## 5 The future of time series and financial analysis in the tid~ Davis Vaug~\n## 6 Developing robust shiny apps with regression testing         Winston Ch~\nGreat! So now what? Well, the point of this analysis is to look at the contributed talks, rather than those from invited speakers or RStudio staff, so in the next part of this post, I’m going to be using lots of dplyr joins to remove these!"
  },
  {
    "objectID": "posts/leaflet-shiny/index.html",
    "href": "posts/leaflet-shiny/index.html",
    "title": "Getting started with Leaflet in Shiny - interactive map apps",
    "section": "",
    "text": "Lately I’ve been enjoying learning how to use Shiny, and experimenting with making a fairly basic app, and wanted to develop my skills further. I’d been looking at an example app from the Shiny website which shows live bus locations in the US, refreshing at short intervals. It looked fairly complex at first, so I was pretty happy when, within 48 hours I’d managed to recreate something of similar complexity, so here’s your guide on how you can quickly get up to speed using interactive reactive maps in Shiny.\nIf you follow all of my steps, you will learn:\nHere I’m presuming you already have a reasonable working knowledge of Shiny, and are at least at the point of having created your own app and run some of the examples. I’m also presuming that, like me, you have no prior knowledge of GISs, visualising spatial data, or Leaflet."
  },
  {
    "objectID": "posts/leaflet-shiny/index.html#step-1-really-get-to-know-the-bus-dashboard-example",
    "href": "posts/leaflet-shiny/index.html#step-1-really-get-to-know-the-bus-dashboard-example",
    "title": "Getting started with Leaflet in Shiny - interactive map apps",
    "section": "Step 1: really get to know the bus dashboard example",
    "text": "Step 1: really get to know the bus dashboard example\n\nThe bus dashboard example can be downloaded here. After playing around with the app in a browser, I printed out all of the code and annotated it with a few different colour pens. I went back and forth, writing notes and drawing diagrams until I understood what all the different components did and how they interacted with each other. This took a few hours, but the time invested was well worth it, as the design of the components of my app later felt pretty easy despite my lack of experience in making complex apps."
  },
  {
    "objectID": "posts/leaflet-shiny/index.html#step-2-read-up-on-visualising-spatial-data-in-r",
    "href": "posts/leaflet-shiny/index.html#step-2-read-up-on-visualising-spatial-data-in-r",
    "title": "Getting started with Leaflet in Shiny - interactive map apps",
    "section": "Step 2: read up on visualising spatial data in R",
    "text": "Step 2: read up on visualising spatial data in R\nNext, go through the Introduction to visualising spatial data in R by Robin Lovelace and James Cheshire, and be sure to run all the code examples. It’s a great introduction to the topic and covers all the relevant background you need, without being verbose. You might want to have a quick read up about geographical mapping systems, but don’t worry too much about in-depth understanding; a basic knowledge of the fact that different systems exist will do."
  },
  {
    "objectID": "posts/leaflet-shiny/index.html#step-3-read-up-on-leaflet-for-r",
    "href": "posts/leaflet-shiny/index.html#step-3-read-up-on-leaflet-for-r",
    "title": "Getting started with Leaflet in Shiny - interactive map apps",
    "section": "Step 3: read up on Leaflet for R",
    "text": "Step 3: read up on Leaflet for R\nThe official site of Leaflet for R tells you everything you need to know to get started. You don’t need to read everything here, but as a bare minimum make sure you check out the sections labelled “Introduction”, “The Map Widget”, and “Shiny Integration”."
  },
  {
    "objectID": "posts/leaflet-shiny/index.html#step-4-get-some-gis-boundary-files",
    "href": "posts/leaflet-shiny/index.html#step-4-get-some-gis-boundary-files",
    "title": "Getting started with Leaflet in Shiny - interactive map apps",
    "section": "Step 4: get some GIS boundary files",
    "text": "Step 4: get some GIS boundary files\nSo now you know that the basics, you need some data to work with. We’ll start off with the boundaries for our maps. I had a few false starts and red herrings, but eventually found the boundary file I was after at the London Datastore. Once I’d downloaded the data and selected the relevant files, I did a quick plot to check it was what I was after."
  },
  {
    "objectID": "posts/leaflet-shiny/index.html#step-5-get-some-data",
    "href": "posts/leaflet-shiny/index.html#step-5-get-some-data",
    "title": "Getting started with Leaflet in Shiny - interactive map apps",
    "section": "Step 5: get some data",
    "text": "Step 5: get some data\nAgain, I went to the London Datastore, but this time I downloaded data on the average income of tax payers by London borough. This was really easy to merge with my GIS boundary files, using the dplyr package to do SQL-style joins."
  },
  {
    "objectID": "posts/leaflet-shiny/index.html#step-6-code",
    "href": "posts/leaflet-shiny/index.html#step-6-code",
    "title": "Getting started with Leaflet in Shiny - interactive map apps",
    "section": "Step 6: code!",
    "text": "Step 6: code!\n\nFinally, it was time to pull it all together. I started off by writing a basic Shiny UI file, with placeholders for content, and then started filling in the different sections. First I added a static map, and then proceeded to adapt my code so that the map would change based on the user’s selections."
  },
  {
    "objectID": "posts/leaflet-shiny/index.html#step-7-refine",
    "href": "posts/leaflet-shiny/index.html#step-7-refine",
    "title": "Getting started with Leaflet in Shiny - interactive map apps",
    "section": "Step 7: refine",
    "text": "Step 7: refine\nThis is the step that I’m still at now. Whilst coding the map, my design changed to take into account the capabilities of Shiny and Leaflet. Originally I’d planned to use Shiny’s animated slider to scroll through the different years, but it became quickly apparent that this would render too slowly to be visually appealing. Although I offset some of this slowness by using the leafletProxy function to only redraw the coloured tiles but keep the underlying map, there was still some delay.\nWith the resources I’ve linked to above and the occasional bit of Googling when things don’t quite work as expected, it’s fairly easy to get up and running making interactive maps with Shiny and Leaflet, so give it a go!\nClick here to view source code on GitHub.\n[Edited to add: I’ve been thinking about how I’d improve my code, and came to the conclusion that whilst this functions as a good introduction to a number of topics, if my goal was solely to create a choropleth of an area, I’d use something more lightweight than Leaflet to accomplish this.]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I’m Nic. I have a PhD in Statistics and have been working in R for 15 years.\nAs a data scientist and self-taught developer, I spent years feeling like I wasn’t technical enough to understand a lot of software engineering concepts - until I realised it wasn’t me; it was the content and how it was explained. I’ve lost count of how many times I’ve gone from “this is impossible” to “oh, I just needed someone to explain it properly.”\nThat’s what I try to do for others - make complex stuff accessible."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\n\n\n\n\nI’ve worked across pharma, public health, academia, and startups - on everything from teaching hundreds of new programmers how to work with R, to maintaining popular open source packages, to deploying R code in production environments where scalability matters.\nI’ve built dashboards and internal tools supporting hundreds of concurrent users deployed across the NHS, and developed and led sold-out workshops at major R conferences including EARL and posit::conf."
  },
  {
    "objectID": "about.html#teaching",
    "href": "about.html#teaching",
    "title": "About Me",
    "section": "Teaching",
    "text": "Teaching\n\n\nI’ve taught thousands of people through online courses, conference workshops, and private training. Topics I teach include:\n\nBig Data in R with Arrow and Parquet\nLLMs and AI tools in R\nCI/CD and deployment\nR package development\n\nSee my upcoming workshops or get in touch about private training for your team."
  },
  {
    "objectID": "about.html#open-source-writing",
    "href": "about.html#open-source-writing",
    "title": "About Me",
    "section": "Open Source & Writing",
    "text": "Open Source & Writing\n\n\n\n\n\nI’m one of the core maintainers of the Apache Arrow R project, where I’ve served as package maintainer and contributed extensively to dplyr support for Arrow.\nIn 2024, I co-authored Scaling Up with R and Arrow, published by CRC Press."
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "Learn Arrow and Parquet to make large-scale analysis fast, reliable, and reproducible.\nData analysis pipelines with larger-than-memory data are becoming more and more commonplace, but it’s not uncommon to hit a wall when data gets too big for your R session. CSVs end up taking forever to load, and even when they do, analyses grind to a halt. This course shows you how to handle big data in R using open, modern tools.\n\n\n\nAnalyse larger-than-memory data without learning new syntax: Use {arrow} with {dplyr} to query millions of rows without crashing R\nExercise fine control over data types to avoid common large data pipeline problems\nConvert messy CSVs into space-saving, shareable Parquet files\nOptimise Arrow datasets to take full advantage of your hardware\nBuild reproducible workflows using formats and tools designed for long-term reliability\n\n\n\n\n4-5 March 2026 (Online via Google Meet)\n\nSession 1: Wed 4th March - 2:00PM - 5:30PM GMT\nSession 2: Thu 5th March - 2:00PM - 5:30PM GMT\n\nAttendee numbers are capped to ensure an interactive experience with opportunities to ask questions and get 1:1 help.\nBook your place\n\n\n\nThis course assumes you have basic knowledge of dplyr syntax. It’s for you if you:\n\nwant to learn how to work with tabular data that is too large to fit in memory using existing R and tidyverse syntax implemented in Arrow\nwant to learn about Parquet and other file formats that are powerful alternatives to CSV files\nwant to learn how to engineer your tabular data storage for more performant access and analysis with Apache Arrow\n\n\n\n\n\n“I had the privilege of attending Nic Crane’s ‘Big Data in R with Arrow’ workshop, and it fundamentally changed how I approach large-scale data processing. Despite having worked with Arrow and DuckDB for a couple of years prior, Nic’s workshop gave me a much deeper understanding of the relationship between Parquet file management and optimizing data processing with Arrow. What stands out most about Nic as an instructor is their ability to take complex technical concepts and make them not just comprehensible, but genuinely exciting to learn.”\n— Javier Orraca-Deatcu, Lead Machine Learning Engineer at Centene"
  },
  {
    "objectID": "workshops.html#big-data-in-r-with-arrow-and-parquet",
    "href": "workshops.html#big-data-in-r-with-arrow-and-parquet",
    "title": "Workshops",
    "section": "",
    "text": "Learn Arrow and Parquet to make large-scale analysis fast, reliable, and reproducible.\nData analysis pipelines with larger-than-memory data are becoming more and more commonplace, but it’s not uncommon to hit a wall when data gets too big for your R session. CSVs end up taking forever to load, and even when they do, analyses grind to a halt. This course shows you how to handle big data in R using open, modern tools.\n\n\n\nAnalyse larger-than-memory data without learning new syntax: Use {arrow} with {dplyr} to query millions of rows without crashing R\nExercise fine control over data types to avoid common large data pipeline problems\nConvert messy CSVs into space-saving, shareable Parquet files\nOptimise Arrow datasets to take full advantage of your hardware\nBuild reproducible workflows using formats and tools designed for long-term reliability\n\n\n\n\n4-5 March 2026 (Online via Google Meet)\n\nSession 1: Wed 4th March - 2:00PM - 5:30PM GMT\nSession 2: Thu 5th March - 2:00PM - 5:30PM GMT\n\nAttendee numbers are capped to ensure an interactive experience with opportunities to ask questions and get 1:1 help.\nBook your place\n\n\n\nThis course assumes you have basic knowledge of dplyr syntax. It’s for you if you:\n\nwant to learn how to work with tabular data that is too large to fit in memory using existing R and tidyverse syntax implemented in Arrow\nwant to learn about Parquet and other file formats that are powerful alternatives to CSV files\nwant to learn how to engineer your tabular data storage for more performant access and analysis with Apache Arrow\n\n\n\n\n\n“I had the privilege of attending Nic Crane’s ‘Big Data in R with Arrow’ workshop, and it fundamentally changed how I approach large-scale data processing. Despite having worked with Arrow and DuckDB for a couple of years prior, Nic’s workshop gave me a much deeper understanding of the relationship between Parquet file management and optimizing data processing with Arrow. What stands out most about Nic as an instructor is their ability to take complex technical concepts and make them not just comprehensible, but genuinely exciting to learn.”\n— Javier Orraca-Deatcu, Lead Machine Learning Engineer at Centene"
  },
  {
    "objectID": "learn-arrow.html",
    "href": "learn-arrow.html",
    "title": "Learn Arrow",
    "section": "",
    "text": "Resources for learning how to work with larger-than-memory data in R using Arrow and Parquet:"
  },
  {
    "objectID": "learn-arrow.html#the-book",
    "href": "learn-arrow.html#the-book",
    "title": "Learn Arrow",
    "section": "The Book",
    "text": "The Book\n\n\n\n\n\n\nScaling Up with R and Arrow is available to read online for free at arrowrbook.com. I wrote it with Jonathan Keane and Neal Richardson, and it covers everything from getting started with Arrow to optimising large-scale data workflows."
  },
  {
    "objectID": "learn-arrow.html#public-workshops",
    "href": "learn-arrow.html#public-workshops",
    "title": "Learn Arrow",
    "section": "Public Workshops",
    "text": "Public Workshops\nI run public workshops on working with larger-than-memory data in R using Arrow and Parquet. You’ll learn how to:\n\nAnalyse larger-than-memory data using dplyr syntax you already know\nConvert CSVs into space-saving Parquet files\nOptimise datasets to take full advantage of your hardware\n\nThe workshops are hands-on with exercises to help you apply what you’ve learned. Attendee numbers are capped to keep things interactive with opportunities for 1:1 help.\nView upcoming dates and book your place"
  },
  {
    "objectID": "learn-arrow.html#private-training",
    "href": "learn-arrow.html#private-training",
    "title": "Learn Arrow",
    "section": "Private Training",
    "text": "Private Training\nI also deliver private workshops for teams. If you’d like to discuss training for your organisation, email me at nic@ncdatalabs.com."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Me",
    "section": "",
    "text": "Loading…"
  },
  {
    "objectID": "talks/shinyinprod2025/index.html",
    "href": "talks/shinyinprod2025/index.html",
    "title": "htmlwidgets Are a Secret Sauce in R",
    "section": "",
    "text": "htmlwidgets quietly power some of the most compelling Shiny apps out there, but writing them from scratch can be fiddly and time-consuming. In this talk, we kick things off by taking an audience-sourced ingredient list and asking a large language model to whip up a fresh htmlwidget. Then we plate up a version we prepared earlier - also model-generated - but chopped, seasoned, and finished with our own touches. Along the way, we explore how LLMs can assist in crafting htmlwidgets that reflect your flavour of R - from tidy eval to package structure - rather than sticking to a bland house style.\nPresented with Charlotte Hadley at Shiny in Production 2025."
  },
  {
    "objectID": "talks/rstudioconf2019/index.html",
    "href": "talks/rstudioconf2019/index.html",
    "title": "The future’s Shiny: Pioneering genomic medicine in R",
    "section": "",
    "text": "Shiny's expanding capabilities are rapidly transforming how it is used in an enterprise. This talk details the creation of a large-scale application, supporting hundreds of concurrent users, making use of the future and promises packages. The 100,000 genomes project is an ambitious exercise that follows on from the Human Genome Project - aiming to put the UK at the forefront of genomic medicine, with the NHS as the first health service in the world to offer precision medicine to patients with rare diseases and cancer. Data is at the heart of this project; not only the outputs of the genomic sequencing, but vast amounts of metadata used to track progress against the 100,000 genome target and the status and path of each case through the sample tracking pipeline. In order to make this data readily available to stakeholders, Shiny was used to create an application containing multiple interactive dashboards. A scaled-up version of the app is being rolled out in early 2019 to a much larger audience to support the National Genomics Informatics Service, with the challenge of creating a complex app capable of supporting so many users without grinding to a halt. In this talk, I will explain why Shiny was the obvious technology choice for this task, and discuss the design decisions which enabled this project’s success."
  },
  {
    "objectID": "talks/rstudioconf2022/index.html",
    "href": "talks/rstudioconf2022/index.html",
    "title": "What they forgot to teach you about becoming an open source contributor",
    "section": "",
    "text": "Getting involved in open source is an amazing learning experience and helps you grow your skills as a developer, but to a new contributor there are so many unknown factors about open source projects. In this talk, I’m going to discuss my journey from occasional open source contributor to full time project maintainer, and answer questions such as: what does it look like from the inside of an open-source project? What’s a good way to get involved, and really learn the internals of an R package? How can I navigate the social dynamics of an open source project? How do contributions look entirely different from the point of view of a contributor versus a maintainer?"
  },
  {
    "objectID": "talks/positconf2025/index.html",
    "href": "talks/positconf2025/index.html",
    "title": "Hacking Productivity with LLMs",
    "section": "",
    "text": "Over the past 18 months, I’ve used large language models (LLMs) in a series of experiments to improve my productivity as a developer on open source projects in R. I wanted to see if these tools could help with the repetitive, frustrating, or time-consuming parts of development, freeing me up to focus on the interesting problems. The results were mixed: some of these experiments were wildly successful, but others were clear failures.\nIn this talk, I share what worked, what didn’t, and what I learned - exploring the kinds of tasks LLMs are good for and where they fall short, how to use LLMs to build lasting skills, and why in some cases, investing in human connection pays off more than AI assistance.\nPresented at posit::conf(2025)."
  },
  {
    "objectID": "projects/migration/index.html",
    "href": "projects/migration/index.html",
    "title": "Scoping a Large-Scale SAS to R Migration",
    "section": "",
    "text": "As part of a consultancy engagement, I audited a large SAS codebase to help a client understand the feasibility and risk profile of migrating to R. The codebase was business-critical and had grown organically over many years.\nWhat I worked on: - Static code analysis of SAS codebase to estimate size and complexity (e.g. SQL-style vs. custom PROC logic) - Identified areas where statistical methods or parameter defaults might behave differently in R - Delivered a scoped migration plan to help inform budget, risk, and staffing decisions\nThe audit helped the client avoid assumptions about direct conversion and provided a structured way to evaluate next steps."
  },
  {
    "objectID": "projects/refactoring/index.html",
    "href": "projects/refactoring/index.html",
    "title": "Refactoring a Legacy Shiny App for Maintainability",
    "section": "",
    "text": "I led a team within a large organisation responsible for maintaining an internal Shiny app used to manage R package installation requests. The codebase had become increasingly fragile and hard to maintain, with tightly coupled logic and growing technical debt.\nWhat I worked on: - Refactored reactive logic to simplify control flow and heavily modularised code to support maintainability - Introduced testing strategies and CI workflows - Improved onboarding for new developers by making the codebase easier to understand\nThe changes made the application significantly more maintainable and safer to extend as the team evolved, reducing development time considerably."
  },
  {
    "objectID": "posts-archive/2023wrapup/index.html",
    "href": "posts-archive/2023wrapup/index.html",
    "title": "Reflections on 2023",
    "section": "",
    "text": "Here’s a summary of my highlights of 2023!"
  },
  {
    "objectID": "posts-archive/2023wrapup/index.html#arrow-positconf-workshop",
    "href": "posts-archive/2023wrapup/index.html#arrow-positconf-workshop",
    "title": "Reflections on 2023",
    "section": "Arrow posit::conf workshop",
    "text": "Arrow posit::conf workshop\nThe biggest professional achievement for me was having the privilege of being invited to co-present a workshop with Steph Hazlitt about Arrow at posit::conf. Steph is fantastic to work with, and while writing and delivering the workshop was hard work, it paid off in the end - we received really positive feedback from attendees. The experience reignited my passion for teaching, and desire to develop those skills. I’ve applied for Carpentries instructor training, which I’m still waiting to hear back about, and I’m excited to also be working with Steph on creating a 2 hour version of the workshop, which we’re planning on teaching at user events in 2024.\n\n\n\nHanging out with some awesome people at posit::conf 2023!"
  },
  {
    "objectID": "posts-archive/2023wrapup/index.html#arrow-r-maintenance-bot",
    "href": "posts-archive/2023wrapup/index.html#arrow-r-maintenance-bot",
    "title": "Reflections on 2023",
    "section": "Arrow R Maintenance bot",
    "text": "Arrow R Maintenance bot\nAnother project I enjoyed working on was creating the Arrow maintenance bot. It pulls data from the GitHub API, aggregates the relevant bits, and then posts it to a Zulip channel (though also works for Slack). It was an itch I’d wanted to scratch for a long time, as I’d previously got this information by clicking on various bookmarks every so often whenever I remembered. The output is shown in the screenshot below; basically, it summarises new bug tickets which need investigating, investigated bugs which need a fix, and open PRs which may need reviewing. \nThere are more aspects of arrow maintenance which I’d love to automate and pull together in dashboard form - I’ve been discussing this with a friend, and I hope we find the time to work on this in 2024!"
  },
  {
    "objectID": "posts-archive/2023wrapup/index.html#arrow-maintenance-and-book",
    "href": "posts-archive/2023wrapup/index.html#arrow-maintenance-and-book",
    "title": "Reflections on 2023",
    "section": "Arrow maintenance and book",
    "text": "Arrow maintenance and book\nIn January, I became the official package maintainer on CRAN. It took my some time to get my head around this; it’s a daunting responsibility, and in reality I’m part of a wider team, all of whom play an important part.\nThe amount of time I’ve been able to spend on Arrow has dropped significantly since I left Voltron Data, though I’m hoping to be able to spend more time this year on it. I feel like my focus will shift a little, away from code, and more towards encouraging new contributors, as well as teaching people how to use the package.\nMost excitingly though, is the book which I am co-authoring! This is taking up the main chunk of my Arrow-related time, but the aim is to have the manuscript submitted to the publishers by summer, if not sooner."
  },
  {
    "objectID": "posts-archive/2023wrapup/index.html#c",
    "href": "posts-archive/2023wrapup/index.html#c",
    "title": "Reflections on 2023",
    "section": "C++",
    "text": "C++\nHaving interacted with bits of C++ code while working on Arrow, I decided to spend time learning C++ more formally. I’ve generally found this tricky - frankly, higher-level languages seem to have more engaging content out there. I made a start on the Udacity C++ Nanodegree, which I completed over half of, but was limited on time and so paused this - I plan to pick it up again next year though. I also tried a more practical approach, and decided to make a PR to the Arrow C++ codebase - one of the PRs I am most proud of this year is this one, adding support for the Substrait cast expression. It was hard, though most of the complexity came from peripheral tasks, like working out how to run the code interactively in the debugger."
  },
  {
    "objectID": "posts-archive/2023wrapup/index.html#joining-the-rladies-global-team",
    "href": "posts-archive/2023wrapup/index.html#joining-the-rladies-global-team",
    "title": "Reflections on 2023",
    "section": "Joining the RLadies Global team",
    "text": "Joining the RLadies Global team\nI spent some time thinking about how I could engage more with the wider R community, and so I volunteered to join the RLadies global team, helping set up new chapters on meetup.com. I’ve not done much here yet, but am delighted to be part of the organisation, and hope to be able to help out more in 2024"
  },
  {
    "objectID": "posts-archive/2023wrapup/index.html#career-break-and-plans-for-2024-recurse-center",
    "href": "posts-archive/2023wrapup/index.html#career-break-and-plans-for-2024-recurse-center",
    "title": "Reflections on 2023",
    "section": "Career break and plans for 2024 (Recurse Center!)",
    "text": "Career break and plans for 2024 (Recurse Center!)\nLast, but absolutely not least, is my decision to take a career break. After leaving Voltron Data, I’d been working part-time in a contractor capacity, alongside working on the Arrow book, but I decided I wanted to narrow my focus and take some time out for me.\nI’m taking a break and spending the next couple of months focusing on writing. Once most of the book is done, I plan to spend more time on my open source work in general, which will involve teaching and talking about Arrow, as well as pushing forward a few side projects.\nThe most exciting news I’ve had this year is that I applied to the Recurse Center a few weeks ago. RC is like a writer’s retreat for programmers, and attendees spend 12 weeks together working on programming problems of their choice. I want to get to grips with R’s internals, and learn about the C API, as well as levelling up my C++ skills in general. I’m delighted to say that I was accepted to attend this morning, and will be joining the spring batch in late March! I’ve got a ton of ideas for projects I want to work on, but will have to see how much time I ultimately end up having.\nLonger-term - I don’t know. Ultimately, I’d love to get paid to work on open source again, but those opportunities are few and far between. I also have some professional goals I’d like to hit before I’m going to be in the right headspace to engage with this too - between writing the book and my plans for Recurse Center, I’ll be there pretty soon.\nThis year has been a real rollercoaster, but I’m extremely grateful to all my friends and mentors who have helped me during this time."
  },
  {
    "objectID": "posts-archive/clineshiny/index.html",
    "href": "posts-archive/clineshiny/index.html",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "",
    "text": "As a fun bit of prep for Appsilon’s upcoming AI-Powered Shiny Hackathon on March 22, 2025, I decided to do a practice run using Cline, an AI coding assistant. My goal was to see how AI can help or hinder the development of a Shiny application.\nI decided to see where I could get in an hour today, and this blog post discusses my experience, highlighting the capabilities, quirks, and costs of using AI for R and Shiny app development."
  },
  {
    "objectID": "posts-archive/clineshiny/index.html#the-hackathon-challenge",
    "href": "posts-archive/clineshiny/index.html#the-hackathon-challenge",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "📌 The Hackathon Challenge",
    "text": "📌 The Hackathon Challenge\nThe hackathon is judged on:\n\nHow well the app meets the challenge objectives\nAdditional features built\nUI design and user experience\nCode clarity and maintainability\n\nThe details of the hackathon will be revealed on the day, so I decided to try to come up with a small task which may have some things in common - creating a Shiny app which visualises and analyses a dataset."
  },
  {
    "objectID": "posts-archive/clineshiny/index.html#data-acquisition-nyc-open-data",
    "href": "posts-archive/clineshiny/index.html#data-acquisition-nyc-open-data",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "📂 Data Acquisition: NYC Open Data",
    "text": "📂 Data Acquisition: NYC Open Data\nI decided to take a look at NYC Open Data and within that, my partner and I agreed that the restaurant health inspection dataset looks the most interesting.\nNYC Open Data provides APIs and browser-based tools for exploring datasets. Instead of using APIs, I opted to download the datasets manually for simplicity.\n\nInspection Results Dataset (120MB CSV)\nData Dictionary (Excel file)\nAdditional Documentation (Word file)\n\nAt this point, I didn’t check the documentation—I wanted to see how much Cline could figure out on its own."
  },
  {
    "objectID": "posts-archive/clineshiny/index.html#enter-cline-ai-coding-assistant",
    "href": "posts-archive/clineshiny/index.html#enter-cline-ai-coding-assistant",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "🛠️ Enter Cline: AI Coding Assistant",
    "text": "🛠️ Enter Cline: AI Coding Assistant\nCline is an AI-powered coding tool that integrates into VS Code and Positron. It autonomously creates and edits files, executes commands, and interacts with code, all while asking for user approval at each step.\n\n🔹 Key Features of Cline:\n\nPlan Mode: Outlines a roadmap before execution\nAct Mode: Runs code when switched to execution mode\nTransparent Cost Tracking: Displays API usage and expenses\n\nI started with a broad prompt:\n\n‘Using all of the resources in the data directory, take a look at the data, and come up with some ideas for interesting analyses, such as “which restaurants have the worst health scores but remain very popular” - focus on what is available in the data and what is unexpected and humorous’\n\nCline responded by:\n\nListing the available files and guessing their contents\nAsking for permission to read each file\nSuccessfully reading the Word doc\nTrying (and failing) to read the Excel file\n\n\n\n\nCline File Summary"
  },
  {
    "objectID": "posts-archive/clineshiny/index.html#data-exploration-and-analysis",
    "href": "posts-archive/clineshiny/index.html#data-exploration-and-analysis",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "🔍 Data Exploration and Analysis",
    "text": "🔍 Data Exploration and Analysis\nCline wanted to open the full 120MB dataset but since the file was large, my partner mentioned that there might be costs associated with sending data to the AI. I mentioned this to Cline, and so it opted instead to sample the first few rows.\nAfter some failed attempts with PowerShell, I nudged it toward using R instead, which worked much better.\n\n\n\nPowerShell Issue\n\n\nI really like how the responses are in a human-readable format while planning actions to take, so that I could see the equivalent of its line of thought. It had proposed analyses such as:\n\nIdentifying popular restaurants with poor health scores\nAnalysing violation trends across neighborhoods\nExamining seasonal variations in health violations\n\nSome ideas sounded interesting, but others were a bit off. This reflected the trend I’d seen in chatGPT where unless prompted to do otherwise, a large number of ideas are returned; some more useful than others.\nI pressed it on some of them, at which point it was able to propose alternatives. There was still a great deal of input needed from me to decide which idea would be the most sensible to go with.\n\n\n\nInspection Proxy Popularity Question"
  },
  {
    "objectID": "posts-archive/clineshiny/index.html#building-the-shiny-app",
    "href": "posts-archive/clineshiny/index.html#building-the-shiny-app",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "🎨 Building the Shiny App",
    "text": "🎨 Building the Shiny App\nWith the analysis ideas outlined, I instructed Cline to build a Shiny app to visualise the insights.\n\n\n\nInitial R Code Approach\n\n\n\n🔹 What Cline Generated:\n\nA UI with slider inputs\nBar charts using the viridis color palette\nIcons for different metrics\n\nIssue #1:\n\nCline overused Plotly, adding unnecessary complexity.\nI switched most plots back to ggplot2 for clarity.\n\nIssue #2:\n\nSome colors were misleading—viridis was applied without meaning, making bars look color-coded incorrectly.\n\nIssue #3:\nThe app was all in one huge file - so I asked Cline to make it more modular, again, remaining vague to see what it did."
  },
  {
    "objectID": "posts-archive/clineshiny/index.html#the-app-itself",
    "href": "posts-archive/clineshiny/index.html#the-app-itself",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "📊 The App Itself",
    "text": "📊 The App Itself\nThe main page of the app itself can be seen below. It looks pretty decent, right?\n Except for that it kinda wasn’t! This overview page was fine, but the individual subpages were suffering from all sorts of issues, from truncated output in DT outputs, to it being unclear what some plots were trying to show. With some time and effort I could have unpicked it, but at this point I was wondering if it would have been quicker to code from scratch."
  },
  {
    "objectID": "posts-archive/clineshiny/index.html#refactoring-and-modularisation",
    "href": "posts-archive/clineshiny/index.html#refactoring-and-modularisation",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "🏗️ Refactoring and Modularisation",
    "text": "🏗️ Refactoring and Modularisation\nAt this point Cline split the app into ui.R, server.R and a few other supporting files, though not as modular as I’d have liked. I tried again, this time seeing if it would use the {golem} framework - with much more success.\nCline successfully:\n✅ Created a new Golem project directory\n✅ Structured the app as an R package\n✅ Included a DESCRIPTION file\nHowever, I had to manually fix some file paths to make everything work.\n\n\n\nRefactoring with Golem"
  },
  {
    "objectID": "posts-archive/clineshiny/index.html#the-cost-of-ai-assistance",
    "href": "posts-archive/clineshiny/index.html#the-cost-of-ai-assistance",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "💰 The Cost of AI Assistance",
    "text": "💰 The Cost of AI Assistance\nCline provides a breakdown of API costs per request. After one hour of work, I had spent:\n💰 $3 total\nInterestingly, some human-like responses cost more than code generation! For example, a single conceptual AI response cost $0.17, more than some entire code snippets.\n\n\n\nCost Breakdown"
  },
  {
    "objectID": "posts-archive/clineshiny/index.html#final-thoughts-ais-role-in-shiny-development",
    "href": "posts-archive/clineshiny/index.html#final-thoughts-ais-role-in-shiny-development",
    "title": "Exploring AI-Powered Shiny App Development with Cline and Positron",
    "section": "🏁 Final Thoughts: AI’s Role in Shiny Development",
    "text": "🏁 Final Thoughts: AI’s Role in Shiny Development\nThis experiment showed me a few important things:\n\nVague prompts get mixed results\nAI is better at writing code than about making decisions about data science and how best to represent data\nCosts can add up quickly and being strategic about which aspects to use Cline for feels like the best way forward\n\nThis made me really reflect on the role of AI here. On an app development team, would you rather have 1 person who does it all, or a team of people with different specialisations? I think the latter would work better, for the hackathon itself, I’m going to experiment with a few different approaches such as:\n\nSetting up the app structure myself in advance, and getting Cline to fill in the gaps based on what’s already there\nGPT product manager - putting together a prompt around constructing really well-defined requirements, and using that to help me tightly define what is being created before passing those requirements onto an AI like Cline to create them\ndesigning any plots myself - there may be ways to improve on what was generated here today, but representing data in a human-understandable form might just be best done by humans\n\nI’m looking forward to Saturday’s hackathon - good luck to everyone else participating and I’m super excited to see what comes out of it! 🚀"
  },
  {
    "objectID": "learn-llms.html",
    "href": "learn-llms.html",
    "title": "Learn LLMs in R",
    "section": "",
    "text": "Resources for learning how to work with LLMs in R:"
  },
  {
    "objectID": "learn-llms.html#athlyticz",
    "href": "learn-llms.html#athlyticz",
    "title": "Learn LLMs in R",
    "section": "Athlyticz: ML and LLMs in R",
    "text": "Athlyticz: ML and LLMs in R\nA comprehensive course I developed with Christoph Scheuch covering the full journey from machine learning fundamentals to building LLM-powered applications in R.\nThe course covers:\n\nML foundations with tidymodels: model evaluation, resampling, tuning, preprocessing\nRegression and classification models including neural networks\nModel deployment with vetiver and orbital\nWorking with LLMs using ellmer\nRetrieval-augmented generation (RAG) for building apps with your own data\n\nAimed at R users comfortable with the tidyverse who want to build and deploy ML and LLM applications.\nSign up at Athlyticz"
  },
  {
    "objectID": "learn-llms.html#self-paced",
    "href": "learn-llms.html#self-paced",
    "title": "Learn LLMs in R",
    "section": "LLMs in R (Self-Paced)",
    "text": "LLMs in R (Self-Paced)\nLearn to work with large language models in R at your own pace.\nView course"
  },
  {
    "objectID": "learn-llms.html#cohort",
    "href": "learn-llms.html#cohort",
    "title": "Learn LLMs in R",
    "section": "Practical AI for R (Cohort)",
    "text": "Practical AI for R (Cohort)\nA cohort-based course with live sessions and group learning.\nView course"
  },
  {
    "objectID": "posts-archive/c_deepr/index.html",
    "href": "posts-archive/c_deepr/index.html",
    "title": "Explaining the C bits at the start of ‘Deep R Programming Ch14: Interfacing compiled code’",
    "section": "",
    "text": "One of my goals for next year is to get a deeper understanding of R’s C API. I’m making a start on this by reading Chapter 14 of Deep R Programming by Marek Gagolewski: “Interfacing compiled code”. It’s a great resource, though the chapter preface states “we assume basic knowledge of the C language”. I do not have this knowledge My C knowledge is fairly limited, and so this blog post will pull out some of the details from that chapter, especially bits where I’ve had to go “wtf is that?!” or remind myself by googling. My current level of C knowledge at the moment is pretty minimal; the main things I know are:\n\nC is a compiled language (as opposed to an interpreted language) and C code needs compiling before it can be run\nA lot of R’s internals are based on C\nC is a statically-type language; this means that variable types are defined when the variable is created and cannot later change other than via explicit manual casting\n\nAnd that’s basically it! I imagine there are other bits I don’t realise I know which I’ve picked up from being an R package maintainer and dabbling in a few C++ tutorials, but I’ll try to explain everything as much as possible. OK, let’s do this!\nThe book chapter provides an example package with a simple C function implemented, and walks us through the code step-by-step. Great!\nSection 14.1.1 starts off with an example of a C function defined in src/cfunc.h. I guess the first thing to note is the location of the file - in the package’s src directory. This is where any compiled code needs to go, typically C or C++ code, or even Fortran if you’re really going old-school.\n\nHeader files and source files\nAnother thing to note here is the file name, which ends in .h. C code can be divided into header files (ending in .h) and source files (ending in .c). Header files contain the function declarations (including variable types) and other things like macros (named bits of code for the pre-processor to work with). They’re sometimes referred to as the interface - they contain information about functions’ inputs and outputs - including the argument names and types.\n\n\nOnce-only includes\nThe first couple of lines of code in the header file contain these lines:\n#ifndef __CFUNS_H\n#define __CFUNS_H\nand the final line is\n#endif\nWhat’s happening here is that, often we can end up with projects containing multiple files, some of which source each other, and include them. We don’t want to end up with duplication of the headers which have been included, otherwise the compiler will raise an error, so we put them in an #ifndef wrapper, and give them a name. Basically, what we’re saying to the preprocessor here is that if this name hasn’t already been defined, defined it and include this code, but if it’s already defined then skip this.\n\n\nIncludes\nThe third line in src/cfunc.h is:\n#include &lt;stddef.h&gt;\nThis allows for the inclusion of a file from the C standard library, which has a few different variables included. The key one for us here is size_t, which is commonly used for iterating over items in arrays - we need this as we’ll be including a for loop in the definition of our function.\n\n\nThe preprocessor\nAbove I casually mentioned the C preprocessor a couple of times without defining it. A succinct and perhaps naive summary is that there are multiple steps in the compilation of C code. One of these phases is preprocessing and it involves things like processing any additional files we’ve said we wanted to include, and replacing macros with their definitions.\n\n\nDeclarations\nSo, the declaration for the function in the example looks like this\ndouble my_c_sum(const double* x, size_t n);\nIn words, this means that:\n\nit is a function which returns an object of type double\nthe function name is my_c_sum\nthe first argument is called x\nx is a pointer to a variable of type double\nx is a const variable, which means it won’t be modified in the body of the function\nthe second argument is called n\nn is of type size_t\n\nThis concept of a pointer just means that x contains the memory address of the double that we pass in, rather than a copy of the values in it. This prevents us from copying the values unnecessarily.\n\n\nSource file\nSource files contain the body of the function, sometimes called the implementation.\nThe code in the book chapter goes on to show the content of the file src/cfuncs.c. The first line of this file is:\n#include \"cfuns.h\"\nThis is including the header file we discussed above. The rest of the code in the source file contains the definition of my_c_sum:\n/* computes the sum of all elements in an array x of size n */\ndouble my_c_sum(const double* x, size_t n)\n{\n    double s = 0.0;\n    for (size_t i = 0; i &lt; n; ++i) {\n        /* this code does not treat potential missing values specially\n           (they are kinds of NaNs); to fix this, add:\n        if (ISNA(x[i])) return NA_REAL;  // #include &lt;R.h&gt;  */\n        s += x[i];\n    }\n    return s;\n}\nThe function signature here is identical to how it is defined in the header file. The for loop uses the n argument which was passed in to represent the size of the array to loop through. In numerous other languages we’d calculate the size of the array in the body of the function, but in C you cannot have an array of unknown size, and so it must be passed in as a parameter. I think this is to do with how the C compiler allocates memory; more modern C does have the concept of variable-length arrays.\nThe chapter goes on to discuss further examples which then show how to include a wrapper which can be called by R. I won’t discuss this here, as the text there is all explained well, and the contents are more specific to R’s C API, and not specifically just C-related topics.\nHere’s a summary of the C-related topics mentioned here:\n\nheader files and source files\nincludes\nonce-only includes\nthe preprocessor\nvariable-length arrays\nconst variables\npointers\nstatically-typed languages\ncompiled and interpreted languages"
  },
  {
    "objectID": "posts-archive/rc/index.html",
    "href": "posts-archive/rc/index.html",
    "title": "Recurse Center Return Statement",
    "section": "",
    "text": "I spent 3 months earlier this year at Recurse Center in New York. Here’s a summary of the highlights of my time there!"
  },
  {
    "objectID": "posts-archive/rc/index.html#what-is-recurse-center",
    "href": "posts-archive/rc/index.html#what-is-recurse-center",
    "title": "Recurse Center Return Statement",
    "section": "What is Recurse Center?",
    "text": "What is Recurse Center?\nThe simplest explanation of what Recurse Center is is that it’s like a writer’s retreat but for programmers. Participants have to apply to get in, and there are 3 stages. The first is a written application, followed by a conversational interview, and then a pair programming interview. Generally, the intention is to assess whether you’ll be a good fit for RC. The selection criteria are stated explicitly on the website, but to summarise, they’re looking for self-motivated people who enjoy programming and are pleasant to be around. It’s not about being an amazing programmer; though I met some people with seriously impressive skills there, I also met newer programmers who were enthusiastic and engaged.\nRC can be attended remotely online or in person, but I opted for the in-person experience to get the most out of it. The hub, the in-person location is in Brooklyn, and takes up 2 floors of an office building. The 4th floor is the main floor where people code, socialise, do pair programming, and hang out in the kitchen, presentation space, or the meeting rooms. The 5th floor is the quiet floor where people can do more focused work without interruption.\nRC has a code of conduct, as well as 4 more general social rules:\n\nNo well-actually’s\nNo feigned surprise\nNo backseat driving\nNo subtle -isms\n\n\n\n\nA printed poster of RC’s social rules\n\n\nAs well as social rules about what not to do, there are 3 guiding principles, or self-directives, about what to do:\n\nWork at the edge of your abilities\nBuild your volitional muscles\nLearn generously\n\nThese explicit expectations lead to a fantastic learning environment where some of the more toxic elements that can sometimes be found in tech were avoided, and folks worked on project that really mattered to them and helped other people do the same.\nRecurse Center is a business, and makes its money from recruitment - RC works with partner companies looking for software engineers and similar roles, though admission to RC isn’t predicated on looking for employment via them, and is never pushed on people."
  },
  {
    "objectID": "posts-archive/rc/index.html#why-did-i-go",
    "href": "posts-archive/rc/index.html#why-did-i-go",
    "title": "Recurse Center Return Statement",
    "section": "Why did I go?",
    "text": "Why did I go?\nWhen I decided to go to RC, I was at the beginning of a career break. I had done some consultancy/contract work early on, but had decided that I needed a proper pause. I had begun working on the book on Arrow that I’m co-authoring, and splitting my attention between the two things wasn’t working well for me. I also wanted some focused learning time - I had a ton of side projects I’d been meaning to work on but had never gotten moving and I felt like this would be a great opportunity to push them forwards.\nMy secondary motivation was how I was feeling at the time. I was lacking confidence in some of my technical skills, and know from experience that the best way to tackle that is learn new things and get things done, to disprove some of the insecurities I was having.\n\n\n\nA cross-stitched piece on the wall at RC with the text “be gay find primes”"
  },
  {
    "objectID": "posts-archive/rc/index.html#what-did-i-intend-to-work-on",
    "href": "posts-archive/rc/index.html#what-did-i-intend-to-work-on",
    "title": "Recurse Center Return Statement",
    "section": "What did I intend to work on?",
    "text": "What did I intend to work on?\nBefore I went to RC, I had a huge list of potential things to work on. I didn’t plan on doing all of these, but this was my initial brain dump of everything I thought might be useful or interesting in future:\n\nRead “Writing R Extensions”\nDo the course Nand2Tetris to get a better foundation of “lower level stuff”\nLearn C\nLearn more about the finer details of how building R packages work\ncontribute to parsermd so it works for Quarto markdown documents\nfinish the Udacity C++ nanodegree I’d done some of a while ago\nlearn more about the C++ build process\nDo all of the C sessions of CS50\nDo some work on the Arrow book\n\n\n\n\nSome of the retro machines at the RC hub."
  },
  {
    "objectID": "posts-archive/rc/index.html#what-did-i-actually-work-on",
    "href": "posts-archive/rc/index.html#what-did-i-actually-work-on",
    "title": "Recurse Center Return Statement",
    "section": "What did I actually work on?",
    "text": "What did I actually work on?\nOne of the principles of RC is to use your volitional muscles and work on what interests you rather than things you think you “should” work on. There’s a balance to be struck here, but there was a great exercise early on where we were guided through questions to ask ourselves to work out what was going to be the best things to focus on.\nIn the end, I didn’t get through all of the things on my original list, but had a much more exciting time doing the following things:\n\nwent through most of Nand2Tetris\nplayed around with compilation and decompilation and how compiler flags change the code produced\nplayed with assembly code\nstarting going through the book Crafting Interpreters and plan to do more another time!\nspent lots of time pairing with other people on my and other people’s code and developing a love for pair programming\ndid a few presentations on Nand2Tetris and why it’s such a great course!\nwrote an R-based Quarto markdown parser which implements recursive descent parsing\ndelved into the Quarto codebase and developed a new version of the parser which takes advantage of pandoc instead of doing everything the hard way\n\nAnd alongside this:\n\ndid a ton of writing of the Arrow book I’m co-authoring\nfinished off interviewing for a job I’d applied for at Novartis which I’m super excited to start tomorrow!\nattended 2 days of Carpentries instructor training\nhad regular lessons with a public speaking tutor\nsubmitted and got accepted to co-present a talk to posit::conf\nhanded over arrow maintainership to the fantastic Jon Keane"
  },
  {
    "objectID": "posts-archive/rc/index.html#what-did-i-learn",
    "href": "posts-archive/rc/index.html#what-did-i-learn",
    "title": "Recurse Center Return Statement",
    "section": "What did I learn?",
    "text": "What did I learn?\n\nlow-level programming concepts felt out of reach for a long time; most docs are aimed at people who already understand the tech being described, there’s a lot of foundational computer science knowledge that is assumed that I didn’t have, and it’s hard to know where to find good learning resources. However, I learned that actually, it isn’t nearly as out of reach as I’d thought and all I really need is to find that one good resource, and have time to explore it. Nand2Tetris is an awesome course!\na solid community of peers is invaluable. I’ve been lucky enough in my career so far to mostly have either a community of peers or excellent mentors, but in future I’m going to aim to make sure I have both, whether that’s via a job role or engaging with wider programming communities more fully.\npair programming is a wonderful thing and is massively underutilised. Pairing shouldn’t only be utilised when there’s a bug, but be a regular practice. It helps share assumptions and norms,\nworking on the things I want to as well as the things I think I should work on is so key to productivity\nto listen to my gut when it comes to my own learning. I have had a frustration for years with public speaking where I know I’m not terrible at it, want to be better, but asking friends and colleagues for feedback often led to (lovely, kind, well-meaning) validation but not the actionable feedback I wanted. Working with a tutor has been amazing for helping me identify my strengths (I convey passion and excitement for my topic, and especially when I teach, really try to connect with my audience) and weaknesses (my natural speaking speed is about 3 times what it should be, and I could work on structuring information better within a talk)\nnot to mistake being limited for time to work on a problem with not being able to solve the problem\n\n\n\n\nSome craft projects which were on display at RC"
  },
  {
    "objectID": "posts-archive/rc/index.html#conclusion",
    "href": "posts-archive/rc/index.html#conclusion",
    "title": "Recurse Center Return Statement",
    "section": "Conclusion",
    "text": "Conclusion\nI can honestly describe the 3 months I spent at Recurse Center as the best solid 3 month period of my life. I learned a huge amount about, not only programming, but my own motivation. I made some amazing friends - my fellow RC participants and the faculty there are some of the coolest people I’ve ever met - and will be staying involved in the community in the longer term."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "htmlwidgets Are a Secret Sauce in R\n\n\n\n\n\n\nNic Crane & Charlotte Hadley\n\n\nOct 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHacking Productivity with LLMs\n\n\n\n\n\n\nNic Crane\n\n\nSep 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimenting with LLM Applications in R\n\n\n\n\n\n\nNic Crane\n\n\nJul 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\n\n\n\n\n\n\nNic Crane\n\n\nAug 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiently Engineering Bigger Data with Arrow\n\n\n\n\n\n\nNic Crane\n\n\nAug 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat they forgot to teach you about becoming an open source contributor\n\n\n\n\n\n\nNic Crane\n\n\nJul 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe future’s Shiny: Pioneering genomic medicine in R\n\n\n\n\n\n\nNic Crane\n\n\nJan 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nTen Steps To Becoming A Tidyverse Contributor\n\n\n\n\n\n\nNic Crane\n\n\nNov 27, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/nhs/index.html",
    "href": "projects/nhs/index.html",
    "title": "Supporting Genomic Medicine in the NHS with R and Shiny",
    "section": "",
    "text": "I contributed to a modular Shiny application used in the national rollout of genomic testing within the NHS, building on the 100,000 Genomes Project. It let operational staff, lab teams, and programme managers track genomic samples and view key metrics without needing to interact directly with backend systems.\nWhat I worked on: - UI and server logic across multiple Shiny sub-apps - Integration with APIs and SQL databases - Interactive visualisations using plotly, visNetwork, sunburstR and other htmlwidget packages - Profiling, refactoring, and scaling reactive code for performance\nThe app was built to support a national rollout and continues to be used across NHS teams."
  },
  {
    "objectID": "drafts/querychat-shiny/index.html",
    "href": "drafts/querychat-shiny/index.html",
    "title": "Exploring querychat: talk to your data in a Shiny app",
    "section": "",
    "text": "I was excited to see the release of the querychat package on CRAN recently. I first noticed this package on Posit’s GitHub, and was really curious to see what it did. In short, it lets you ask questions to a chatbot which then generates SQL and queries a data frame or database connection to answer your question."
  },
  {
    "objectID": "drafts/querychat-shiny/index.html#reproducibility-through-sql",
    "href": "drafts/querychat-shiny/index.html#reproducibility-through-sql",
    "title": "Exploring querychat: talk to your data in a Shiny app",
    "section": "Reproducibility through SQL",
    "text": "Reproducibility through SQL\nquerychat a really exciting and powerful tool because it allows you to overcome some of the key issues with using LLMs for data analysis. LLMs are pretty poor at many tasks to do with numbers, and in a data analysis context we need results which we can rely on, understand where they come from, and can be reproduced by others.\nWe can’t just feed our tabular data into an LLM and expect to get helpful results back, but querychat helps overcome some of these issues - we now have code that we can run again and get the same answers every time, and if we can trust that the generated SQL properly answers the question we asked, we now have something we can depend on more easily.\nHow this works is pretty neat; under the hood, querychat uses DuckDB; data frames are converted into DuckDB tables which can then be queried using SQL."
  },
  {
    "objectID": "drafts/querychat-shiny/index.html#the-schema",
    "href": "drafts/querychat-shiny/index.html#the-schema",
    "title": "Exploring querychat: talk to your data in a Shiny app",
    "section": "The schema",
    "text": "The schema\nquerychat generates a system prompt; the default one contains instructions on how to generate the required SQL, and some information about the database schema. In the case of mtcars, the schema looks like this:\nYou have access to a DuckDB SQL database with the following schema:\n\n&lt;database_schema&gt;\nTable: mtcars\nColumns:\n- mpg (FLOAT)\n  Range: 10.4 to 33.9\n- cyl (FLOAT)\n  Range: 4 to 8\n- disp (FLOAT)\n  Range: 71.1 to 472\n- hp (FLOAT)\n  Range: 52 to 335\n- drat (FLOAT)\n  Range: 2.76 to 4.93\n- wt (FLOAT)\n  Range: 1.513 to 5.424\n- qsec (FLOAT)\n  Range: 14.5 to 22.9\n- vs (FLOAT)\n  Range: 0 to 1\n- am (FLOAT)\n  Range: 0 to 1\n- gear (FLOAT)\n  Range: 3 to 5\n- carb (FLOAT)\n  Range: 1 to 8\n&lt;/database_schema&gt;\nIt contains column names, types, and value ranges, but crucially doesn’t contain our full dataset."
  },
  {
    "objectID": "drafts/querychat-shiny/index.html#filtering-the-data",
    "href": "drafts/querychat-shiny/index.html#filtering-the-data",
    "title": "Exploring querychat: talk to your data in a Shiny app",
    "section": "Filtering the data",
    "text": "Filtering the data\nYou can create the simplest possible app by calling the querychat_app() function.\n\nlibrary(querychat)\n\nquerychat_app(mtcars)\n\n\nWhen data is filtered, the SQL necessary to select a subset is generated, and the data displayed in the app is updated.\nThis doesn’t require access to the underlying data; just the schema so that appropriate queries can be generated based on the available data.\nThis is a simpler kind of query, but we can also perform aggregations, which work a little differently."
  },
  {
    "objectID": "drafts/querychat-shiny/index.html#aggregating-the-data",
    "href": "drafts/querychat-shiny/index.html#aggregating-the-data",
    "title": "Exploring querychat: talk to your data in a Shiny app",
    "section": "Aggregating the data",
    "text": "Aggregating the data\nWhen querychat has full access to the data (enabled by default), it can perform aggregations, the results of which are passed back to the LLM to to inform answers to questions.\n\nThis is pretty neat, but what if we don’t want to pass this data back to the LLM?\nOne feature of querychat that I like is this ability to enable partial data privacy. We can disable the tool that querychat uses to query the data in this way, only allowing it to perform filtering. We can’t answer as many interesting questions, but still have functionality while only passing the schema to the LLM.\nBy default both “query” and “update” tools are enabled, so setting tools = \"update\" disables the query tool entirely, and enables partial data privacy.\n\nlibrary(querychat)\n\nquerychat_app(mtcars, tools = \"update\")"
  },
  {
    "objectID": "drafts/querychat-shiny/index.html#learn-more",
    "href": "drafts/querychat-shiny/index.html#learn-more",
    "title": "Exploring querychat: talk to your data in a Shiny app",
    "section": "Learn more",
    "text": "Learn more\nLLMs are going to have a huge impact on how we communicate with and about data, and so the querychat package is a huge step forward in this regard. If you want to learn more, here’s a more detailed post on the Shiny blog by Veerle Eeftink - van Leemput, or check out her video below."
  },
  {
    "objectID": "talks/nyhackr/index.html",
    "href": "talks/nyhackr/index.html",
    "title": "Efficiently Engineering Bigger Data with Arrow",
    "section": "",
    "text": "Data analysis pipelines with larger-than-memory data are becoming more and more commonplace. There are often blurred lines between data science and data engineering, and knowing a bit of both is a sure-fire way to make your life easier when working with big datasets. In this talk, I will give an overview of the arrow R package and best practices for getting the most out of your data when working with bigger datasets. I’ll demo the dplyr interface to arrow, and give you some tips and tricks for getting the most out of arrow’s functionality, as well as applying data engineering principles to speed things up even more."
  },
  {
    "objectID": "talks/positconf2024_2/index.html",
    "href": "talks/positconf2024_2/index.html",
    "title": "Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail",
    "section": "",
    "text": "A talk on Arrow that Alenka Frim and I wrote and presented at posit::conf 2024.\nhttps://docs.google.com/presentation/d/1ViXduR7CNrz_9QFDk5snAlD3tFJpJs156-UEBxCHiks/edit?usp=sharing"
  },
  {
    "objectID": "talks/londonr/index.html",
    "href": "talks/londonr/index.html",
    "title": "Ten Steps To Becoming A Tidyverse Contributor",
    "section": "",
    "text": "A talk about getting involved in open source!\nSlides here"
  },
  {
    "objectID": "talks/user2025/index.html",
    "href": "talks/user2025/index.html",
    "title": "Experimenting with LLM Applications in R",
    "section": "",
    "text": "Large language models (LLMs) are surprisingly easy to use and at their core, they’re just an API call away. But how do you go from calling a model to actually building something useful?\nIn this talk, I share my experiences creating and deploying LLM-powered applications in R. I walk through different approaches I’ve tried, from incorporating LLMs into Shiny apps, improving my R code, and experimenting with different models and deployment options. Along the way, I highlight what worked, what didn’t, and what I learned in the process.\nWhether you’re curious about integrating LLMs into your own projects or just want to see what’s possible, this session offers a practical look at building with LLMs in R without the hype.\nPresented at useR! 2025."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr Nic Crane",
    "section": "",
    "text": "I help data teams scale up their R workflows - from handling larger-than-memory data to getting code into production.\nI specialise in big data with R (Arrow, Parquet) and training teams to use AI tools effectively. I run public courses for individuals and private training for organisations, and take on consulting projects.\nGet in touch if you’d like to work together."
  },
  {
    "objectID": "index.html#helping-data-teams-do-more-with-r.",
    "href": "index.html#helping-data-teams-do-more-with-r.",
    "title": "Dr Nic Crane",
    "section": "",
    "text": "I help data teams scale up their R workflows - from handling larger-than-memory data to getting code into production.\nI specialise in big data with R (Arrow, Parquet) and training teams to use AI tools effectively. I run public courses for individuals and private training for organisations, and take on consulting projects.\nGet in touch if you’d like to work together."
  },
  {
    "objectID": "index.html#get-in-touch-to-discuss-your-project",
    "href": "index.html#get-in-touch-to-discuss-your-project",
    "title": "Dr Nic Crane",
    "section": "Get in touch to discuss your project",
    "text": "Get in touch to discuss your project\nContact Me"
  },
  {
    "objectID": "index.html#what-people-say",
    "href": "index.html#what-people-say",
    "title": "Dr Nic Crane",
    "section": "What People Say",
    "text": "What People Say\n\n\n\n“I recently attended Nic’s training course on using AI with R. This was my first hands-on experience at interacting with AI agents through R and Nic walked us through from step 0 (pre-course preparation including setting up API keys), interacting with agents within the IDE and programmatically, setting up system prompts and then presenting latest developments at passing information into agents and formatting returned information. This is a topic where things are developing quickly, and Nic managed to walk the fine line between basic concepts and introducing the ‘latest greatest’. This was a good course and it certainly helped me ‘cross the threshold’ into using AI with R.”\n— Mike Smith, Senior Director Statistics, Pfizer\n\n\n\n\n“I had the privilege of attending Nic Crane’s ‘Big Data in R with Arrow’ workshop, and it fundamentally changed how I approach large-scale data processing. Despite having worked with Arrow and DuckDB for a couple of years prior, Nic’s workshop gave me a much deeper understanding of the relationship between Parquet file management and optimizing data processing with Arrow. What stands out most about Nic as an instructor is their ability to take complex technical concepts and make them not just comprehensible, but genuinely exciting to learn.”\n— Javier Orraca-Deatcu, Lead Machine Learning Engineer at Centene"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Dr Nic Crane",
    "section": "Resources",
    "text": "Resources\nI’ve been an active contributor to the Apache Arrow R package, where I’ve worked on implementing features, acting as package maintainer, managing complex build configuration, and extending dplyr support to Arrow. This work has influenced how I approach performance, reproducibility, and maintainability - both in open source and in client projects.\nIn 2024, I co-authored Scaling Up With R and Arrow, which was published by CRC Press and also available online at arrowrbook.com."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Past Work",
    "section": "",
    "text": "A selection of consulting projects I’ve worked on."
  },
  {
    "objectID": "projects.html#supporting-genomic-medicine-in-the-nhs",
    "href": "projects.html#supporting-genomic-medicine-in-the-nhs",
    "title": "Past Work",
    "section": "Supporting Genomic Medicine in the NHS",
    "text": "Supporting Genomic Medicine in the NHS\n\n\n\n\n\nI contributed to a modular Shiny application used in the national rollout of genomic testing within the NHS, building on the 100,000 Genomes Project. It let operational staff, lab teams, and programme managers track genomic samples and view key metrics without needing to interact directly with backend systems.\n\nUI and server logic across multiple Shiny sub-apps\nIntegration with APIs and SQL databases\nInteractive visualisations using plotly, visNetwork, sunburstR and other htmlwidget packages\nProfiling, refactoring, and scaling reactive code for performance\n\nThe app was built to support a national rollout and continues to be used across NHS teams."
  },
  {
    "objectID": "projects.html#scoping-a-large-scale-sas-to-r-migration",
    "href": "projects.html#scoping-a-large-scale-sas-to-r-migration",
    "title": "Past Work",
    "section": "Scoping a Large-Scale SAS to R Migration",
    "text": "Scoping a Large-Scale SAS to R Migration\n\n\nI audited a large SAS codebase to help a client understand the feasibility and risk profile of migrating to R. The codebase was business-critical and had grown organically over many years.\n\nStatic code analysis of SAS codebase to estimate size and complexity\nIdentified areas where statistical methods or parameter defaults might behave differently in R\nDelivered a scoped migration plan to help inform budget, risk, and staffing decisions\n\nThe audit helped the client avoid assumptions about direct conversion and provided a structured way to evaluate next steps."
  },
  {
    "objectID": "projects.html#refactoring-a-legacy-shiny-app",
    "href": "projects.html#refactoring-a-legacy-shiny-app",
    "title": "Past Work",
    "section": "Refactoring a Legacy Shiny App",
    "text": "Refactoring a Legacy Shiny App\n\n\n\n\n\nI led a team within a large organisation responsible for maintaining an internal Shiny app used to manage R package installation requests. The codebase had become increasingly fragile and hard to maintain, with tightly coupled logic and growing technical debt.\n\nRefactored reactive logic to simplify control flow and heavily modularised code\nIntroduced testing strategies and CI workflows\nImproved onboarding for new developers by making the codebase easier to understand\n\nThe changes made the application significantly more maintainable and safer to extend, reducing development time considerably.\n\n\n\nInterested in working together? Get in touch."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "AI Tooling and Open Source\n\n\n\nAI\n\n\n\n\n\n\n\n\n\nJan 27, 2026\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ChatGPT as a Code Tutor: A One-Hour Learning Experiment\n\n\n\nR\n\nAI\n\n\n\n\n\n\n\n\n\nAug 3, 2025\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nIterating on an app to create social media posts generating package using {shiny} and {ellmer}\n\n\n\nR\n\nAI\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating social media posts to promote blog posts with {shiny} and {ellmer}\n\n\n\nR\n\nAI\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nMonitoring R Package Updates with Github Actions\n\n\n\nR\n\nGitHub Actions\n\n\n\n\n\n\n\n\n\nDec 28, 2023\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nDebugging\n\n\n\nR\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nThree Goats in a Raincoat\n\n\n\nR\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nType inference in readr and arrow\n\n\n\nR\n\nArrow\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nArrow New Feature Showcase: show_exec_plan()\n\n\n\nR\n\nArrow\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nCustomising pkgdown with a version selector\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nError chaining\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 9, 2022\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Source Developer Apprenticeship - 9 months in\n\n\n\nR\n\nArrow\n\nCareer\n\n\n\n\n\n\n\n\n\nJan 23, 2022\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nR package documentation - what makes a good example?\n\n\n\nR\n\nPython\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nR package documentation - what makes a good example?\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nMy First Month at Ursa Computing\n\n\n\nR\n\nArrow\n\nCareer\n\n\n\n\n\n\n\n\n\nMay 13, 2021\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is eval parse bad?\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 20, 2021\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nThree Valued Logic\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 17, 2021\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nTen Steps to Becoming a Tidyverse Contributor\n\n\n\nR\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nScraping rstudio::conf 2018 Abstracts\n\n\n\nR\n\n\n\n\n\n\n\n\n\nAug 22, 2018\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nComparing R packages using packagemetrics\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 31, 2018\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nPop-up to confirm action in Shiny\n\n\n\nR\n\nShiny\n\n\n\n\n\n\n\n\n\nAug 16, 2016\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started with Leaflet in Shiny - interactive map apps\n\n\n\nR\n\nShiny\n\n\n\n\n\n\n\n\n\nApr 28, 2016\n\n\nNic Crane\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/socialmediagen/index.html",
    "href": "posts/socialmediagen/index.html",
    "title": "Generating social media posts to promote blog posts with {shiny} and {ellmer}",
    "section": "",
    "text": "Writing blog posts is fun, but the next step, getting people to actually read them, can be trickier. I wanted to promote a recent blog post I co-authored for R-Ladies, but coming up with multiple social media posts felt like a bit of a chore.Instead of doing it it manually, I decided to automate the process by building a Shiny app that generates social media content using large language models (LLMs). It was a fun little project, and could even be useful for others too!"
  },
  {
    "objectID": "posts/socialmediagen/index.html#the-solution-a-shiny-app-powered-by-ai",
    "href": "posts/socialmediagen/index.html#the-solution-a-shiny-app-powered-by-ai",
    "title": "Generating social media posts to promote blog posts with {shiny} and {ellmer}",
    "section": "The Solution: A Shiny App Powered by AI",
    "text": "The Solution: A Shiny App Powered by AI\nThe app does the following:\n\nTakes a GitHub Markdown link as input.\nLets users specify hashtags, the number of posts to generate, and whether or not to use emojis.\nAllows selection of tone (e.g., serious, playful, informative).\nCalls Google’s Gemini API via the ellmer package to generate social media posts promotinhg the blog post.\n\n\nWhy ellmer and the Gemini API?\nThe ellmer package was released last year, and is a delightfully straightforward way to directly call LLMs from R. Google’s Gemini API, in particular, offers a generous free tier - up to 15 requests per minute, which was more than enough for my needs."
  },
  {
    "objectID": "posts/socialmediagen/index.html#how-it-works",
    "href": "posts/socialmediagen/index.html#how-it-works",
    "title": "Generating social media posts to promote blog posts with {shiny} and {ellmer}",
    "section": "How It Works",
    "text": "How It Works\nThe app is simple:\n\nEnter the GitHub Markdown file link (or other text source).\nSet your preferences—hashtags, emojis, tone, and how many posts to generate.\nClick generate, and it spits out a bunch of posts.\nCopy and paste into your social media scheduler.\n\nUnder the hood, it builds a prompt using the user’s inputs and calls the Gemini API. The response is formatted into platform-appropriate posts, making it much easier to schedule across different social networks.\n\n\n\nPreview of the app"
  },
  {
    "objectID": "posts/socialmediagen/index.html#deployment-considerations",
    "href": "posts/socialmediagen/index.html#deployment-considerations",
    "title": "Generating social media posts to promote blog posts with {shiny} and {ellmer}",
    "section": "Deployment Considerations",
    "text": "Deployment Considerations\nRight now, the app runs locally, and users need to enter their own Gemini API key as an evironment variable. This prevents abuse of a shared key, but it also makes the app a bit more DIY. I’m considering deploying it on Posit Connect Cloud or a similar service in the future. The trick will be allowing users to bring their own API keys while keeping things secure.\nAnother thought: supporting multiple AI models so users can plug in API keys for different services like OpenAI’s ChatGPT or Anthropic’s Claude."
  },
  {
    "objectID": "posts/socialmediagen/index.html#future-ideas",
    "href": "posts/socialmediagen/index.html#future-ideas",
    "title": "Generating social media posts to promote blog posts with {shiny} and {ellmer}",
    "section": "Future Ideas",
    "text": "Future Ideas\nSome things I’d love to add:\n\nMore refined prompts tailored to different platforms (e.g., longer for LinkedIn, punchier for Bluesky).\nSuggested images to go with posts."
  },
  {
    "objectID": "posts/socialmediagen/index.html#try-it-yourself",
    "href": "posts/socialmediagen/index.html#try-it-yourself",
    "title": "Generating social media posts to promote blog posts with {shiny} and {ellmer}",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\nWant to give it a go? You can find the repo here: https://github.com/thisisnic/socialmediagen"
  },
  {
    "objectID": "posts/socialmediagen/index.html#wrapping-up",
    "href": "posts/socialmediagen/index.html#wrapping-up",
    "title": "Generating social media posts to promote blog posts with {shiny} and {ellmer}",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThis was a fun little side project that turned out to be actually useful - I’m going to be using it to promote this blog post!\nI’d love to hear if you have any ideas for improvements—hit me up if you try it out!"
  },
  {
    "objectID": "posts/ursa-computing-first-month/index.html",
    "href": "posts/ursa-computing-first-month/index.html",
    "title": "My First Month at Ursa Computing",
    "section": "",
    "text": "I’ve now been working at Ursa Computing on Apache Arrow for just over a month, and thought it would be a good time to write a blog post about my experiences over the past month.\nSo when I found out I got the apprenticeship, I will admit, I shrieked with joy. It was a similar noise to the one I made when I found out that my talk had been accepted for rstudio::conf 2019. At the time I’d been interviewing for a few different things, but this was the only one where I had to cut things down in the cover letter, rather than bulk it up. My cover letter was quite the ramble about my passion for open source community, and encouraging people to get involved in open source. The interviews and the small task I had to do were the least arduous of all the interviews and assignments for possible jobs I had to do at the time. It just felt right.\nI’ve had a really great first month. It’s been really bloody challenging - but in a good way. I’ve had 12 pull requests merged so far, and that’s been all sorts of things, from adding a new job to the CI, to moving some C++ code from the R layer to the C++ layer and writing the relevant C++ unit tests, to writing R bindings that allow users to call common R functions like any() and all(), but actually harness the power of the underlying C++ code for faster execution on large datasets.\nI’ve learned a lot. Before this, I hadn’t done any C++, and found it pretty daunting. I still do, but tackling the tickets where I had to work with the C++ code taught me a lot about the power of using analogy and looked at similar bits of code and previous pull requests which solve similar problems. Many times I’ve seen, in advice for people wanting to get involved in open source, the suggestion that they should read previous pull requests, but I’d never really understood the benefits until now. I’ve enjoyed getting to learn independently, but also more structured opportunities to learn - there is a fortnightly meeting which about different topics in open source maintenance, and this has been fascinating so far. The most interesting so far was a session on The Apache Way.\nI’m also learning a lot about the difference between being a developer in the open source world compared to the commercial world. As much as in industry, plenty of people do strive to achieve great code quality, ultimately, if the code you write gets the job done and is sufficiently well-tested, then that’s good enough a lot of the time. However, often, when writing code for a specific application or use-case, you have a well-defined idea of how people are going to use your code, and might literally be able to see every time a function you write is being called, and so there is less risk of things going wrong from edge cases. Things are a lot different when you’re building frameworks that other people are going to be able to use. You need to think a lot more about edge cases as people could be doing all sorts of things with your code, and investing extra time in devising an elegant solution that reduces complexity is time well spent. My perfectionist tendencies absolutely adore this, though the part of me that likes to “MacGyver” something impressive together with string and duct tape is sitting crying in a corner! It’s not a bad thing though - I feel like my R skills have improved a lot over the past month, thanks to the feedback I’ve received.\nAnother difference I’ve observed is in communication patterns. I’m working on a globally distributed project, and so a lot of communication is asynchronous. In addition to this, one of the key tenets of The Apache Way - a set of principles that govern how Apache Software Foundation projects are run - is open communication. This means that much of the communication about the project must be done on project mailing lists, and other public but “unofficial” channels like Zulip are used for real time chat. This can be a bit daunting at first, but I’m getting my head around the idea. There’s still the internal Ursa Slack, which I tend to use for a lot of questions I have. I really do enjoy working with a distributed team though; I like the fact that communication is expected to be somewhat asynchronous, so whilst some conversations may happen in one go, it’s perfectly fine to start a conversation on a topic and people come back to it later depending on their individual time zones and what they’re up to. I find myself spending less time concerned about my Slack notifications and being more able to focus when I need to.\nAt this point, I do want to drop my tendency to be overwhelmingly positive, to instead be genuine. There are some things I’m finding it harder to adapt to. When I was working as a consultant or a data scientist, I always would push to be involved in the planning side of the work I was doing, really dig into what the big picture was, and aim to be working with as specific requirements and expectations as possible. This approach makes a lot of sense when you’re doing client work and have deadlines, but this is not the world which I am inhabiting now. Whilst there are certain items targeted for the next release, work is generally self-assigned and people work on what they’re interested in or what they think needs doing. This approach absolutely works - stuff gets done, and gets done well - but as someone who quells their anxiety by evaluating how things are by comparing the current state of things to the grand plan, it’s a huge mindset shift. I have a tonne of things I want to get better at - CI, R, Python, C++, documentation - and occasionally I find myself feeling a bit overwhelmed trying to do a bit of everything as I don’t have a huge amount of specific focus, and so feeling like I’m not moving much. Ultimately though, it’s up to me to talk to people, think about what I think is worth doing and making my own plan/goals if that structure is what I need. I’m optimistic about solving this though and I anticipate that I will actually really enjoy this extra freedom once I’ve adjusted to it.\nOne really great thing is the people. Things got off to a good start before I’d even started, with interviews taking a conversational tone, and not feeling like the interviewers were trying to interrogate me or catch me out. When I did start, I was paired up with someone with whom I had daily catchups with in the first couple of weeks, and now twice-weekly catchups. I think this was really important as a new person starting out somewhere that is entirely remote - there are a lot of incidental conversations that don’t happen and questions one doesn’t just ask in passing, and so having dedicated time to discuss things is helpful. As well of this, people seem to be really good at coaching/mentoring. When I ask questions about how/why things worked the way they do, people were very clear in encouraging me to ask these kinds of things. When I include code in a pull request that, perhaps unkindly, I might later label “f-ing stupid”, they’d gently ask me questions like “what would happen if I passed X into this function?” so I’d come to the realisation myself. Similarly, when I asked questions to which the answers are in the vignettes, instead of just pointing me there, to explain what I did understand so far, and started a conversation. It’s, honestly, an absolute delight to be working with people who are really great at a lot of things I’m still a relative newbie to, and just being able to enjoy it rather than feeling constant “imposter syndrome”. As an aside, I’m being a lot more careful about using that term now, as it ignores the impact of the context and environment, and frames things in terms of neuroses when there are things that can be done about it.\nSo, what next then? I have a lot to learn and a lot of skills to level up, so I’m going to set myself some goals so I have a bit more focus, as mentioned earlier. I also want to be more communicative without adding meaningless noise. For example, getting involved in code reviews a little more; yesterday someone had done a huge amount of refactoring, and I realised that even though the author was a lot more experienced as a developer than me, it was still worth expressing an opinion on whether their changes made the codebase easier to work with, even if I was just confirming that it was. I’ve also started a slightly bigger project on documentation. I think there’s a fine line between the fact that sometimes people really do need to just RTFM, but on the other hand, many people trying to use a new codebase are busy and so skimming the docs is as much effort as it makes sense for them to invest. I want documentation that works for the people in the latter category, as that’s often been me in the past. I want all my man pages to have examples I can run. As a developer, I want complicated internal functions to be documented so I can read the code more easily. I have quite strong feelings about this, and am really excited to dig more into this topic.\nThis month has been fun but challenging - which, to be honest, is my favourite combination of things. I’m working on a really exciting project with some fantastic people, and I’m really glad to be here."
  },
  {
    "objectID": "posts/three-valued-logic/index.html",
    "href": "posts/three-valued-logic/index.html",
    "title": "Three Valued Logic",
    "section": "",
    "text": "This week I was writing Arrow bindings for some R functions and came across a concept that was new to me: three-valued logic.\nI encountered this when working out the expected behaviour of the functions any and all which are used for aggregation of boolean/logical values.\nIf I have a vector without any NA values, the behaviour of these functions is pretty straightforward. In the example below, vec &gt; 3 evaluates to FALSE FALSE FALSE  TRUE  TRUE and so a call to any() results in TRUE and a call to all() results in FALSE.\nvec &lt;- 1:5\nany(vec &gt; 3) # are any values greater than 5?\n## [1] TRUE\nall(vec &gt; 3) # are all values greater than 5?\n## [1] FALSE\nThings get a little more complicated when we have a vector containing NA values. My vector, na_vec, has had an NA appended to the end and I’ve changed the expression to na_vec &gt; 5. In this case of all(), as there are definitely some values which are less than 5, we can say definitively that the answer is FALSE. However, in the case of any(), things are less clear. As NA represents a missing value, it might be the case that this missing value is greater than 5, and so even though none of the known values are greater than 5, we cannot be sure that the missing one is not. Therefore, we get NA back.\nna_vec &lt;- c(1:5, NA)\nany(na_vec &gt; 5) # are any values greater than 5?\n## [1] NA\nall(na_vec &gt; 5) # are all values greater than 5?\n## [1] FALSE\nIf needed in principle, we can set the argument na.rm = TRUE for both of these functions, stripping out the NA values and continuing with the evaluation as if they were not there.\nI don’t think this is overly complex when using these functions, though it makes things more interesting when trying to implement them. There a Wikipedia article on the topic, and it also has other names like “Kleene logic”.\nI drew this out as a simple flowchart to make sure I had it right, and this is the logic inside it:\n\nIn the case of any:\n\nCan you say that the condition has been met? If yes, return TRUE. If no, go to next step.\nAre there are missing values? If no, return FALSE. If yes, return NA.\n\n\n\nAnd in the case of all:\n\nCan you say that the condition has been met? If no, return FALSE. If yes from the non-missing values (so more like “maybe”!), go to next step.\nAre there are missing values? If no, return TRUE If yes, return NA.\n\n\nNow I write it out, it doesn’t seem overly complex, but it was pretty interesting encountering it for the first time. This concept also exists in SQL and I’m surprised I’ve not come across it before."
  },
  {
    "objectID": "posts/socalmediagen2/index.html",
    "href": "posts/socalmediagen2/index.html",
    "title": "Iterating on an app to create social media posts generating package using {shiny} and {ellmer}",
    "section": "",
    "text": "I recently posted about a repo I created containing a Shiny app which allows users to generate social media posts to promote blog posts, using {shiny} and {ellmer}.\nIf you haven’t used it before, {ellmer} is an R package which allows you to call LLMs directly from R.\nI’ll admit right now that, although I’d already been using the LLM prompt used in that project for a while, I threw together the app and associated code in a couple of hours to give me something to write about. People seemed receptive to the blog post, but I wasn’t happy with the code, and so I decided to tidy it up a bit.\nIn this post, I’m going to discuss the changes I made to get from a few messy scripts to a deployable R package that is much easier to maintain."
  },
  {
    "objectID": "posts/socalmediagen2/index.html#step-1-convert-the-scripts-into-an-r-package",
    "href": "posts/socalmediagen2/index.html#step-1-convert-the-scripts-into-an-r-package",
    "title": "Iterating on an app to create social media posts generating package using {shiny} and {ellmer}",
    "section": "Step 1: Convert the scripts into an R package",
    "text": "Step 1: Convert the scripts into an R package\nI’m a big fan of writing code as R packages wherever possible; it provides instant structure and reminds me to do important fundamentals such as documenting functions, and writing modular pieces instead of long scripts.\nI moved the app code into ./R/app.R and separated out the other functions into a file ./R/prompts.R.\nI also moved the CSS for the app into the package’s inst dir and created a subdirectory in inst to store my prompts.\n\n\n\nContents of the inst dir"
  },
  {
    "objectID": "posts/socalmediagen2/index.html#step-2-saving-the-prompts-in-their-own-file",
    "href": "posts/socalmediagen2/index.html#step-2-saving-the-prompts-in-their-own-file",
    "title": "Iterating on an app to create social media posts generating package using {shiny} and {ellmer}",
    "section": "Step 2: Saving the prompts in their own file",
    "text": "Step 2: Saving the prompts in their own file\nNext, was where to save the prompts. Originally these were saved as variables, but in the ellmer vignette on prompt design, it’s recommended that these are saved in ./inst/prompts/ with one file per prompt.\nThe ellmer vignette recommends saving prompts as markdown files as they’re both human-readable and LLM-readable, and so I saved my main prompt as shown below:\nCreate me social media posts for each of these platforms: {{platforms}}, to promote the blog post below.\n\n* create {{n}} posts per platform\n* use a {{tone}} tone\n* use hashtags: {{hashtags}}\n* use emojis? {{emojis}}\n\n# Blog post contents\n\n{{post_contents}}\nYou’ll notice the use of placeholders in the prompt. This is because {ellmer} has a helpful function which can read the prompt and inject in variable values. So the last line of my get_prompt() function looks like this:\nellmer::interpolate_file(\n    system.file(\"prompts\", \"prompt-main.md\", package = \"socialmediagen\"),\n    platforms = paste(platforms, collapse = \", \"),\n    n = n,\n    tone = tone,\n    hashtags = hashtags,\n    emojis = emojis,\n    post_contents = post_contents\n)"
  },
  {
    "objectID": "posts/socalmediagen2/index.html#step-3-creating-platform-specific-prompts",
    "href": "posts/socalmediagen2/index.html#step-3-creating-platform-specific-prompts",
    "title": "Iterating on an app to create social media posts generating package using {shiny} and {ellmer}",
    "section": "Step 3: Creating platform-specific prompts",
    "text": "Step 3: Creating platform-specific prompts\nI realised that best-practices for social media posts vary from platform to platform, and I wanted to experiment with having different prompts depending on which platforms were selected by the user.\nI first added the following to my main prompt:\n# Platform-specific advice\n\nUse the following advice to customise the output for individual platforms:\n\n{{platform_specific_advice}}\nI then created additional prompts tailored to each platform, for example, my LinkedIn prompt looks like this:\nLinkedIn:\n* Keep posts between 1,300 and 2,000 characters. \n* Use short sentences: Posts with sentences under 12 words perform better. \n* ask questions: Encourage comments by asking questions that prompt discussion. \n* give specific instructions: Ask readers to like your post or take another action. \n* use a compelling headline: Grab attention with your first line. \n* use 3 hashtags\nI then saved all of these into the inst/prompts directory.\n\n\n\nContents of the prompt dir\n\n\nI then created a super-simple function to retrieve the relevant prompt:\n#' Retrieve post-writing advice unique to specific platforms\n#'\n#' @param platforms Which platforms to get advice for\nget_platform_specific_advice &lt;- function(platforms){\n  prompt_files &lt;- paste0(\"prompt-\", tolower(platforms), \".md\")\n  file_paths &lt;- system.file(\"prompts\", prompt_files, package = \"socialmediagen\")\n\n  contents &lt;- lapply(file_paths, readLines)\n  paste(unlist(contents), collapse = \"\\n\")\n}\nFinally, I updated my get_prompt() function to incorporate these changes:\n#' Construct LLM prompt\n#'\n#' Construct a LLM prompt based on user input\n#'\n#' @param blog_link URL of source material\n#' @param platforms Social media platform to create prompts for\n#' @param n Number of prompts to create for each platform\n#' @param emojis Use emojis in post?\n#' @param tone Desired tone of the post\n#' @param hashtags Hashtags to include in the post\n#' @importFrom ellmer interpolate_file\nget_prompt &lt;- function(blog_link, platforms, n, emojis, tone, hashtags) {\n  # retrieve post contents from GitHub\n  post_contents &lt;- fetch_github_markdown(blog_link)\n\n  platform_specific_advice &lt;- get_platform_specific_advice(platforms)\n\n  # combine components\n  ellmer::interpolate_file(\n    system.file(\"prompts\", \"prompt-main.md\", package = \"socialmediagen\"),\n    platforms = paste(platforms, collapse = \", \"),\n    n = n,\n    tone = tone,\n    hashtags = hashtags,\n    emojis = emojis,\n    post_contents = post_contents,\n    platform_specific_advice = platform_specific_advice\n  )\n}"
  },
  {
    "objectID": "posts/socalmediagen2/index.html#the-resulting-app",
    "href": "posts/socalmediagen2/index.html#the-resulting-app",
    "title": "Iterating on an app to create social media posts generating package using {shiny} and {ellmer}",
    "section": "The resulting app",
    "text": "The resulting app\nMy app now is able to create platform-specific content and is much better organised than it was before. I think that updating the prompt was useful - check out the example below of the content generated before and after. The LinkedIn post now it has a strong call-to-action and asks the user a question.\n\nBefore\n\n📢 Calling all data enthusiasts! 📢\n\nThinking about speaking at Posit Conf 2025 but feeling a bit 😬 about the video abstract? Don't sweat it! This year the abstract just needs to be a short video!\n\nWe've compiled some top tips from past speakers to help you nail that one-minute pitch! 🚀\n\n➡️ Learn how to structure your video, what tech to use, and how to inject some of YOUR personality into it! (Humor and creativity are welcome!)\n\nPlus, we're offering feedback on drafts! 📝\n\n[Link to Blog Post]\n\n#rstats\n\n\nAfter\n\n**Headline:** Nervous About Your Posit Conf Video Abstract? 😩 Don't Be!\n\nThinking of speaking at #PositConf2025 but dreading the video abstract? You're not alone! It's just one minute to shine ✨.\n\nWe've got tips from past speakers Rachael Dempsey, Shannon Pileggi, and Nic Crane to help you nail it! From tech options (easy peasy phone cam to fancy OBS Studio) to structuring your pitch, we've got you covered.\n\n**Blog Post Highlights:**\n\n*   Simple tech options\n*   Easy structure for your video\n*   How to get feedback\n\nReady to record? Check out our tips and let your brilliance shine. 💡\n[Link to blog post]\n\nWhat's your biggest video recording fear? Share in the comments! 👇\n\n#rstats #rladies #positconf"
  },
  {
    "objectID": "posts/socalmediagen2/index.html#reflecting-on-llm-based-apps",
    "href": "posts/socalmediagen2/index.html#reflecting-on-llm-based-apps",
    "title": "Iterating on an app to create social media posts generating package using {shiny} and {ellmer}",
    "section": "Reflecting on LLM-based apps",
    "text": "Reflecting on LLM-based apps\nI enjoyed building this example, and on some levels, it doesn’t do anything particularly radical. The same results could be achieved by using the same prompts in a browser session and manually filling in the parameters. However, what I do like about this is that I have a specific place to store my prompts - in a GitHub repo - where I can iterate on them and track changes over time.\nI also like that I now have a deployable artifact that I can share with others - I had mentioned to other members of the R-Ladies Global Team that I had been using LLMs to generate social media posts for promoting our news and blog posts, and this means I can easily share the link to the repo with the app, instead of having to share a prompt. Creating this as a distinct project encourages collaboration, whether that’s on which parameters we want to include in the app, or improving the quality of the prompts."
  },
  {
    "objectID": "posts/type-inference/index.html",
    "href": "posts/type-inference/index.html",
    "title": "Type inference in readr and arrow",
    "section": "",
    "text": "The CSV format is widely used in data science, and at its best works well as a simple human-readable format that is widely known and understood. The simplicity of CSVs though, as a basic text format also has its drawbacks. One is that it contains no information about data types of its columns, and if you’re working with CSVs in an application more complex than a text editor, those data types must be inferred by whatever is reading the data.\nIn this blog post, I’m going to discuss how CSV type inference works in the R packages readr and arrow, and highlight the differences between the two.\nBefore I get started though, I’d like to acknowledge that this post is an exercise in examining the underlying mechanics of the two packages. In practice, I’ve found that when working with datasets small enough to fit in-memory, it’s much more fruitful to either read in the data first and then manipulate it into the required shape, or just specify the column types up front. Still, the strategies for automatic guessing are interesting to explore."
  },
  {
    "objectID": "posts/type-inference/index.html#how-does-type-inference-work-in-readr",
    "href": "posts/type-inference/index.html#how-does-type-inference-work-in-readr",
    "title": "Type inference in readr and arrow",
    "section": "How does type inference work in readr?",
    "text": "How does type inference work in readr?\nSince readr version 2.0.0 (released in July 2020), there was a significant overhaul of the underlying code, which subsequently depended on the vroom package.\nThe type inference is done by a C++ function in vroom called guess_type__() which guesses types in the following order:\n\nDoes the column contain 0 rows? If yes, return “character”\nAre all values missing? If yes, return “logical”\nTries to parse column to each of these formats and returns the first one it successfully parses:\n\nLogical\nInteger (though the default is to not look for these)\nDouble\nNumber (a special type which can remove characters from strings representing numbers and then convert them to doubles)\nTime\nDate\nDatetime\nCharacter\n\n\nThe ordering above in the parsing bullet point goes from most to least strict in terms of the conditions which have to be met to successfully parse an input as that data type. For example, for a column to be of logical type, it can only contain a small subset of values representing true (T, t, True, TRUE, true), false (F, f, False, FALSE, false) or NA, which is why this is the most strict type, but all of the other types could be read in as character data, which is the least strict and why this is last in the order."
  },
  {
    "objectID": "posts/type-inference/index.html#how-does-type-inference-work-in-arrow",
    "href": "posts/type-inference/index.html#how-does-type-inference-work-in-arrow",
    "title": "Type inference in readr and arrow",
    "section": "How does type inference work in arrow?",
    "text": "How does type inference work in arrow?\nIn arrow, read_csv_arrow() handles CSV reading, and much of its interface has been designed to closely follow the excellent APIs of readr::read_csv() and vroom::vroom(). The intention here is that users can use parameter names they’re familiar with from the aforementioned readers when using arrow, and get the same results. The underlying code is pretty different though.\nIn addition, Arrow has a different set of possible data types compared to R; see the Arrow docs for more information about the mapping between R data types and Arrow types.\nIn the Arrow docs, we can see that types are inferred in this order:\n\nNull\nInt64\nBoolean\nDate32\nTimestamp (with seconds unit)\nFloat64\nDictionary&lt;String&gt; (if ConvertOptions::auto_dict_encode is true)\nDictionary&lt;Binary&gt; (if ConvertOptions::auto_dict_encode is true)\nString\nBinary\n\nNote that if you use arrow::read_csv_arrow() with parameter as_data_frame = TRUE (the default), the Arrow data types are then converted to R data types.\nsimple_data &lt;- data.frame(x = c(1, 2, 3), y = c(\"a\", \"b\", \"c\"), z = c(1.1, 2.2, 3.3))\n\nreadr::write_csv(simple_data, \"simple_data.csv\")\n\n# columns are arrow's int64, string, and double (aka float64) types\narrow::read_csv_arrow(\"simple_data.csv\", as_data_frame = FALSE)\n## Table\n## 3 rows x 3 columns\n## $x &lt;int64&gt;\n## $y &lt;string&gt;\n## $z &lt;double&gt;\n# columns converted to R's integer, character, and double types\narrow::read_csv_arrow(\"simple_data.csv\", as_data_frame = TRUE)\n## # A tibble: 3 x 3\n##       x y         z\n##   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n## 1     1 a       1.1\n## 2     2 b       2.2\n## 3     3 c       3.3"
  },
  {
    "objectID": "posts/type-inference/index.html#what-are-the-main-differences-between-readr-and-arrow-type-inference",
    "href": "posts/type-inference/index.html#what-are-the-main-differences-between-readr-and-arrow-type-inference",
    "title": "Type inference in readr and arrow",
    "section": "What are the main differences between readr and arrow type inference?",
    "text": "What are the main differences between readr and arrow type inference?\nAlthough there appear to be quite a few differences between the order of type inference when comparing arrow and readr, in practice, this doesn’t have much effect. Type inference for logical/boolean and integer values are the opposite way round, but given that the underlying data that translates into these types looks very different, they’re not going to be mixed up. The biggest differences come from custom behaviours which are specific to readr and arrow; I’ve outlined them below.\n\nGuessing integers\nIn the code for readr, the default setting is for numeric values to always be read in as doubles but never integers. If you want readr to guess that a column may be an integer, you need to read it in as character data, and then call type_convert(). This isn’t necessarily a great workflow though, and in most cases it would make sense to just manually specify the column type instead of having it inferred.\nIn arrow, if data can be represented as integers but not doubles, then it will be.\nint_or_dbl &lt;- data.frame(\n  x = c(1L, 2L, 3L)\n)\n\nreadr::write_csv(int_or_dbl, \"int_or_dbl.csv\")\n\nreadLines(\"int_or_dbl.csv\")\n## [1] \"x\" \"1\" \"2\" \"3\"\n# double\nreadr::read_csv(\"int_or_dbl.csv\")\n## Rows: 3 Columns: 1\n## -- Column specification --------------------------------------------------------\n## Delimiter: \",\"\n## dbl (1): x\n##\n## i Use `spec()` to retrieve the full column specification for this data.\n## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\n## # A tibble: 3 x 1\n##       x\n##   &lt;dbl&gt;\n## 1     1\n## 2     2\n## 3     3\n# integer via inference\nreadr::read_csv(\"int_or_dbl.csv\", col_types = list(.default = col_character())) %&gt;%\n  type_convert(guess_integer = TRUE)\n##\n## -- Column specification --------------------------------------------------------\n## cols(\n##   x = col_integer()\n## )\n## # A tibble: 3 x 1\n##       x\n##   &lt;int&gt;\n## 1     1\n## 2     2\n## 3     3\n# integer via specification\nreadr::read_csv(\"int_or_dbl.csv\", col_types = list(col_integer()))\n## # A tibble: 3 x 1\n##       x\n##   &lt;int&gt;\n## 1     1\n## 2     2\n## 3     3\n# integer via inference\narrow::read_csv_arrow(\"int_or_dbl.csv\")\n## # A tibble: 3 x 1\n##       x\n##   &lt;int&gt;\n## 1     1\n## 2     2\n## 3     3\n\n\n32-bit integers\nAnother difference between readr and arrow is the difference between how integers larger than 32 bits are read in. Natively, R can only support 32-bit integers, though it can support 64-bit integers via the bit64 package. If we create a CSV with one column containing the largest integer that R can natively support, and then another column containing that value plus 1, we get different behaviour when we import this data with readr and arrow. In readr, when we enable integer guessing, the smaller value is read in as an integer, and the larger value is read in as a double. However, once we move over to manually specifying column types, we can use vroom::col_big_integer() to use bit64 and get us a large integer column. The arrow package also uses bit64, and its integer guessing results in 64-bit integer via inference.\nsixty_four &lt;- data.frame(x = 2^31 - 1, y = 2^31)\n\nreadr::write_csv(sixty_four, \"sixty_four.csv\")\n\n# doubles by default\nreadr::read_csv(\"sixty_four.csv\")\n## Rows: 1 Columns: 2\n## -- Column specification --------------------------------------------------------\n## Delimiter: \",\"\n## dbl (2): x, y\n##\n## i Use `spec()` to retrieve the full column specification for this data.\n## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\n## # A tibble: 1 x 2\n##            x          y\n##        &lt;dbl&gt;      &lt;dbl&gt;\n## 1 2147483647 2147483648\n# 32 bit integer or double depending on value size\nreadr::read_csv(\"sixty_four.csv\", col_types = list(.default = col_character())) %&gt;%\n  type_convert(guess_integer = TRUE)\n##\n## -- Column specification --------------------------------------------------------\n## cols(\n##   x = col_integer(),\n##   y = col_double()\n## )\n## # A tibble: 1 x 2\n##            x          y\n##        &lt;int&gt;      &lt;dbl&gt;\n## 1 2147483647 2147483648\n# integers by specification\nreadr::read_csv(\n  \"sixty_four.csv\",\n  col_types = list(x = col_integer(), y = vroom::col_big_integer())\n)\n## # A tibble: 1 x 2\n##            x          y\n##        &lt;int&gt;    &lt;int64&gt;\n## 1 2147483647 2147483648\n# integers by inference\narrow::read_csv_arrow(\"sixty_four.csv\")\n## # A tibble: 1 x 2\n##            x          y\n##        &lt;int&gt;    &lt;int64&gt;\n## 1 2147483647 2147483648\n\n\nThe “number” parsing strategy\nOne really cool feature in readr is the “number” parsing strategy. This allows values which have been stored as character data with commas to separate the thousands to be read in as doubles. This is not supported in arrow.\nnumber_type &lt;- data.frame(\n  x = c(\"1,000\", \"1,250\")\n)\n\nreadr::write_csv(number_type, \"number_type.csv\")\n\n# double type, but parsed in as number in column spec shown below\nreadr::read_csv(\"number_type.csv\")\n## Rows: 2 Columns: 1\n## -- Column specification --------------------------------------------------------\n## Delimiter: \",\"\n## num (1): x\n##\n## i Use `spec()` to retrieve the full column specification for this data.\n## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\n## # A tibble: 2 x 1\n##       x\n##   &lt;dbl&gt;\n## 1  1000\n## 2  1250\n# read in as character data in Arrow\narrow::read_csv_arrow(\"number_type.csv\")\n## # A tibble: 2 x 1\n##   x\n##   &lt;chr&gt;\n## 1 1,000\n## 2 1,250\n\n\nDictionaries/Factors\nAnyone who’s been around long enough might remember that R’s native CSV reading function read.csv() had a default setting of importing character columns as factors (I definitely have read.csv(..., stringAsFactors=FALSE) carved into a groove in some dark corner of my memory). This default was changed in version 4.0.0, released in April 2020, reflecting the fact that in most cases users want their string data to be imported as characters unless otherwise specified. Still, some datasets contain character data which users do want to import as factors. In readr, this can be controlled by manually specifying the column as a factor\nIn arrow, if you don’t want to individually specify column types, you can set up an option to import character columns as dictionaries (the Arrow equivalent of factors), which are converted into factors.\ndict_type &lt;- data.frame(\n  x = c(\"yes\", \"no\", \"yes\", \"no\")\n)\n\nreadr::write_csv(dict_type, \"dict_type.csv\")\n\n# character data\nreadr::read_csv(\"dict_type.csv\")\n## Rows: 4 Columns: 1\n## -- Column specification --------------------------------------------------------\n## Delimiter: \",\"\n## chr (1): x\n##\n## i Use `spec()` to retrieve the full column specification for this data.\n## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\n## # A tibble: 4 x 1\n##   x\n##   &lt;chr&gt;\n## 1 yes\n## 2 no\n## 3 yes\n## 4 no\n# factor data\nreadr::read_csv(\"dict_type.csv\", col_types = list(x = col_factor()))\n## # A tibble: 4 x 1\n##   x\n##   &lt;fct&gt;\n## 1 yes\n## 2 no\n## 3 yes\n## 4 no\n# set up the option. there's an open ticket to make this code a bit nicer to read.\nauto_dict_option &lt;- arrow::CsvConvertOptions$create(auto_dict_encode = TRUE)\narrow::read_csv_arrow(\"dict_type.csv\", convert_options = auto_dict_option)\n## # A tibble: 4 x 1\n##   x\n##   &lt;fct&gt;\n## 1 yes\n## 2 no\n## 3 yes\n## 4 no\n\n\nCustom logical/boolean values\nAnother slightly niche but potentially useful piece of functionality available in arrow is the ability to customise which values can be parsed as logical/boolean type and how they translate to TRUE/FALSE. This can be achieved by setting some custom conversion options.\nalternative_true_false &lt;- arrow::CsvConvertOptions$create(\n  false_values = \"no\", true_values = \"yes\"\n)\narrow::read_csv_arrow(\"dict_type.csv\", convert_options = alternative_true_false)\n## # A tibble: 4 x 1\n##   x\n##   &lt;lgl&gt;\n## 1 TRUE\n## 2 FALSE\n## 3 TRUE\n## 4 FALSE"
  },
  {
    "objectID": "posts/type-inference/index.html#using-schemas-for-manual-control-of-data-types",
    "href": "posts/type-inference/index.html#using-schemas-for-manual-control-of-data-types",
    "title": "Type inference in readr and arrow",
    "section": "Using schemas for manual control of data types",
    "text": "Using schemas for manual control of data types\nAlthough relying on the reader itself to guess your column types can work well, what if you want more precise control?\nIn readr, you can use the col_types parameter to specify column types. You can use the same parameter in arrow to use R type specifications.\ngiven_types &lt;- data.frame(x = c(1, 2, 3), y = c(4, 5, 6))\n\nreadr::write_csv(given_types, \"given_types.csv\")\n\nreadr::read_csv(\"given_types.csv\", col_types = list(col_integer(), col_double()))\n## # A tibble: 3 x 2\n##       x     y\n##   &lt;int&gt; &lt;dbl&gt;\n## 1     1     4\n## 2     2     5\n## 3     3     6\nYou can also use this shortcode specification. Here, “i” means integer and “d” means double.\nreadr::read_csv(\"given_types.csv\", col_types = \"id\")\n## # A tibble: 3 x 2\n##       x     y\n##   &lt;int&gt; &lt;dbl&gt;\n## 1     1     4\n## 2     2     5\n## 3     3     6\nIn arrow you can use the shortcodes (though not the col_*() functions), but you must specify the column names.\nWe skip the first row as our data has a header row - this is the same behaviour as when we use both names and types in readr::read_csv() which then assumes that the header row is data if we don’t skip it.\narrow::read_csv_arrow(\"given_types.csv\", col_names = c(\"x\", \"y\"), col_types = \"id\", skip = 1)\n## # A tibble: 3 x 2\n##       x     y\n##   &lt;int&gt; &lt;dbl&gt;\n## 1     1     4\n## 2     2     5\n## 3     3     6\nWhat if you want to use Arrow types instead of R types though? In this case, you need to use a schema. I won’t go into detail here, but in short, schemas are lists of fields, each of which contain a field name and a data type. You can specify a schema like this:\n# this gives the same result as before - because our Arrow data has been converted to the relevant R type\narrow::read_csv_arrow(\"given_types.csv\", schema = schema(x = int8(), y = float32()), skip = 1)\n## # A tibble: 3 x 2\n##       x     y\n##   &lt;int&gt; &lt;dbl&gt;\n## 1     1     4\n## 2     2     5\n## 3     3     6\n# BUT, if you don't read it in as a data frame you'll see the Arrow type\narrow::read_csv_arrow(\"given_types.csv\", schema = schema(x = int8(), y = float32()), skip = 1, as_data_frame = FALSE)\n## Table\n## 3 rows x 2 columns\n## $x &lt;int8&gt;\n## $y &lt;float&gt;"
  },
  {
    "objectID": "posts/type-inference/index.html#parquet",
    "href": "posts/type-inference/index.html#parquet",
    "title": "Type inference in readr and arrow",
    "section": "Parquet",
    "text": "Parquet\nAn alternative approach is to use Parquet format, which stores the data types along with the data. This means that if you’re sharing your data with others, you don’t need to worry about it being read in as the wrong data types. In a follow-up post I’ll explore the Parquet format and compare management of data types in CSVs and Parquet."
  },
  {
    "objectID": "posts/type-inference/index.html#further-reading",
    "href": "posts/type-inference/index.html#further-reading",
    "title": "Type inference in readr and arrow",
    "section": "Further Reading",
    "text": "Further Reading\nIf you want a much more detailed discussion of Arrow data types, see this excellent blog post by Danielle Navarro."
  },
  {
    "objectID": "posts/type-inference/index.html#thanks",
    "href": "posts/type-inference/index.html#thanks",
    "title": "Type inference in readr and arrow",
    "section": "Thanks",
    "text": "Thanks\nHuge thanks to everyone who helped review and tweak this blog post, and special thanks to Jenny Bryan who gave some really helpful feedback on the content on readr/vroom!"
  },
  {
    "objectID": "posts/debugging/index.html",
    "href": "posts/debugging/index.html",
    "title": "Debugging",
    "section": "",
    "text": "As a package maintainer, I’m a little disappointed when folks mention Arrow bugs they’re aware of but haven’t reported. Not disappointed with the individual in question, but disappointed with the fact that we’re not at the point where we’ve created an environment in which folks are happy to just report bugs immediately. This is not an Arrow-specific problem, and I find myself behaving in exactly the same way with other open source projects. If I’m not entirely sure something is a bug, I’m not going to risk looking foolish publicly by complaining, but the irony of this is that I don’t judge users who make those mistakes with Arrow, as usually it means we need to improve our docs to be more clear, and that in itself is valuable feedback.\nI really love interacting with people who use the package, as without that interaction, development and maintenance can feel like shouting into the void. I like being able to solve problems that make other people’s lives easier, and I thrive off that social energy. I implemented bindings for dplyr::across() because someone commented on Twitter that they’d love to see it as a feature. Last night I got home from a friend’s birthday drinks, saw a user question on Stack Overflow which had an easy win, and within an hour had a pull request up which implemented a new function and fixed the particular issue. I am not promising this level of responsiveness in perpetuity, but I’m still at the point where I find this kind of thing exciting and energising.\nOne particular bug which has haunted me for the past 6 months, is a particularly irritating one whereby when working with a partitioned CSV dataset in arrow (I’m using lowercase to denote the R package, rather than Arrow the overall project), if the user manually specified a schema, the variable used for partitioning did not appear in the resulting dataset. This is a huge problem IMO - while we can sit here all day talking about the virtues of Parquet, in reality, a lot of our users will be using CSVs, and it’s issues like these that can rule out being able to use arrow at all in some cases.\nWhen I opened the original issue based on another user issue, I knew it was important, but felt a bit stuck. It wasn’t immediately obvious to anyone what the source of the error was. I’d assumed it must be a C++ error and flagged it as such, but nobody had taken a look at it, and I’m always hesitant to mindlessly tag folks on issues when I don’t feel like I’ve done the work to investigate (though to be fair, didn’t really know what “the work” should be in this case).\nI’d ended up assuming that this bit of functionality just didn’t work with CSV datasets, and had been working around it, until I was presenting about arrow at New York Open Statistical Programming Meetup, and someone asked about it again. I take 1 user question as representative of 99 other people with the same issue who aren’t being so vocal about it, and felt like it needed to be fixed. I am unashamed to admit that I occasionally have the taste for a bit of melodrama, and publicly declared it to a few of my fellow contributors as “my white whale”, and so set out to find the source, even if it required me to delve deep into the guts of Arrow’s C++ libraries, a task which can often send me down endless rabbit holes and chasing red herrings (this sentence has become quite the nature park…)\nMy original exploration didn’t result in much useful - the arrow package does some cool things with R6 objects to binding them to Arrow C++ objects, but accessing the inner properties of these bound objects would mean manually creating print methods for every single one of them, and when you don’t know in which class the problem lies, this becomes, frankly, a massive pain in the arse. I still didn’t have enough to go on to take it to an Arrow C++ contributor and ask for their help, but showing I’d done some of “the work” to at least make an effort myself.\nAnd then collaborative debugging saved me! I had a catchup with the fantastic Dane Pitkin, and I asked for his help just walking through the problem. Dane’s main contributions to Arrow have been to Python, though he has a ton of previous C++ experience, even if he isn’t a regular contributor to the Arrow C++ library. I walked through the problem with him, and the steps I’d taken so far to try to figure things out, and the fact that I still needed to figure out if the problem was in R or C++. Dane commented that the object bindings we’d been looking at had little surface area for the problem to be in R - most of them were straight-up mappings from the C++ object to an R6 object with no extension. This was my first big clue! I remembered that there’s a bit of open_dataset() where we do some manual reconfiguration of specified options, which involves a whole load of R code - something I’ll come back to later. Dane also suggested I check out Stack Overflow to see if people were complaining about the issue in C++ there. I was sceptical that I’d find anything - lots of these bugs are more often surfaced in the R and Python libraries - but realised that this wasn’t the dead end that I’d thought. It suddenly occurred to me that if I could reproduce the bug in PyArrow, then the problem must lie in the C++ library, but if I couldn’t, then the problem lay in the R code.\nFifteen minutes later, and I had confirmed it was an R problem. I happened to mention on Slack the problem I was having, steps I’d taken so far to investigate, and potential ideas to look at next, and ended up engaging in a bit more collaborative debugging, this time with the wonderful Dewey Dunnington, who mentioned more disparities between PyArrow and R in terms of how we construct schemas, which put me on the path of testing the schema values at different points in Dataset creation and able to rule that out. At that point, with a smaller problem space to explore, the only logical thing left to look into was the R code which sets up the options for the various Arrow classes, and I ended up spotting the rogue instantiation of CSVReadOptions which just needed to have the partitioning column excluded (it relates to the reading in of the individual files which make up the dataset, and so has no “knowledge” of the partitions, and so previously raised an error as it treated them as an extraneous column).\nOne pull request later, and the problem that I’d given myself a week to look at had been solved in less than a day! This is probably one of the most gratifying bugs I’d worked on all year; there was a user with a problem to solve, a bug which had been annoying me for ages, the chance to fall into the puzzle-like aspects of debugging, and some great opportunities for collaboration with folks whose help here I really appreciated. This is one of the things I enjoy most about being a software engineer; this process of starting off feeling entirely clueless about something, and having to work out where I need to be and how I’m going to get there, and then doing it. Actually, in the abstract, that’s probably one of the things I enjoy most about being a human :)"
  },
  {
    "objectID": "posts/llms_for_learning/index.html",
    "href": "posts/llms_for_learning/index.html",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "",
    "text": "There’s a lot of advice out there at the moment about using LLMs for code generation, but something I’ve been curious about is using LLMs for learning around code.\nSpecifically, I don’t want to rely on LLMs to generate the answers for me every time; I want LLMs to help me gain skills so that next time I can generate the right answer myself.\nIn this blog post I’m going to talk about using an LLM, specifically, ChatGPT to extend my own knowledge in R."
  },
  {
    "objectID": "posts/llms_for_learning/index.html#context---adding-a-feature-to-daisyuiwidget",
    "href": "posts/llms_for_learning/index.html#context---adding-a-feature-to-daisyuiwidget",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "Context - adding a feature to daisyuiwidget",
    "text": "Context - adding a feature to daisyuiwidget\nI’m currently collaborating with Charlotte Hadley on an R package called daisyuiwidget, which is a htmlwidgets package.\n\nhtmlwidgets\nCharlie and I both really love htmlwidgets, which are used to wrap JavaScript libraries and work with them from R. They can be used in HTML outputs like Quarto documents and Shiny apps.\nOne example of a htmlwidgets package is DT, which can be used to create interactive tables. You can see an example below.\n\nlibrary(DT)\n\npackages &lt;- data.frame(\n    package = c(\"htmlwidgets\", \"leaflet\", \"plotly\", \"DT\"),\n    first_release = as.Date(c(\"2014-12-09\", \"2015-06-24\", \"2015-11-17\", \"2015-06-09\")\n    )\n)\n\nDT::datatable(packages)\n\n\n\n\n\nOne of the excellent features about packages like DT is that they can go beyond just displaying outputs. If you include a DT table in a Shiny app and have enabled “row selection”, then the JavaScript code for the table can pass the index of the user’s selected row back to R. This then can be used in other components of the app, for example, displaying information relevant to the user’s selection.\n\n\ndaisyuiwidget\nThe goal with daisyuiwidget is to create a new htmlwidgets package, using LLMs - we’ll be talking more about this in our joint presentation at Shiny in Production later on this year.\nWe decided to create a wrapper for the daisyUI library which has a feature where it can be used to create timelines like the one below.\n\n\n\n\n\nThe project was at the point where the package successfully created a timeline from a data.frame input. The next task for me to do was implement the functionality for timeline event selection, with the goal that when a user clicks on an event in the timeline, an input variable in R is updated, which then can be hooked up to other bits of Shiny reactivity.\nThis is what I’ll be discussing below - how I got ChatGPT to help with this, but with helping me learn how to do it myself, and not just getting the LLM to write the code."
  },
  {
    "objectID": "posts/llms_for_learning/index.html#aside---the-stretch-zone",
    "href": "posts/llms_for_learning/index.html#aside---the-stretch-zone",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "Aside - the stretch zone",
    "text": "Aside - the stretch zone\nI’m going to be talking at Posit Conf about increasing productivity with LLMs and a central thesis of my talk is the idea that LLMs are most useful when working on tasks just outside of your comfort zone, sometimes described as your stretch zone.\nI’m defining the “stretch zone” in this context as something that isn’t within your current knowledge, but doesn’t seem so challenging that you have doubts about whether you can find a solution.\nThis task seemed like a great fit for this in terms of my past experience:\n✅ Extensive Shiny app development experience\n✅ Experience using row selection in DT-based apps to update other components\n✅ Some JavaScript experience\n❌ Very limited experience of how to implement htmlwidgets package from scratch\n❌ Never implemented Shiny reactivity for UI components before"
  },
  {
    "objectID": "posts/llms_for_learning/index.html#learning-with-llms",
    "href": "posts/llms_for_learning/index.html#learning-with-llms",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "Learning with LLMs",
    "text": "Learning with LLMs\nI’d been reading a paper by Dr Xi Lin about the idea of using LLMs in self-directed learning.\n\nLin, X. (2023). Exploring the Role of ChatGPT as a Facilitator for Motivating Self-Directed Learning Among Adult Learners. Adult Learning, https://doi.org/10.1177/10451595231184928\n\nDr Lin discusses a self-directed learning framework proposed by Kim et al (2014) with the following aspects:\n\nEstablishing Learning Goals\nLocate and Access Resources\nAdopt and Execute Learning Activities\nMonitor and Evaluate Performance\nReassess Learning Strategies\n\nThis framework can be used with ChatGPT to help support adult learners, and Dr Lin goes on to walk through a hypothetical example where an adult learner is using ChatGPT in each of those steps and discusses the kind of help that could be given.\nI wanted to try some of this out, and so I created LLM summaries of each of the steps including Dr Lin’s suggestions of how an LLM might implement them, and used these summaries as prompts in a learning session with ChatGPT. I focused solely on steps 1 to 3 as the learning goal was an individual task rather than an ongoing programme."
  },
  {
    "objectID": "posts/llms_for_learning/index.html#step-1---establish-learning-goals",
    "href": "posts/llms_for_learning/index.html#step-1---establish-learning-goals",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "Step 1 - Establish Learning Goals",
    "text": "Step 1 - Establish Learning Goals\nDuration: 7 minutes\nI wrote a little context about what I was trying to achieve, and pasted that and my summary of step 1, into ChatGPT.\nChatGPT asked me five questions:\n\nwhat my goals were\nmy past experience\nexactly what it was that I was trying to achieve\nmy preferred end state\nhow long I wanted to spend working on this\n\nIt took my answers and generated a primary learning goal with supporting objectives.\n\nPrimary Learning Goal:\nWithin the next few hours, I want to understand and implement the necessary code to make my existing htmlwidget timeline respond to user interaction in a Shiny app—specifically, capturing a selected event and updating a variable on the server side.\nSupporting Objectives:\n\nIdentify how event selection can be captured in JavaScript and passed to Shiny via Shiny.setInputValue().\nModify the widget’s JS code to emit a Shiny input when a user selects an event.\nTest and verify that the input value is received in Shiny and reflects the expected event ID or index.\nWrite a minimal working Shiny app that uses the widget and displays the selected event value.\n\n\nI feel that the value I got here from incorporating ChatGPT was clarity. The resulting learning goal was more precise than my original stated aim, and the supporting objectives made for concrete steps to aim for in pursuit of the overall goal.\n\n\n\n\n\n\nExpand to view full conversation\n\n\n\n\n\n\nI am looking at how I can get LLMs to help me with a coding task as an intelligent tutor and not just solve the problem for me. My first step is establishing a learning goal or goals. Using the brief below, ask me questions one at a time to help me work out a small set of goals in this exercise.\nEstablish learning goals\nWhen establishing goals for self-directed learning, it’s most important for learners to set SMART goals: Specific, Measurable, Achievable, Relevant, and Time-bound. This approach helps adult learners identify clear learning objectives and stay focused on achieving them.\nGoals should be personalized and align with the learner’s interests, motivations, and expected learning outcomes. It’s crucial to break down broader objectives into smaller, manageable tasks to make them more achievable.\nAdditionally, learners should consider how their goals relate to their personal and professional contexts, ensuring relevance. Setting a realistic timeline for accomplishing these goals is also essential.\nWhile learners can use tools like ChatGPT to assist in goal-setting, it’s important to remember that the quality of recommendations depends on the information provided by the learner. Therefore, clear communication of interests, motivations, and expectations is crucial when using such tools.\n\n\nLet’s start by grounding this in why you’re doing it.\nQuestion 1: What’s your motivation for using LLMs as a tutor rather than a problem-solver—what do you hope to gain from this approach (e.g., deeper understanding, confidence, skill retention, etc.)?\n\n\nI want to understand how I can take an R package which creates a htmlwidget and adds the correct code so that I can use it in a shiny app and update a variable based on what the user has selected. I want to learn how to do this as I think it’s a useful skill to have. I don’t want an LLM to do it as I won’t know if the code is correct or not and would only have surface level understanding and so would be hard to repeat the skill in future.\n\n\nQuestion 2:\nWhere are you starting from? For example:\n\nHave you ever written your own htmlwidget package from scratch\nDo you already have a specific widget/package in mind?\nHave you previously connected any JS-based input to Shiny reactivity?\n\n(Just answer what’s true for you—this is to calibrate the scope of your learning goal.)\n\n\nI have never written an htmlwidget package from scratch although I do have one I’ve written with an LLM that I’d like to add this functionality to. It creates the widget but doesn’t have the shiny functionality yet. I’ve not done much of this connectivity before, though have worked with packages that implement it themselves.\n\n\nThat gives us a baseline. You’ve got a partial htmlwidget and prior exposure to packages that do this right, but you haven’t yet wired up JS-to-R reactivity yourself.\nQuestion 3:\nWhat exactly do you want the user to select in your widget, and what kind of R-side variable or behavior do you want to update in response? (Try to be concrete—e.g., “user clicks on a bar in a chart, I want to update a reactiveVal() in the server with the bar’s label.”)\n\n\nSo the widget display a timeline of events and I want the user to select an event, and some variable updates. I’ve seen similar functionality in the DT package where the user clicks on a row in a table, and some variable tracks which row is selected and the data from that row. I can’t remember exactly what data it tracks, it could just be the row number and then the user can write a function to extract the data from an underlying data.frame; this is fine too.\n\n\nPerfect—this is concrete enough.\nYou want to let the user select an event from a timeline displayed via your htmlwidget, and capture that selection in the Shiny server logic—ideally as an index or ID you can work with to retrieve more data.\nQuestion 4:\nWhat’s your ideal end state for this learning exercise? For example:\n\nDo you want to be able to reproduce this pattern in other widgets you make?\nAre you aiming to read or modify similar functionality in other packages?\nOr is this just about getting this one widget working?\n\n\n\nIt’s about getting this one working, although I would like to be able to do it again if I ever make another htmlwidgets package.\n\n\nGot it. So the primary goal is getting this specific widget working, and a secondary goal is gaining enough understanding to repeat the pattern if needed.\nQuestion 5:\nWhat’s your time horizon? Do you want to finish this within a day, a week, or are you treating it as an open-ended background task?\n\n\nI’d like to finish this within a few hours and not overcomplicate it\n\n\nGood. A short, focused goal with an emphasis on practical understanding.\nHere’s a draft of your SMART-aligned learning goals based on what you’ve said:\n\nPrimary Learning Goal:\nWithin the next few hours, I want to understand and implement the necessary code to make my existing htmlwidget timeline respond to user interaction in a Shiny app—specifically, capturing a selected event and updating a variable on the server side.\nSupporting Objectives:\n\nIdentify how event selection can be captured in JavaScript and passed to Shiny via Shiny.setInputValue().\nModify the widget’s JS code to emit a Shiny input when a user selects an event.\nTest and verify that the input value is received in Shiny and reflects the expected event ID or index.\nWrite a minimal working Shiny app that uses the widget and displays the selected event value.\n\n\nDo you want to adjust or narrow any part of that before we move to designing how the LLM will help you stay in tutor mode for this?"
  },
  {
    "objectID": "posts/llms_for_learning/index.html#step-2---locate-and-access-resources",
    "href": "posts/llms_for_learning/index.html#step-2---locate-and-access-resources",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "Step 2 - Locate and Access Resources",
    "text": "Step 2 - Locate and Access Resources\nDuration: 11 minutes\nThe second stage was identifying resources for learning.\nIt started smoothly - ChatGPT asked me if I preferred to learn by reading, watching, or doing, and when I replied “doing” it assured me that it would find code-first resources with working examples - great!\nIt gave me some options about kinds of examples I was interested in, and asked which htmlwidgets packages I was more familiar with so we could use those as examples to look at. At this point it suggested some initial links, but 2 out of the 3 didn’t actually work.\nWhen I complained about this, it gave me some code examples to try out, which I pushed back on, stating I wanted resources to learn from and not ChatGPT-generated code,\nAt this point, it gave me a list of resources, divided into categories and a summary of what it had provided. The links supplied here did all work this time, but there was too much information, and a real risk of overwhelm.\nI began to skim through a few articles, before I remembered that the aim of this step was only to identify resources, and not to deep dive into them quite yet, and so instead I moved onto the next step.\n\n\n\n\n\n\nExpand to view full conversation\n\n\n\n\n\n\nNow the next thing I need to do it find learning resources. Here are the notes I have.\n\nLocate and access resources\n\nWhen it comes to locating and accessing resources for self-directed learning, the most important things to know are:\nAdult learners often struggle with finding available resources, particularly interactive and multimedia materials. This challenge can be a significant barrier to effective self-directed learning.\nLearners should proactively identify a variety of resources beyond those provided by instructors. These can include online articles, videos, multimedia resources, books, journals, publications, blogs, and social media groups related to their learning objectives.\nTools like ChatGPT can assist in recommending relevant external resources based on personalized learning plans. However, learners should be cautious and verify the information provided by such tools.\nIt’s crucial to develop skills in searching for and accessing information, as insufficient technical skills can hinder this process. Improving these skills is an important part of self-directed learning.\n\n\nLet’s approach this systematically.\nQuestion 1:\nDo you learn better by reading, watching, or doing?\n(That’ll help decide whether to focus on docs, code examples, videos, interactive tutorials, etc.)\n\n\nI learn best by doing\n\n\nGreat—so you’ll want code-first resources, ideally with small working examples you can tinker with.\nQuestion 2:\nWhat kind of prior art do you want to learn from? For example:\n\nExisting htmlwidgets that support Shiny inputs (like DT, leaflet, plotly)?\nGeneral tutorials on writing htmlwidgets?\nTutorials or examples specifically about integrating JS events with Shiny via Shiny.setInputValue()?\nOr are you mostly looking for real widget repos to read and adapt?\n\n\n\nItems 1 and 3 on your list look good!\n\n\nGood. Then your resource categories are:\n\nReal htmlwidget packages with Shiny input support — to reverse-engineer and adapt\nTargeted examples or tutorials on using Shiny.setInputValue() inside htmlwidgets\n\nLet’s start with category 1.\nQuestion 3:\nPick one or two widgets you’ve used or are familiar with—ideally ones where clicking/selecting something in the widget updates a Shiny input. (e.g., DT::datatable, plotly::plotlyOutput). Which ones should we dissect?\n\n\nI’m mostly familiar with those two you mention, more so DT then plotly tho\n\n\nPerfect. DT is a solid choice—it’s well-documented, simple enough to follow, and has clean Shiny integration.\nHere’s what I suggest for Category 1 (Prior art):\n\nPrimary resource:\n\nDT GitHub repo – especially inst/htmlwidgets/datatables.js and how it calls Shiny.setInputValue() when rows are selected.\nFocus on how selection is captured (usually via a JS event listener) and how that’s passed back to Shiny.\n\nBackup/contrast resource:\n\nplotly – check srcjs/ for how it emits click or hover events to Shiny. Slightly more complex but good for broader perspective.\n\n\nNow for Category 3 (Targeted examples of Shiny.setInputValue()):\n\n*Mini-tutorials / blogs / gists**:\n\nWinston Chang’s guide on JS &lt;-&gt; Shiny – clear and concise\nDean Attali: Custom inputs – practical advice\nRStudio’s official htmlwidgets Shiny integration doc – short, focused example\n\n\nWant me to fetch code excerpts or links for any of those right now? Or would you rather search through them yourself and come back with questions?\n\n\nThe Dean Attali post looks great and all, but the other two guides links don’t work.\n\n\nYou’re leaning into learning by doing, which means you need hands‑on code examples where you can modify and test. That’s the right approach.\nHere are solid, working resources for your two resource types:\n\n\n🧩 1. Real htmlwidget packages with Shiny input support – reverse engineering\n\nDean Attali’s “htmlwidgets tips” post explains how timevis uses JavaScript to detect item selection and sends it to Shiny:\ntimeline.on('select', properties =&gt; {\n  Shiny.onInputChange(elementId + \"_selected\", properties.items);\n});\nThis snippet, wrapped in a check for HTMLWidgets.shinyMode, ensures that Shiny receives the selected IDs via the widget’s container ID (JavaScript for R, Dean Attali).\n\n\n\n\n🎯 2. Tutorials/examples of using Shiny.setInputValue() or .onInputChange()\n\nCommunicating with Shiny via JavaScript (Posit article) covers sending values directly from JS to R using Shiny.setInputValue(id, value);. It’s the modern and preferred method (older Shiny.onInputChange() is an alias, but can still work) (Shiny).\nRyouReady blog post offers a minimal example showing an HTML div with an onclick handler that sends a random number to R via Shiny.onInputChange(\"mydata\", number) and displays it reactively server-side (“R” you ready?).\n\n\n\n✅ Quick Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResource\n\nWhy it helps\n\nFocus on\n\n\n\n\n\n\n\n\n\n\n\n\n\nDean Attali (“htmlwidgets tips”)\n\nReal widget code invoking JS event + input change\n\ntimeline.on('select', ...) and chaining to Shiny\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosit “Communicating with JS”\n\nModern API and rationale\n\nShiny.setInputValue(...) inside a listener\n\n\n\n\n\n\n\n\n\n\n\n\n\nRyouReady blog\n\nMinimal standalone JS ↔︎ R demo\n\nUsing Shiny.onInputChange in UI and reading input$mydata in server\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📌 Suggested next step\nPick one real widget example (like timevis or similar) and one minimal demo. Then:\n\nReview the JS code where selection events are handled and Shiny.setInputValue() (or .onInputChange()) is called.\nModify your widget JS similarly—hook into user’s event selection and send the event ID (or index) via Shiny.setInputValue().\nAdd reactive code in your server to observe input$&lt;widgetId&gt;_selected and log or display it.\nBuild a minimal working Shiny app with your widget and observe whether it captures user clicks.\nWant me to pull that specific “page code” from the timevis repo or the Attali post so you can step through it?\n\n\n\n\n\nWait, can you find me more resources? I don’t want to learn from your summaries\n\n\nHere’s a curated list of solid, hands‑on resources to go beyond summaries and dive into real working code you can adapt:\n\n\n🔧 Real htmlwidget packages with Shiny input support\n\nDean Attali — “How to write a useful htmlwidgets in R: tips and walk‑through a real example”\n\nDeals directly with timevis and how it handles events: it includes a live example of passing selections back to Shiny via JavaScript callbacks CRAN+8Dean Attali+8Stack Overflow+8Shiny+2r4ds.github.io+2engineering-shiny.org+2.\n“Widgets with Shiny” ‑ JavaScript for R book (Chapter 17)\n\nCovers patterns for embedding event handlers in widget JS and using Shiny.setInputValue() in the context of htmlwidgets JavaScript for Rdatanovia+3engineering-shiny.org+3Stack Overflow+3.\n\n\n\n\n🧠 Tutorials and minimal examples on Shiny.setInputValue()\n\nPosit article — “Communicating with Shiny via JavaScript”\n\nConcise and modern: shows core usage of Shiny.setInputValue(id, value) and the optional {priority: \"event\"} flag to treat values as events rstudio.github.io+7Shiny+7r4ds.github.io+7.\n“How to build a JavaScript‑based widget” by Posit\n\nBuilds an example widget using D3/C3 and includes code on client‑server communication with Shiny (including setInputValue) Shiny+1JavaScript for R+1.\nR for Data Science Bookclub: “JavaScript ↔︎ Shiny Communication”\n\nOffers compact code snippets showing how to send data back using Shiny.setInputValue and receive it in R with reactive observers htmlwidgets.org+15r4ds.github.io+15datanovia+15.\n\n\n\n\n📚 JavaScript integration & custom input patterns\n\nDatanovia article — “JavaScript Integration and Custom Functionality: Extend Shiny Beyond R” (May 2025)\n\nA recent and comprehensive guide to advanced interaction patterns, custom bindings, and message passing between JS and Shiny JavaScript for R+2datanovia+2CRAN+2.\n\n\n\n\n🧪 StackOverflow & minimal snippets\n\nSO Example — Reactable row click setting input value in Shiny modules\n\nShows how onClick handlers call Shiny.setInputValue() to pass selected row data into R, along with observeEvent responses Stack Overflow+1engineering-shiny.org+1.\nSO thread — Reading selection from radialNetworkR htmlwidget in Shiny\n\nIllustrates a real-world case where someone needed to capture user selection in a custom widget, and references using Tip 7a from Dean’s blog for guidance search.r-project.org+10Stack Overflow+10nielsenmark.us+10.\n\n\n\n\n📋 Summary Table\n\n\n\n\n\n\n\n\nResource\nUse Case\nWhat you’ll get\n\n\n\n\nDean Attali (timevis code)\nReal-world widget code\nUnderstand how JS event to Shiny input is implemented\n\n\nPosit JS‑Shiny tutorial\nCore API\nClean examples of Shiny.setInputValue() usage\n\n\nJS for R book (Chapter 17)\nIn‑widget integration\nFull example of wiring JS and htmlwidgets with Shiny\n\n\nDatanovia (May 2025)\nAdvanced patterns\nModern perspectives on custom input and messaging\n\n\nSO posts (Reactable, radialNetworkR)\nMinimal demos\nConcrete handlers showing event → R logic"
  },
  {
    "objectID": "posts/llms_for_learning/index.html#real-htmlwidget-packages-with-shiny-input-support-reverse-engineering",
    "href": "posts/llms_for_learning/index.html#real-htmlwidget-packages-with-shiny-input-support-reverse-engineering",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "🧩 1. Real htmlwidget packages with Shiny input support – reverse engineering",
    "text": "🧩 1. Real htmlwidget packages with Shiny input support – reverse engineering\n\nDean Attali’s “htmlwidgets tips” post explains how timevis uses JavaScript to detect item selection and sends it to Shiny:\ntimeline.on('select', properties =&gt; {\n  Shiny.onInputChange(elementId + \"_selected\", properties.items);\n});\nThis snippet, wrapped in a check for HTMLWidgets.shinyMode, ensures that Shiny receives the selected IDs via the widget’s container ID (JavaScript for R, Dean Attali)."
  },
  {
    "objectID": "posts/llms_for_learning/index.html#tutorialsexamples-of-using-shiny.setinputvalue-or-.oninputchange",
    "href": "posts/llms_for_learning/index.html#tutorialsexamples-of-using-shiny.setinputvalue-or-.oninputchange",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "🎯 2. Tutorials/examples of using Shiny.setInputValue() or .onInputChange()",
    "text": "🎯 2. Tutorials/examples of using Shiny.setInputValue() or .onInputChange()\n\nCommunicating with Shiny via JavaScript (Posit article) covers sending values directly from JS to R using Shiny.setInputValue(id, value);. It’s the modern and preferred method (older Shiny.onInputChange() is an alias, but can still work) (Shiny).\nRyouReady blog post offers a minimal example showing an HTML div with an onclick handler that sends a random number to R via Shiny.onInputChange(\"mydata\", number) and displays it reactively server-side (“R” you ready?).\n\n\n\n✅ Quick Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResource\n\nWhy it helps\n\nFocus on\n\n\n\n\n\n\n\n\n\n\n\n\n\nDean Attali (“htmlwidgets tips”)\n\nReal widget code invoking JS event + input change\n\ntimeline.on('select', ...) and chaining to Shiny\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosit “Communicating with JS”\n\nModern API and rationale\n\nShiny.setInputValue(...) inside a listener\n\n\n\n\n\n\n\n\n\n\n\n\n\nRyouReady blog\n\nMinimal standalone JS ↔︎ R demo\n\nUsing Shiny.onInputChange in UI and reading input$mydata in server\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n📌 Suggested next step\nPick one real widget example (like timevis or similar) and one minimal demo. Then:\n\nReview the JS code where selection events are handled and Shiny.setInputValue() (or .onInputChange()) is called.\nModify your widget JS similarly—hook into user’s event selection and send the event ID (or index) via Shiny.setInputValue().\nAdd reactive code in your server to observe input$&lt;widgetId&gt;_selected and log or display it.\nBuild a minimal working Shiny app with your widget and observe whether it captures user clicks.\nWant me to pull that specific “page code” from the timevis repo or the Attali post so you can step through it?"
  },
  {
    "objectID": "posts/llms_for_learning/index.html#step-3---adopt-and-execute-learning-activties",
    "href": "posts/llms_for_learning/index.html#step-3---adopt-and-execute-learning-activties",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "Step 3 - Adopt and Execute Learning Activties",
    "text": "Step 3 - Adopt and Execute Learning Activties\nDuration: 36 minutes\nThe third and final step was the most useful by far!\nOnce again, I pasted in the guide to this step, and ChatGPT created a plan for me containing four items:\n\nReverse-engineer one working widget\nBuild minimal standalone Shiny demo\nAdd minimal event tracking to your widget\nWire the widget into a basic Shiny app\n\nThis was actually pretty impressive; each step contained a goal, the steps required to achieve it, suggestions for how an LLM could help, and a timebox of how long to spend on each activity.\nHere’s the first activity suggested:\n\n🔍 1. Reverse-engineer one working widget\nGoal: Understand how an existing widget (e.g. DT) captures selection and sends it to Shiny.\nActivity:\n\nOpen the DT GitHub repo.\nLocate where Shiny.setInputValue() is used in the JS files (inst/htmlwidgets/datatables.js).\nMake brief notes:\n\nWhat JS event triggers the update?\nHow is the input ID constructed?\nWhat data is passed to Shiny?\n\n\nLLM Role:\n\nAsk ChatGPT to walk you through unfamiliar lines or patterns (e.g. “What’s this event handler doing?” or “What does this argument to setInputValue mean?”).\n\nTimebox: 20–30 minutes.\n\nI walked through each activity, setting a timer on my phone. I found that I completed each in less than the allotted time. I skipped the last activity, as I only required 1-3 to complete the task.\n\n\n\n\n\n\nExpand to view learning plan\n\n\n\n\n\n\nOK, my next step is below\n\nAdopt and Execute Learning Activities\n\nAdopting and executing learning activities is a crucial task in self-directed learning. This step involves developing a specific plan of action that aligns with established goals and available resources.\nLearners should create tailored plans that consider their preferred learning styles and available time for learning. This personalization helps ensure the learning activities are effective and manageable.\nTime-management techniques, such as spaced repetition and active recall, are important to incorporate into the learning plan. These strategies can help learners stay on track and optimize their learning process.\nLearning activities should be diverse and match the learner’s preferred style. This might include visual aids, podcasts, reading materials, or hands-on activities, depending on whether the learner is visual, auditory, reading/writing, or kinesthetic.\nSupplementing core learning activities with additional resources, such as online workshops, can enhance the learning experience and provide different perspectives on the subject matter.\n\n\n🛠️ Execution Plan: LLM-Assisted Learning Without Code Outsourcing\n\n⏱️ Estimated Total Time: ~2–3 hours\n\n\n\n🔍 1. Reverse-engineer one working widget\nGoal: Understand how an existing widget (e.g. DT) captures selection and sends it to Shiny.\nActivity:\n\nOpen the DT GitHub repo.\nLocate where Shiny.setInputValue() is used in the JS files (inst/htmlwidgets/datatables.js).\nMake brief notes:\n\nWhat JS event triggers the update?\nHow is the input ID constructed?\nWhat data is passed to Shiny?\n\n\nLLM Role:\n\nAsk ChatGPT to walk you through unfamiliar lines or patterns (e.g. “What’s this event handler doing?” or “What does this argument to setInputValue mean?”).\n\nTimebox: 20–30 minutes.\n\n\n\n🧪 2. Build minimal standalone Shiny demo\nGoal: Create a toy Shiny app where JS sends a value to R.\nActivity:\n\nUse this Posit guide or RyouReady’s example to build a plain HTML div or button that triggers Shiny.setInputValue() in a browser console or inline JS.\n\nLLM Role:\n\nAsk it to review the minimal app you’ve built and explain/debug if input isn’t showing up server-side.\n\nTimebox: 20 minutes.\n\n\n\n🧱 3. Add minimal event tracking to your widget\nGoal: Modify your widget to track one user interaction and send something to Shiny.\nActivity:\n\nIn your widget’s JS binding, add an event listener (e.g. click/select).\nSend a dummy value to Shiny using Shiny.setInputValue().\nObserve input in input$yourwidget_selected or similar.\n\nLLM Role:\n\nAsk for feedback on the specific event binding you’re trying (“Why isn’t this firing?” or “What selector do I use to bind the click?”).\n\nTimebox: 45–60 minutes.\n\n\n\n🧪 4. Wire the widget into a basic Shiny app\nGoal: End-to-end test: selection in your timeline → reactive output in Shiny.\nActivity:\n\nWrite a basic Shiny app using your widget.\nUse reactivePrint(), observeEvent(), or renderText() to display the selected item.\n\nLLM Role:\n\nAsk it to sanity check your Shiny app logic or spot input ID mismatches.\n\nTimebox: 30 minutes.\n\n\n\nOptional: 🔁 Space repetition (1–2 days later)\nCome back and:\n\nRebuild the Shiny example from scratch.\nAdd a new input (e.g. multiple selection).\nExplain the flow (JS → R) out loud or in a notebook.\n\n\n\n\n✅ How I can help\nYou run the show. Use me like a senior dev standing over your shoulder:\n\n“Explain this line of JS.”\n“This event handler isn’t firing—what’s wrong?”\n“What’s the right selector for this DOM node?”\n“Can you sanity-check this widget binding?”\n“What’s the minimal Shiny app I can use to test this?”\n\n\nWant a markdown checklist version of this plan to copy into your notes?\n\n\n\n\n\nThere was a lot of interaction between me and ChatGPT here, and I’m not going to paste it all, but essentially, during the time I used it to:\n\nexplain some JavaScript code I didn’t understand by walking me through it line-by-line\nget clarity on how to implement some of its suggestions by asking exactly what it meant\nmultiple rounds of debugging code I had written which didn’t work\nhelping me work out exactly where the change was needed in my own code\n\nIn the last example, ChatGPT didn’t just tell me where to put the code, but walked through the important concepts to reason through the answer. It then asked me questions so that I could work out where in the code the change needed to go. Here’s a snippet from that interaction.\n\n🤔 So… where do you want interactivity?\nLet me ask you this:\n\nOut of everything that’s being created here, what part of the DOM would the user actually click on if they were selecting a timeline event?\n\nOnce we know that, we can talk about how and where to attach the event handler.\n\n\n\nI think they’d click on the timeline-box element\n\n\n\nExactly—that’s the actionable node.\nThat means you’ll want to attach a JS event handler (e.g. .onclick or .addEventListener('click', ...)) to each .timeline-box as it’s created.\nNow a follow-up:\n\nWhen you’re looping over x.events, what info do you need to send back to Shiny when a .timeline-box is clicked? (Think: what identifier, value, or payload do you want to capture?)\n\n\nWe carried on the back and forth like this for a while, with ChatGPT asking me questions to guide me towards the solution. If I got an answer wrong, it replied with an encouraging tone and explained why I was incorrect, and the specific piece of knowledge I was missing.\nEventually, we go to the end of our interaction.\n\nIt’s working!!\n\n\n\nExcellent. That means your widget is now:\n\nRendering cleanly ✅\nSending input to Shiny ✅\nUpdating on click ✅\n\nYou’ve now got the full JS → R feedback loop working in your custom htmlwidget.\n\nIf you want to improve it later, next steps could include:\n\nMaking the selected element visually highlighted (add/remove a CSS class)\nSending richer data to Shiny (e.g. event ID, label, timestamp)\nSupporting multiple selection or deselection\nWriting a unit test or snapshot test for the widget rendering\n\n\n\nBut honestly—you’ve just crossed the hardest barrier. Nice work.\n\n\nAnything else you want to do with it right now?\n\n\nIt sounds like a lot, but this whole experience took around 1 hour from end to end which I found pretty impressive."
  },
  {
    "objectID": "posts/llms_for_learning/index.html#execution-plan-llm-assisted-learning-without-code-outsourcing",
    "href": "posts/llms_for_learning/index.html#execution-plan-llm-assisted-learning-without-code-outsourcing",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "🛠️ Execution Plan: LLM-Assisted Learning Without Code Outsourcing",
    "text": "🛠️ Execution Plan: LLM-Assisted Learning Without Code Outsourcing\n\n⏱️ Estimated Total Time: ~2–3 hours\n\n\n\n🔍 1. Reverse-engineer one working widget\nGoal: Understand how an existing widget (e.g. DT) captures selection and sends it to Shiny.\nActivity:\n\nOpen the DT GitHub repo.\nLocate where Shiny.setInputValue() is used in the JS files (inst/htmlwidgets/datatables.js).\nMake brief notes:\n\nWhat JS event triggers the update?\nHow is the input ID constructed?\nWhat data is passed to Shiny?\n\n\nLLM Role:\n\nAsk ChatGPT to walk you through unfamiliar lines or patterns (e.g. “What’s this event handler doing?” or “What does this argument to setInputValue mean?”).\n\nTimebox: 20–30 minutes.\n\n\n\n🧪 2. Build minimal standalone Shiny demo\nGoal: Create a toy Shiny app where JS sends a value to R.\nActivity:\n\nUse this Posit guide or RyouReady’s example to build a plain HTML div or button that triggers Shiny.setInputValue() in a browser console or inline JS.\n\nLLM Role:\n\nAsk it to review the minimal app you’ve built and explain/debug if input isn’t showing up server-side.\n\nTimebox: 20 minutes.\n\n\n\n🧱 3. Add minimal event tracking to your widget\nGoal: Modify your widget to track one user interaction and send something to Shiny.\nActivity:\n\nIn your widget’s JS binding, add an event listener (e.g. click/select).\nSend a dummy value to Shiny using Shiny.setInputValue().\nObserve input in input$yourwidget_selected or similar.\n\nLLM Role:\n\nAsk for feedback on the specific event binding you’re trying (“Why isn’t this firing?” or “What selector do I use to bind the click?”).\n\nTimebox: 45–60 minutes.\n\n\n\n🧪 4. Wire the widget into a basic Shiny app\nGoal: End-to-end test: selection in your timeline → reactive output in Shiny.\nActivity:\n\nWrite a basic Shiny app using your widget.\nUse reactivePrint(), observeEvent(), or renderText() to display the selected item.\n\nLLM Role:\n\nAsk it to sanity check your Shiny app logic or spot input ID mismatches.\n\nTimebox: 30 minutes.\n\n\n\nOptional: 🔁 Space repetition (1–2 days later)\nCome back and:\n\nRebuild the Shiny example from scratch.\nAdd a new input (e.g. multiple selection).\nExplain the flow (JS → R) out loud or in a notebook.\n\n\n\n\n✅ How I can help\nYou run the show. Use me like a senior dev standing over your shoulder:\n\n“Explain this line of JS.”\n“This event handler isn’t firing—what’s wrong?”\n“What’s the right selector for this DOM node?”\n“Can you sanity-check this widget binding?”\n“What’s the minimal Shiny app I can use to test this?”\n\n\nWant a markdown checklist version of this plan to copy into your notes?"
  },
  {
    "objectID": "posts/llms_for_learning/index.html#the-final-output",
    "href": "posts/llms_for_learning/index.html#the-final-output",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "The final output",
    "text": "The final output\nHere is the final PR: https://github.com/thisisnic/daisyuiwidget/pull/11. It’s not particularly complicated, but in the hour it took to write, I learned about some of the internals of the DT package, created a standalone example of sending information between JavaScript and R, and then implemented this in the package."
  },
  {
    "objectID": "posts/llms_for_learning/index.html#takeaways-from-a-structured-llm-learning-session",
    "href": "posts/llms_for_learning/index.html#takeaways-from-a-structured-llm-learning-session",
    "title": "Using ChatGPT as a Code Tutor: A One-Hour Learning Experiment",
    "section": "Takeaways from a structured LLM Learning session",
    "text": "Takeaways from a structured LLM Learning session\nWhat made this work wasn’t that ChatGPT giving me the right code but that it helped me ask the right questions. It acted like a more senior engineer, talking me through unfamiliar concepts, helping me debug my code, and nudging me towards the right mental model.\n\nLess overhead, faster feedback\nIt helped reduce context-switching and help me stay in flow. I didn’t need to spend time refining my overall goal, deciding what steps to take, and skim through partially relevant resources. By instead being able to focusing solely on writing the code, it prevented the cognitive fatigue of having to switch between “coding” and “planning” mindsets.\n\n\nStill my own work\nCould I have done it without the LLM? Yes, but it would have taken longer, and I would have learned less.\nAlthough I was using an LLM, I was an active participant in my own learning and so felt empowered as a learner. ChatGPT provided scaffolding and support but not all of the answers.\n\n\nWhat I’d change next time\nA reasonable next step to develop this idea further would be to refine the prompts for each stage - the prompts I included were just summaries of the snippets of the article, rather than specific instructions for ChatGPT. In small tasks like this, I’d treat information gathering as part of the task, and collapse steps 2 and 3.\nI found this process really rewarding to try out, and I’d encourage others to give it a go. Let me know how you get on if you do!"
  },
  {
    "objectID": "posts/eval-parse/index.html",
    "href": "posts/eval-parse/index.html",
    "title": "Why is eval parse bad?",
    "section": "",
    "text": "I was writing Arrow bindings for na.omit and needed a way of applying is.na to every column of a table at once. I’d originally implemented using dplyr::filter, but had failed to realise that Arrow only suggests dplyr but doesn’t import it, and so needed to refactor it.\nTable objects in Arrow have a Filter method, and I knew that a naive implementation would be a for loop which iteratively called Table$Filter(!is.na(column_name)) for each column in my table. However, this would be ugly at best and inefficient at worst, and so what I really wanted was to be able to call Table$Filter(!is.na(col1) & !is.na(col2) & ... !is.na(colN)) where 1…N are my column names.\nAn early implementation without dplyr::filter looked like this:\nfilter_text = paste0(\"!is.na(tbl$\", names(tbl), \")\", collapse = \" & \")\nfilter = rlang::parse_expr(filter_text)\ntbl$Filter(rlang::eval_bare(filter))\nTo break this down a bit, I’ll demonstrate a line by line run, using airquality as my input data. Below you can see that we start off with 153 rows.\ntbl = arrow::Table$create(airquality)\ntbl\n## Table\n## 153 rows x 6 columns\n## $Ozone &lt;int32&gt;\n## $Solar.R &lt;int32&gt;\n## $Wind &lt;double&gt;\n## $Temp &lt;int32&gt;\n## $Month &lt;int32&gt;\n## $Day &lt;int32&gt;\n##\n## See $metadata for additional Schema metadata\nIf I call na.omit on the airquality data.frame, I’m left with 111 rows.\nnrow(na.omit(airquality))\n## [1] 111\nSo now for the Arrow implementation. I first construct my filter:\nfilter_text = paste0(\"!is.na(tbl$\", names(tbl), \")\", collapse = \" & \")\nfilter_text\n## [1] \"!is.na(tbl$Ozone) & !is.na(tbl$Solar.R) & !is.na(tbl$Wind) & !is.na(tbl$Temp) & !is.na(tbl$Month) & !is.na(tbl$Day)\"\nI then convert the string to an expression.\nfilter = rlang::parse_expr(filter_text)\nfilter\n## !is.na(tbl$Ozone) & !is.na(tbl$Solar.R) & !is.na(tbl$Wind) &\n##     !is.na(tbl$Temp) & !is.na(tbl$Month) & !is.na(tbl$Day)\nFinally, I evaluate the expression:\ntbl$Filter(rlang::eval_bare(filter))\n## Table\n## 111 rows x 6 columns\n## $Ozone &lt;int32&gt;\n## $Solar.R &lt;int32&gt;\n## $Wind &lt;double&gt;\n## $Temp &lt;int32&gt;\n## $Month &lt;int32&gt;\n## $Day &lt;int32&gt;\n##\n## See $metadata for additional Schema metadata\nIt looks like it’s worked - it has! - but something feels wrong. I’ve heard people in the past talk about eval(parse(text = stuff)) being a bad way of doing things, and my solution felt a bit dirty in a similar way, though I wasn’t sure why.\nA bit of a search and I found an answer on StackOverflow. In short, the main reasons that eval/parse is warned against are:\n\nif you’re evaluating text passed in by a user there could be security concerns\nthere are other more efficient and human-readable ways of accomplishing the same thing\nit may be unclear what’s being evaluated from what environment\n\nSo, my code wasn’t as bad as I thought. However, in another bit of the Arrow codebase I’d seen some pretty nice code written using data masks and environment, and so refactored my code a little to mirror this style.\n# paste together the filter function as a character vector\nfilter_text = paste0(\n  \".data$Filter(\",\n  paste0(\"!is.na(.data$\", names(tbl), \")\", collapse = \" & \"),\n  \")\"\n)\n# parse expression; results in:\n# .data$Filter(!is.na(.data$Ozone) & !is.na(.data$Solar.R) & !is.na(.data$Wind) &\n#  !is.na(.data$Temp) & !is.na(.data$Month) & !is.na(.data$Day))\nfilter = rlang::parse_expr(filter_text)\n\n# tidily evaluate the results, within the data mask\nrlang::eval_tidy(filter,  rlang::new_data_mask(rlang::env(.data = tbl)))\nThe above solution is my latest one at time of writing but hasn’t been reviewed or merged yet, so may require further changes. Here, I move in the called to Table$Filter into the filter text, so the whole expression is captured. This now means that I can use the .data pronoun to refer to the dataset, and instead of depending on the fact that a variable called tbl has been passed into my function, can instead manually pass it in when I call rlang::env.\nI’m happier with this solution; whilst it still feels weird to construct calls using strings, at least this way, what is being called against what feels a little cleaner.\nOne thing that really helped me work all this out was the rlang cheatsheet, which is a resource I’d recommend to anyone wrestling with tidy eval.\n[Edited to add below]\nSo in the end, after discussing with a colleague, we decided that parsing an expression was still potentially problematic. Although I’d dismissed the possible security concerns, they mentioned that it could be the case that someone has a dodgy column name, and it was unclear whether encapsulating things in their own environment would remove this risk or not. They suggested that I see if I could get purrr to do something useful, and after a little experimentation, here was my eventual solution.\n# Creates a list of ChunkedArrays containing true/false for each value depending\n# if it's an NA or not\nnot_na &lt;- purrr::map(tbl$columns, ~!is.na(.x))\n\n# reduces across lists to a single ChunkedArray (i.e. only true if all rows contain true)\nnot_na_agg &lt;- purrr::reduce(not_na, `&`)\n\n# filter only to retrieve rows which match with true\ntbl$Filter(not_na_agg)\nAs is common in these circumstances, there was a simpler and more aesthetically pleasing solution available that didn’t require parsing of expressions. I’m much happier with the above code - it feels like a much better approach!\n[Edited again!]\nUltimately we went with a different approach that implemented functionality that already exists in the Arrow package. This approach is even better as, instead of evaluating !is.na on each column of data and combining them, we instead build an expression that is evaluated against the dataset.\nnot_na &lt;- purrr::map(tbl$columns, ~arrow:::build_array_expression(\"is_valid\", .x))\nnot_na_agg &lt;- Reduce(\"&\", not_na)\ntbl$Filter(arrow:::eval_array_expression(not_na_agg))\n## Table\n## 111 rows x 6 columns\n## $Ozone &lt;int32&gt;\n## $Solar.R &lt;int32&gt;\n## $Wind &lt;double&gt;\n## $Temp &lt;int32&gt;\n## $Month &lt;int32&gt;\n## $Day &lt;int32&gt;\n##\n## See $metadata for additional Schema metadata"
  },
  {
    "objectID": "posts/packagemetrics/index.html",
    "href": "posts/packagemetrics/index.html",
    "title": "Comparing R packages using packagemetrics",
    "section": "",
    "text": "A colleague asked for my opinion on 2 packages; loggit and futile.logger. Whilst I have used futile.logger before, I hadn’t used loggit and so used the metrics of the package to evaluate the package itself.\nThe packagemetrics package allows us to generate a number of metrics about a package, so we can compare them. The first thing I do is call package_list_metrics() to get metrics about these two packages. I’ve changed the shape of the table, just so it’s easier to read in this blog post.\nlibrary(packagemetrics)\nlibrary(dplyr)\npkg_df &lt;- package_list_metrics(c(\"loggit\", \"futile.logger\"))\nas_data_frame(t(pkg_df), rownames = \"stat\")\n## # A tibble: 18 x 3\n##    stat               V1                           V2\n##    &lt;chr&gt;              &lt;chr&gt;                        &lt;chr&gt;\n##  1 package            loggit                       futile.logger\n##  2 published          2018-04-09                   2016-07-10\n##  3 title              Effortless Exception Logging A Logging Utility for R\n##  4 depends_count      1                            1\n##  5 suggests_count     3                            2\n##  6 tidyverse_happy    \" TRUE\"                      FALSE\n##  7 has_vignette_build FALSE                        FALSE\n##  8 has_tests          TRUE                         TRUE\n##  9 reverse_count      \" 0\"                         27\n## 10 dl_last_month      \"  250\"                      37313\n## 11 ci                 Travis                       &lt;NA&gt;\n## 12 test_coverage      NONE                         &lt;NA&gt;\n## 13 forks              \" 0\"                         &lt;NA&gt;\n## 14 stars              15                           &lt;NA&gt;\n## 15 watchers           \" 5\"                         &lt;NA&gt;\n## 16 last_commit        &lt;NA&gt;                         &lt;NA&gt;\n## 17 last_issue_closed  8.4                          &lt;NA&gt;\n## 18 contributors       \" 3\"                         &lt;NA&gt;\nYou can see from the table that the last publication date for loggit is April 2018 whereas for futile.logger it is July 2016. It can be quite hard to gain much from this metric; whereas more recent updates can suggest active development and ongoing improvements to functionality, a lack of update may indicate that a package has reached a stable state.\nReverse dependencies, however, tells us a lot more; if a package has a high number of reverse dependencies, one might conclude that there is a higher degree of trust in it in the community, but also more importantly, that it has been exposed to a higher number of users and therefore be less likely to have undiscovered bug. This is also true for the number of downloads.\nThe second half of the values in the table are blank, implying that futile.logger isn’t on GitHub. For me, if this were true, it’d be a cause for concern as I’d like to be able to see the number of open issues and how recent updates were. However, a quick Google search tells me that futile.logger is in fact on GitHub, but just not linked to in its CRAN page.\nReading GitHub issues pages is fairly simple. Rather than read every issue in detail, I try to get a rough idea of what’s going on, by skimming through. Typically a large number of issues tend to stem from user questions and enhancement requests, so just looking at the raw numbers isn’t enough.\n\nLooking through the open issues for futile.logger, the package authors has already flagged a couple of items as bugs, reducing my work. They appear to be do with namespacing issues, so certainly something to be aware of although not necessarily important.\nGoing to the equivalent page for loggit, I see that there is only one ticket open, but has been flagged as an enhancement. However, fewer issues is not necessarily a sign of package quality, as it cannot be decoupled from package popularity.\n\nAt this point, I could go on, and explore the source code for each package, but at this point I feel it’d lead to diminishing returns. The conclusion I draw and pass on to my colleague is that both packages seem to be functional with no glaring issues. As I’ve used futile.logger before, and it seems to be more widely used, I’d lean towards that one. My colleague thanks me for my advice and goes with loggit anyway because he’s used it before on a project and it worked pretty well there, which is fair enough.\nPackage metrics can be a useful source of information about a package, but ultimately, there’s no substitute for experience. Other logging packages are available; these were merely the ones I’d heard of."
  },
  {
    "objectID": "posts/error-chaining/index.html",
    "href": "posts/error-chaining/index.html",
    "title": "Error chaining",
    "section": "",
    "text": "In this post, I’m going to talk about error chaining - overriding default error messages to add further hints for a user. I had a need to learn this while working on Arrow on code which resulted in a C++ error message, to which I wanted to add extra hints relevant to R users. I’ve used a toy example below to make it more straightforward to demonstrate."
  },
  {
    "objectID": "posts/error-chaining/index.html#introduction",
    "href": "posts/error-chaining/index.html#introduction",
    "title": "Error chaining",
    "section": "Introduction",
    "text": "Introduction\nLet’s imagine I work in the HR department of the UK’s number 1 employer of cats. Our employees are available for modelling, snuggles, and avian assassination. Our individual offices keep records of new employees, and send them to me via email so I can collate them. Our process could be better, but it works for now."
  },
  {
    "objectID": "posts/error-chaining/index.html#basic-data-validation",
    "href": "posts/error-chaining/index.html#basic-data-validation",
    "title": "Error chaining",
    "section": "Basic data validation",
    "text": "Basic data validation\nWhen I get a new set of records, I want to do some analysis, and because I’m awesome, I want to do it in R. Unfortunately our regional office input data manually, which is subject to human error. I’ve decided I want to write some R code which automates data import and validation for me.\nI know that Dave in the Birmingham office often inputs the cat’s ages wrong - too little caffeine in the mornings really throws him off his game - but given the fact that the world’s oldest cat was 38, we can safely say that if the data shows an age older than that it’s either an input error or cause to call the Guinness Book of World Records.\nHere’s my initial function with import and basic validation!\nlibrary(readr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(rlang)\n\n# function with basic validation\nimport_cat_data &lt;- function(file){\n\n  # Column types are always the same so I may as well specify them here\n  data &lt;- readr::read_csv(\n    file,\n    col_types = cols(\n      col_double(),\n      col_integer(),\n      col_double()\n    )\n  )\n\n  # Validation\n  if (any(data$age &gt; 38)) {\n    rlang::abort(\n      c(\n        \"Values in `age` must be 38 or less\",\n        paste(\"Invalid values detected:\", paste0(data$age[data$age &gt; 38], collapse = \", \"))\n      )\n    )\n  }\n\n  data\n}\nSo how does this look if I have perfectly good data?\n# Create some example data and write to a CSV dile\ncats &lt;- tibble::tibble(age = c(5, 0.5, 13), paws = rep(4, 3), teeth = c(30, 26, 26))\nreadr::write_csv(cats, file = \"cats.csv\")\n\n# Import data\nimport_cat_data(\"cats.csv\")\n## # A tibble: 3 x 3\n##     age  paws teeth\n##   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1   5       4    30\n## 2   0.5     4    26\n## 3  13       4    26\nAnd how does this look when I run it on Dave’s dodgy data?\ndave_data &lt;- tibble::tibble(age = c(10, 200), paws = c(3, 4), teeth = c(30, 30))\nreadr::write_csv(dave_data, file = \"dave_data.csv\")\n\n# Triggers the error\nimport_cat_data(\"dave_data.csv\")\n## Error in `import_cat_data()`:\n## ! Values in `age` must be 38 or less\n## * Invalid values detected: 200"
  },
  {
    "objectID": "posts/error-chaining/index.html#custom-warning-handling",
    "href": "posts/error-chaining/index.html#custom-warning-handling",
    "title": "Error chaining",
    "section": "Custom warning handling",
    "text": "Custom warning handling\nOK, so we’ve covered Dave’s dodgy data, but what other problems do I have in my pipeline? Sometimes Anja in our Wigan branch will send me .tsv. files instead of .csv. I’m not interested in detecting the file type - I just want an error message. So, what does it look like when I try to import that data?\n# Set up tsv file saved as a csv\neven_more_cats &lt;- tibble::tibble(age = c(7, 3), paws = c(4, 4), teeth = c(30, 28))\ncats_tsv &lt;- readr::write_tsv(even_more_cats, file = \"even_more_cats.csv\")\n\n# Import the file\nimport_cat_data(\"even_more_cats.csv\")\n## Warning: Unnamed `col_types` should have the same length as `col_names`. Using\n## smaller of the two.\n## Warning: 2 parsing failures.\n## row              col               expected   actual                 file\n##   1 age  paws    teeth no trailing characters 7  4   30 'even_more_cats.csv'\n##   2 age  paws    teeth no trailing characters 3  4   28 'even_more_cats.csv'\n## Warning: Unknown or uninitialised column: `age`.\n## # A tibble: 2 x 1\n##   `age\\tpaws\\tteeth`\n##                &lt;dbl&gt;\n## 1                 NA\n## 2                 NA\nUrgh, this is kinda messy. I keep forgetting that it’s Anja’s dodgy tsv files which cause this, so I want to do 2 things here:\n\nPromote the warning to an error\nGive myself a little reminder about the cause of the error\n\nI’ve now wrapped my data import stage in a tryCatch() so I can provide some custom behaviour if this warning appears, via another function I’ve written called handle_cats_import_warning().\nimport_cat_data &lt;- function(file){\n\n  tryCatch(\n    data &lt;- readr::read_csv(\n      file,\n      col_types = cols(\n        col_double(),\n        col_integer(),\n        col_double()\n      )\n    ),\n    warning = function(w){\n      handle_cats_import_warning(w)\n    }\n  )\n\n  if (any(data$age &gt; 38)) {\n    rlang::abort(\n      c(\n        \"Values in `age` must be 38 or less\",\n        paste(\"Invalid values detected:\", paste0(data$age[data$age &gt; 38], collapse = \", \"))\n      )\n    )\n  }\n\n  data\n}\nThe warning helper is below. Basically, it extracts the message from the warning, and if this message matches the one I saw above, I append it with an extra little informational message as a hint about the possible cause. Then I call rlang::abort() to raise an error containing the content of the message. Remember, even if the warning is caused by something else, I still want it to error.\nhandle_cats_import_warning &lt;- function(w){\n  msg &lt;- conditionMessage(w)\n  if (grepl(\"Unnamed `col_types` should have the same length as `col_names`.\", msg)) {\n    msg &lt;- c(\n        msg,\n        i = \"Is the file you're importing a `.tsv`? Only `.csv' files are accepted.\"\n    )\n  }\n  rlang::abort(msg)\n}\nSo how does this look now?\nimport_cat_data(\"even_more_cats.csv\")\n## Error in `handle_cats_import_warning()`:\n## ! Unnamed `col_types` should have the same length as `col_names`. Using smaller of the two.\n## i Is the file you're importing a `.tsv`? Only `.csv' files are accepted.\nIt looks OK, but it’s not done yet - you might have noticed that the error is reported as coming from handle_cats_import_warning(). This isn’t quite right - that function is just the warning helper; I want to report the error as coming from import_cat_data(). So how do I do this? Let’s take a quick look at the source of the error - we’ll come back to this later.\nrlang::last_error()\n## &lt;error/rlang_error&gt;\n## Error in `handle_cats_import_warning()`:\n## ! Unnamed `col_types` should have the same length as `col_names`. Using smaller of the two.\n## i Is the file you're importing a `.tsv`? Only `.csv' files are accepted.\n## Backtrace:\n##  1. global import_cat_data(\"even_more_cats.csv\")\n##  2. base::tryCatch(...)\n##  3. base tryCatchList(expr, classes, parentenv, handlers)\n##  4. base tryCatchOne(expr, names, parentenv, handlers[[1L]])\n##  5. value[[3L]](cond)\n##  6. global handle_cats_import_warning(w)\n## Run `rlang::last_trace()` to see the full context.\nWe can see from the enumerated items on the backtrace that the function we want to show as the source of the error (import_cat_data()) is 5 items higher that the function currently being shown as the source of the error (handle_cats_import_warning()). So how do we change the reported error source?"
  },
  {
    "objectID": "posts/error-chaining/index.html#changing-the-calling-environment",
    "href": "posts/error-chaining/index.html#changing-the-calling-environment",
    "title": "Error chaining",
    "section": "Changing the calling environment",
    "text": "Changing the calling environment\nIt basically comes down to using the call parameter when calling rlang::abort(). If you take a look at the docs, you’ll see it documented as:\n\nThe execution environment of a currently running function, e.g. call = caller_env(). The corresponding function call is retrieved and mentioned in error messages as the source of the error.\n\n\nYou only need to supply call when throwing a condition from a helper function which wouldn’t be relevant to mention in the message.\n\nOK, perfect! So all I need to do is add another parameter to my warning helper function below - this means that wherever I’m calling it from, I can pass in information about the correct environment to report in the error message.\nhandle_cats_import_warning &lt;- function(w, call){\n  msg &lt;- conditionMessage(w)\n  if (grepl(\"Unnamed `col_types` should have the same length as `col_names`.\", msg)) {\n    msg &lt;- c(\n        msg,\n        i = \"Is the file you're importing a `.tsv`? Only `.csv' files are accepted.\"\n    )\n  }\n  rlang::abort(msg, call = call)\n}\nNext, I need to update my call to tryCatch() to also incorporate this change, and I use rlang::caller_env() to specify the environment. The parameter n = 4 means “go back 4 callers”.\nimport_cat_data &lt;- function(file){\n\n  tryCatch(\n    data &lt;- readr::read_csv(\n      file,\n      col_types = cols(\n        col_double(),\n        col_integer(),\n        col_double()\n      )\n    ),\n    warning = function(w, call = caller_env(n = 4)){\n      handle_cats_import_warning(w, call)\n    }\n  )\n\n  # Validation\n\n  if (any(data$age &gt; 38)) {\n    rlang::abort(\n      c(\n        \"Values in `age` must be 38 or less\",\n        paste(\"Invalid values detected:\", paste0(data$age[data$age &gt; 38], collapse = \", \"))\n      )\n    )\n  }\n\n  data\n}\nI discovered this number through trial and error, but after reading a bit more about the stack trace, I have a better idea about the reason Remember before when I said the function call we wanted to say was the error’s source was five items higher on the stack? In the warning handler above, we call caller_env() one place higher in the stack than handle_cats_import_warning(), and so 5 - 1 = 4.\nSo now let’s take a look - is our error message attributed to the right place in the call stack?\nimport_cat_data(\"even_more_cats.csv\")\n## Error in `import_cat_data()`:\n## ! Unnamed `col_types` should have the same length as `col_names`. Using smaller of the two.\n## i Is the file you're importing a `.tsv`? Only `.csv' files are accepted.\nYes, it is!\n\nIt was super interesting figuring out the details of this, and if you need to write code which involves error helpers and/or error chaining, I’d recommend that you check out this excellent rlang vignette on error chaining which covers even more things you can do around this."
  },
  {
    "objectID": "posts/rwatcher/index.html",
    "href": "posts/rwatcher/index.html",
    "title": "Monitoring R Package Updates with Github Actions",
    "section": "",
    "text": "As maintainer of the Arrow R package, there are a few packages I want to keep up to date with, so I can make sure that our bindings continue to be compatible with the latest versions of these packages. The packages I’m most interested in here are:\nI’m also interested in knowing when packages that folks use for vaguely similar purposes have been updated; I like to be up-to-date on this as sometimes people ask me about how things compare.\nPreviously, I’d occasionally caught glimpses of things via social media, but I wanted a more methodical approach, and so decided to write a GitHub Actions CRON job that does this. Now, when any of the packages on my list is updated, I receive an email that looks a little like this:\nIn this blog post, I’m going to walk through how I created this repository, and how you can do the same for your own packages."
  },
  {
    "objectID": "posts/rwatcher/index.html#how-it-all-works",
    "href": "posts/rwatcher/index.html#how-it-all-works",
    "title": "Monitoring R Package Updates with Github Actions",
    "section": "How it all works",
    "text": "How it all works\nThe repo itself it pretty simple in structure - it contains the GitHub Actions workflow, and a folder containing the changelogs for the packages I’m interested in.\n# tree\n.github\n└── workflows\n    └── compare_hashes.yml\nchangelogs\n├── data.table-NEWS.md\n├── dbplyr-NEWS.md\n├── dplyr-NEWS.md\n├── dtplyr-NEWS.md\n├── duckdb-NEWS.md\n├── duckplyr-NEWS.md\n├── lubridate-NEWS.md\n├── r-polars-NEWS.md\n├── stringr-NEWS.md\n└── tidypolars-NEWS.md\nThe workflow is triggered every day at 1am UTC, and runs a script that compares the hashes of the changelogs in my repo with the hashes of the changelogs in the package repos. If there’s a difference, the new changelog is saved to my repo and it sends me an email."
  },
  {
    "objectID": "posts/rwatcher/index.html#the-github-actions-workflow",
    "href": "posts/rwatcher/index.html#the-github-actions-workflow",
    "title": "Monitoring R Package Updates with Github Actions",
    "section": "The GitHub Actions Workflow",
    "text": "The GitHub Actions Workflow\nIn the next sections, I’ll walk through the GitHub Actions workflow, step by step, explaining what each bit does.\n\nScheduling\nThe start of the workflow looks like this:\nname: Check for package updates\n\non:\n  schedule:\n    # * is a special character in YAML so you have to quote this string\n    - cron:  '00 1 * * *'\nIt has a name, and the schedule is set to run every day at 1am UTC. The cron syntax is a bit weird, but there’s a handy crontab guru that can help you figure out what you need to put in here.\n\n\nJobs\nNext, we set up the different jobs we want to run. I want 1 job to run for each package.\nI used GitHub Copilot to help me with some of the syntax here; it was fantastic when I just added a comment describing what I’d like to be added, and then it filled it in. This wasn’t a perfect process as you have to know what to ask, and setting up the list of packages to work with was tricky, as I didn’t quite have the understanding of how matrices (which can run code in parallel) interacted with arrays (for specifying multiple inputs for each parallel job) as I hadn’t used this before. A bit of googling and a skim of StackOverflow and I got there pretty quickly though.\njobs:\n  compare-hashes:\n    name: ${{matrix.package.name}} updates\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    strategy:\n        matrix:\n          package: \n            [\n                { name: dbplyr, file: dbplyr-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/dbplyr/main/NEWS.md' }, \n                { name: lubridate, file: lubridate-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/lubridate/main/NEWS.md' },\n                { name: dplyr, file: dplyr-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/dplyr/main/NEWS.md'}, \n                { name: data.table, file: data.table-NEWS.md, url: 'https://raw.githubusercontent.com/Rdatatable/data.table/master/NEWS.md'},\n                { name: dtplyr, file: dtplyr-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/dtplyr/main/NEWS.md'},\n                { name: duckdb-r, file: duckdb-r-NEWS.md, url: 'https://raw.githubusercontent.com/duckdb/duckdb-r/main/NEWS.md'},\n                { name: r-polars, file: r-polars-NEWS.md, url: 'https://raw.githubusercontent.com/pola-rs/r-polars/main/NEWS.md'},\n                { name: stringr, file: stringr-NEWS.md, url: 'https://raw.githubusercontent.com/tidyverse/stringr/main/NEWS.md'},\n                { name: duckplyr, file: duckplyr-NEWS.md, url: 'https://raw.githubusercontent.com/duckdblabs/duckplyr/main/NEWS.md'},\n                { name: tidypolars, file: tidypolars-NEWS.md, url: 'https://raw.githubusercontent.com/etiennebacher/tidypolars/main/NEWS.md'},\n            ]\nThe runs-on specifies the operating system to run the job on, and the permissions section allows the job to write to the repo. The strategy section is where we set up the matrix of packages to work with. Each package has a name, a file name, and a URL to the changelog. The name of the job is set to the name of the package.\n\n\nSteps\nNext, we set up the steps that we want to run. The first step is to check out the repo we are working in, and get the hash of the relevant changelog file I have stored in my repo. This is saved to the GITHUB_OUTPUT environment variable, which is a file that is shared between all the steps in the job.\n    steps:\n        - name: Checkout code\n          uses: actions/checkout@v2\n        - name: Get local file hash\n          id: local-hash\n          run: echo \"local_hash=$(md5sum changelogs/${{ matrix.package.file }} | cut -d ' ' -f 1)\" &gt;&gt; $GITHUB_OUTPUT\nNext, I want to get the hash of the latest version of the package’s changelog file. I do this by downloading the file, and then getting the hash of the downloaded file. This is also saved to the GITHUB_OUTPUT environment variable.\n        - name: Get remote file\n          id: remote-file\n          run: |\n                  mkdir tmp\n                  curl -s ${{ matrix.package.url }} &gt; tmp/${{ matrix.package.file }}#\n        - name: Get remote file hash\n          id: remote-hash\n          run: echo \"remote_hash=$(md5sum tmp/${{ matrix.package.file }} | cut -d ' ' -f 1)\" &gt;&gt; $GITHUB_OUTPUT\nFinally, I want to compare the hashes of the two files. If they’re different, I want to update the changelog in my repo. I do this by setting up a conditional step that only runs if the hashes are different. In this case, I’m setting the git config, copying the new changelog to my repo, and then committing and pushing the changes.\n        - name: Update changed files\n          if: ${{ steps.local-hash.outputs.local_hash != steps.remote-hash.outputs.remote_hash }}\n          run: |\n            echo \"Hashes do not match!\"   \n            git config --global user.email \"github-actions[bot]@users.noreply.github.com\"\n            git config --global user.name \"GHA Bot\"\n            cp tmp/${{ matrix.package.file }} changelogs/${{ matrix.package.file }}\n            git add changelogs/${{ matrix.package.file }}\n            git pull --ff-only\n            git commit -m \"Update ${{matrix.package.name}} changelog\"\n            git push\nFinally, I want to send an email notification if the changelog has been updated. I do this by setting up a conditional step that only runs if the hashes are different. I use the dawidd6/action-send-mail action to send the email. I set up a few secrets in my repo to store the email address and password, and then use those in the action. I also set up the subject and body of the email using the package name and URL.\nThe username and password are not my actual email address and password; instead, you can set up an app password for your email account, and use that instead, which is more secure.\n        - name: Send email notification\n          if: ${{ steps.local-hash.outputs.local_hash != steps.remote-hash.outputs.remote_hash }}\n          uses: dawidd6/action-send-mail@v3\n          with:\n            server_address: smtp.gmail.com\n            server_port: 465\n            username: ${{ secrets.MAIL_USERNAME }}\n            password: ${{ secrets.MAIL_PASSWORD }}\n            subject: \"${{matrix.package.name}} update\"\n            body: \"${{matrix.package.name}} has been updated! Please check the changelog at ${{matrix.package.url}}.\"\n            to: ${{ secrets.MAIL_RECIPIENT }}\n            from: ${{ secrets.MAIL_USERNAME}}\nAnd that’s it! The full repository can be found here."
  },
  {
    "objectID": "posts/rwatcher/index.html#conclusion",
    "href": "posts/rwatcher/index.html#conclusion",
    "title": "Monitoring R Package Updates with Github Actions",
    "section": "Conclusion",
    "text": "Conclusion\nI really enjoyed working on this and learning more about GitHub Actions. This has proved to be a useful tool, though there are a few improvements that could be made:\n\nsome packages update their changelog more frequently than others and so some of the updates I get feel a bit spammy. I could fix this by running my CRON job on a weekly rather than daily schedule.\nI don’t use this as much as I anticipated because some changes are really minor, and I tend to skim them and not pay too much attention. Again, a different CRON frequency could probably help here.\nI’m more interested in some packages than others. {dplyr}, {lubridate}, and {stringr} are the most important, whereas others are just a “nice to have” here. I could separate these out into different jobs, and run them on different schedules.\n\nAnyway, I’d love to hear your thoughts - how do you keep up to date with changes in R packages? Do you have any suggestions for improvements to this workflow? Let me know! Get in touch via Mastodon or LinkedIn."
  }
]