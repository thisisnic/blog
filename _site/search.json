[
  {
    "objectID": "posts/goats/index.html",
    "href": "posts/goats/index.html",
    "title": "Three Goats in a Raincoat",
    "section": "",
    "text": "I am not a fan of the term “imposter syndrome.” It’s a lazy way of ignoring the complex and interconnected, often structural reasons that contribute to many people’s feelings, which manifests itself in a variety of deeply idiosyncratic ways. But I guess the tl;dr is that this is what this post is about.\nI am not someone who has “hobbies” as such; I have interests that I adopt with great enthusiasm, pursue obsessively for a short period of time, often declaring that I have found a lifelong fascination, and then drop once the initial feeling of “endless possibility” has worn off and something else shiny and new finds it way across my path. I suppose my hobby is “pursuing novelty.” The 2022-2023 edition of this came in the form of tabletop role-playing games, and earlier this year, I found myself sitting with some good friends, beers and snacks, working out how my character, “Ach!”, a small goat with a fondness for fainting at inappropriate moments could make their way into a human party undetected.\nThe solution for Ach! and friends was to don a disguise and stand on each others’ shoulders, and this image of 3 goats in a raincoat trying to sneak into a human party perfectly reflects how I feel about myself sometimes as an open source maintainer.\n(Here, I need to get round to commissioning an image of 3 goats in a raincoat standing on each other’s shoulders. The top one has black and red braids and is wearing a hat and fake beard/moustache)\nI am the current maintainer of the arrow R package, which in reality just means I am the person who is listed as such in the package DESCRIPTION file, and have the responsibility of clicking the link in the confirmation email from CRAN when someone submits the latest version of of the package to CRAN. Of course, I do more than just this; I triage bugs, submit patches, review PRs, and occasionally implement new features, but so do other people. While for career-serving purposes, I can technically claim to be “the” maintainer of the package, honestly, I am “a” maintainer and part of a team. This isn’t always reflected in other people’s language, and results in weird moments where people say things to me like, “ah, you wrote arrow!” and I have to respond with, “I absolutely did not, though I did write a lot of the bindings for dplyr::across!”, a topic which subsequently interests me more than it does them."
  },
  {
    "objectID": "posts/goats/index.html#the-tyranny-of-should",
    "href": "posts/goats/index.html#the-tyranny-of-should",
    "title": "Three Goats in a Raincoat",
    "section": "The tyranny of “should”",
    "text": "The tyranny of “should”\nI’ve spent much of this year suffering from the tyranny of “should” regarding my own knowledge and abilities, in a way that has been verging on ridiculous. A lot of it is the perfect storm of adjusting to new responsibility, the deeply unhelpful way in which my self-image when I was younger was built up around being “smart” and “knowing things”, and a knack for punching above my weight by being good at figuring out just enough about a topic to get shit done. I don’t know every single detail of object orientation in R, but I know enough to be able to create new bindings between C++ objects and R6 classes in the arrow R package and expose their methods via fields and active bindings. Despite having looked it up repeatedly over the past few years, I still cannot recall the difference between how things work with static and shared libraries, but I did manage to fix a bug requiring me to bump the version of the C++ date library that Arrow vendors, via inspecting old pull requests and helpful feedback I received on the PR I submitted.\nIt came to a head at this year’s posit::conf, where I had the privilege of co-teaching a workshop about Arrow with the wonderful Stephanie Hazlitt. We spent months preparing materials and practicing teaching to our laptop screens, but when the event itself rolled around, teaching it to a room just shy of 60 people was a different experience entirely. It was intimidating being in front of a room of eager learners, and any jokey asides I was usually capable of making went out of the window, and I just taught the materials fairly plainly, stumbling over my words a bit, and finding the moments of silence in the room as I was teaching pretty awful. Subsequent feedback from folks I trust enough to deliver at least some honesty alongside their validation has led me to conclude that the only person who really noticed or cared about this was me. I was doing my old trick of snatching defeat from the jaws of victory, and realised later that my self-expectations were totally unreasonable. While I used to be able to absolutely smash through delivering a well-practiced 2-day “introduction to R” course, and delighted in engaging the folks in the room and showing them what they (and R!) were capable of, it’s 5 years since I last did that on the regular. This was the first time these materials had seen the light of day, and at no less than posit::conf, daunting enough on its own. And honestly, this is all ego. We ran a good workshop, people were engaged, and we wrote some quality reusable materials. Yes, there’s room for improvement, but overall it was solid. Delivery of content is something I can (and will) work on. I’ve applied for The Carpentries instructor training, and have plans to get back into the swing of teaching by running workshops (on Arrow and other topics) at user events."
  },
  {
    "objectID": "posts/goats/index.html#the-power-of-i-dont-know-but-i-can-find-out",
    "href": "posts/goats/index.html#the-power-of-i-dont-know-but-i-can-find-out",
    "title": "Three Goats in a Raincoat",
    "section": "The power of “I don’t know but I can find out”",
    "text": "The power of “I don’t know but I can find out”\nThe second day of posit::conf was spent being a teaching assistant for the fantastic Andy Teucher, who was teaching “Fundamentals of Package Development”. I managed to escape a lot of this “should” nonsense here, so when one of the course attendees asked me about where to store CSV files to use in unit tests, I happily admitted I wasn’t sure but could find out, and together we had a look in “Writing R Packages”, where I couldn’t find the information, and then suggested the approach I usually take when working on arrow - look at what folks who often define best practices do in the packages that they maintain. We took a look at readr, found the files in the testthat directory, and concluded that unless there’s reason to want to do anything more complicated (like make those datasets available to end users), then that approach is fine. Not knowing the answer instantly wasn’t a hindrance to working out the necessary solution. Learning how to say “I don’t know, but let’s find out together” is one of the most important things I learned when I was first teaching people how to use R. A teacher doesn’t have to know everything, but should be able to help a learner find the information they need."
  },
  {
    "objectID": "posts/goats/index.html#the-power-of-community",
    "href": "posts/goats/index.html#the-power-of-community",
    "title": "Three Goats in a Raincoat",
    "section": "The power of community",
    "text": "The power of community\nThe rest of the conference week was pretty awesome. I was in the middle of a CRAN resubmission (well, actually a re-re-resubmission, but let’s not go there) which had got a bit hairy, and I needed some help tracking down the source of the bug. I spent some time briefly engaging in a bit of pair programming with Jonathan Keane and Neal Richardson, who’ve both been involved with Arrow for a lot longer than me, and I’m always taken aback by just how quickly they both get to the source of a problem any time I ask for help. Part of my “knowledge anxiety” manifests as me sometimes taking longer than I would like to process things when communicating verbally about technical topics, which leads to me getting really frustrated when I’m just not “getting” things in the moment that I know later will seem pretty obvious and uncomplicated. This happened when we were looking at the arrow bug, but I had this sudden moment of clarity upon realising…nobody cared except me. Jon and Neal are great collaborators and are always happy to explain things again if there’s time, and aren’t going to judge me for it.\nI also realised that there is no universal measure of what I should know or where I should be at. While I always want to know more about how things work, I don’t have to know everything and it’s a pretty unrealistic expectation for me to have of myself; sure I can (and often do!) go read a whole load of the codebase and related concepts, but that’ll never be the same as also leaning on other people’s years of experience and knowledge. The whole point of being part of a community is to be able to share both overlapping and divergent knowledge."
  },
  {
    "objectID": "posts/goats/index.html#progress-is-power",
    "href": "posts/goats/index.html#progress-is-power",
    "title": "Three Goats in a Raincoat",
    "section": "Progress is power",
    "text": "Progress is power\nI don’t think there is a universal solution to this problem of feeling like you “should” know more, but all I know is what has worked for me. Public work is important - social media, blog posts, talks, workshops, whatever. It can be scaled to wherever you’re at. There’s always something you’ll be surprised that you know but others don’t. And it creates a lovely feedback loop whereby a little bit of external validation can go a long way.\nLearning is also important - I did the first half of the Udacity C++ Nanodegree this year, and while a lot of what it taught me was that I never want to be a full-blown C++ developer (I’d rather make weird buildings and spaceships out of existing Lego pieces than become a polymer scientist just to create custom bricks), realising that I could learn C++ at that level if I really wanted to, was invaluable. Part of the course involved getting acquainted with cmake, and whilst I can’t claim expertise there, dabbling in a bit of the whats and whys helped make many related Arrow project issues seem less mystical.\nReassurance and validation are good and well, but in my experience, having tangible proof of the things that I do know is more effective."
  },
  {
    "objectID": "posts/goats/index.html#fake-it-til-you-make-it-is-deeply-problematic",
    "href": "posts/goats/index.html#fake-it-til-you-make-it-is-deeply-problematic",
    "title": "Three Goats in a Raincoat",
    "section": "“Fake it til you make it” is deeply problematic",
    "text": "“Fake it til you make it” is deeply problematic\nThe commonly received advice is “fake it ’til you make it”. Pretend you feel like you belong and are confident, until that becomes the case. I’ve spent a long time attempting this, and the problem is that it doesn’t actually work, because it doesn’t address the underlying issue. It’s only since I started to own the fact that I don’t feel comfortable in every environment or domain and working out what I need to do to feel more comfortable that I’ve felt my confidence growing.\nIn the past month, I’ve been a lot more open about how I’ve been feeling around this. It’s been tricky as I’ve been scared that my vulnerability just looks weakness that I shouldn’t show around other people, or just come off as moaning, but it’s had massive benefits. During the first couple of days of posit::conf this year, I suffered the worst ongoing anxiety I’ve had all year - I had a constant knot in my chest - but despite that managed to have a good time as everyone was very accepting. I can say without a doubt that pretending to be fine would have made things infinitely worse.\nThere was another point during the conference when I casually mentioning that I was feeling quite hopeful about a potential opportunity but not entirely sure if it was in the bag, as I suspected I might be up against someone whose background I find impressive. It was kindly pointed out to me that each of us bring different things to the table, a comment which has since set off a chain reaction of me starting to appreciate what I can do rather than what I can’t."
  },
  {
    "objectID": "posts/goats/index.html#the-cure-for-imposter-syndrome",
    "href": "posts/goats/index.html#the-cure-for-imposter-syndrome",
    "title": "Three Goats in a Raincoat",
    "section": "The cure for imposter syndrome",
    "text": "The cure for imposter syndrome\nThe most important thing I learned this year is that the cure for imposter syndrome isn’t persuading yourself that you aren’t just 3 goats in a raincoat pretending to be a person, but instead surrounding yourself with folks who wouldn’t really care if you were anyway because goats are cool and that’s a pretty awesome feat of acrobatics and balancing."
  },
  {
    "objectID": "posts/r-examples/index.html",
    "href": "posts/r-examples/index.html",
    "title": "R package documentation - what makes a good example?",
    "section": "",
    "text": "I’m currently working on adding to the documentation of the arrow R package, and I’ve started thinking about the qualities of good examples. Specifically, I’m referring to the examples included as part of function documentation. In my experience, the best way for me to achieve rapid familiarity with an R function I haven’t worked with before, or understand how to use a function about which I already understand the basics, is by having example code that I can run. In the book ‘R Packages’, Hadley Wickham remarks that examples are “a very important part of the documentation because many people look at the examples first” and rOpenSci recommend that each of a package’s exported functions should be accompanied by examples.\nIn this blog post, I will explore the things that I believe make for good function examples in documentation, focussing mainly on R."
  },
  {
    "objectID": "posts/r-examples/index.html#what-good-looks-like-in-r",
    "href": "posts/r-examples/index.html#what-good-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like in R",
    "text": "What good looks like in R\nI asked people on Twitter for their opinions of good R package documentation in general, and Jonathan Sinclair highlighted the ‘examples’ section from dplyr::case_when, the beginning of which is shown below.\n\n\n\n(image from: https://rdrr.io/cran/dplyr/man/case_when.html)\n\n\nI think Jonathan is spot on in his assessment. To paraphrase, the highlights for him are:\n\nthere is next to no prose or intro\nthere are plenty of comments, as needed, to explain the examples\nthere is a variety of different examples\nthere are examples of what not to do.\n\nThis kind of documentation appeals to my skim-reading self. If I’m trying to accomplish a task, sometimes I just want to run some code and see what happens to get an intuitive feel for what a function does. While I am fully prepared to slow down and read the rest of the documentation, a “quick win” motivates me to invest the additional effort. It tells me that the developers of this code have prioritised making things easy to understand and that the time I am investing will pay off.\nI’ve been skimming through the documentation of some tidyverse and related packages - as I consider many of these to be well documented and easy to read. Here are some things I’ve observed which I think one can do to make function examples look great:\n\ninclude the most basic usage of a function\nuse very simple toy datasets or standard in-built datasets\ndemonstrate non-obvious behaviours of a function\ndemonstrate different parameter values/combinations where relevant\ndemonstrate any unusual parameters\ndemonstrate on different objects if appropriate\nsometimes go beyond the use of an individual function to include common mini-workflows\ngroup documentation and examples for similar functions together\ninclude examples that may lead to unexpected results\ninclude comments to explain examples\nno examples for deprecated functions to discourage their use\nno unpredictable external dependencies - rvest::html_text manually creates HTML to demonstrate capabilities rather than scraping an external site\nsometimes showing the output when it adds to the example (e.g. tidyselect::starts_with() and many other examples from that package)\nexamples should be correct and run without error (unless intended to show erroneous output)"
  },
  {
    "objectID": "posts/r-examples/index.html#what-bad-looks-like-in-r",
    "href": "posts/r-examples/index.html#what-bad-looks-like-in-r",
    "title": "R package documentation - what makes a good example?",
    "section": "What bad looks like in R",
    "text": "What bad looks like in R\nI am not intending to “name and shame” any package authors who haven’t included examples for their functions. It may have been overlooked, there may be plenty of explanation elsewhere, or they may have felt that the code was not sufficiently complex to require examples. It might be true that it seems obvious what a function does, but that makes assumptions about the users of your code that might not hold."
  },
  {
    "objectID": "posts/r-examples/index.html#what-good-looks-like-generally",
    "href": "posts/r-examples/index.html#what-good-looks-like-generally",
    "title": "R package documentation - what makes a good example?",
    "section": "What good looks like generally",
    "text": "What good looks like generally\nWhen reading through examples, one thing that struck me is that when I’m looking at Python docs in Jupyter Notebook (press shift + tab), I also see the output of running the examples.\n\nSimilarly, both examples and outputs are shown in the official docs for some libraries, for example, pandas.\n\nI think this is a helpful feature - less effort is required to see how a function works.\nIn R function documentation, runnable code is often included, but in most cases needs to be manually run by the reader to see the output. I’m torn as to whether this is good or not. On the one hand, it encourages you to run the code and get a more tangible feel for what it does and saves valuable space in the Viewer window in RStudio. On the other hand, it adds an extra manual step to your workflow and lengthens the time until that precious “quick win” of enlightenment when exploring a new function.\nYou get a lot closer to this on the website rdrr.io, which indexes R package documentation and allows examples to be run inline. However, examples are run one after the other without the original code being displayed. So in the case of multiple examples, you have to match up the output to which example it is from.\n\n\n\n(from https://rdrr.io/cran/dplyr/man/case_when.html)\n\n\nSome packages include output as comments within their examples. For instance, the tidyselect package; here’s an example from tidyselect::all_of:\n\n\n\n(from https://rdrr.io/cran/tidyselect/man/all_of.html)\n\n\nAll that said, while the ability to see the output of examples is a nice-to-have, I don’t think it’s essential to good function documentation. With any piece of documentation, it’s necessary to consider the purpose; at a minimum, examples exist to tell the reader how to use a function, and you don’t need to see the output to do that.\nSince I first wrote this, I found out that it is possible to easily run examples from help files by selecting them and then hitting Ctrl+Enter, the same as running code in the Source pane.\n\nAnother thing I wasn’t aware of - pkgdown - commonly used to automatically render docs for packages run examples and displays the output underneath. Check out the example below from the Arrow pkgdown site.\n\nIn conclusion, good examples make functions easier to work with and help readers of your documentation gain a deeper understanding of how a function works. While any examples are better than no examples, you can give your users the best chance of success when using your code with careful thought about the content of your documentation.\nHuge thanks to everyone who responded to my Twitter thread, and to my fantastic colleague Joris Van den Bossche for reading the first draft of this, and our conversations about how things are done in R and Python."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks and Workshops",
    "section": "",
    "text": "Big Data in R with Arrow\n\n\n\n\n\n\nNic Crane\n\n\nSep 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficiently Engineering Bigger Data with Arrow\n\n\n\n\n\n\nNic Crane\n\n\nAug 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat they forgot to teach you about becoming an open source contributor\n\n\n\n\n\n\nNic Crane\n\n\nJul 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe future’s Shiny: Pioneering genomic medicine in R\n\n\n\n\n\n\nNic Crane\n\n\nJan 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTen Steps To Becoming A Tidyverse Contributor\n\n\n\n\n\n\nNic Crane\n\n\nNov 27, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/londonr/index.html",
    "href": "talks/londonr/index.html",
    "title": "Ten Steps To Becoming A Tidyverse Contributor",
    "section": "",
    "text": "A talk about getting involved in open source!\nSlides here"
  },
  {
    "objectID": "talks/positconf2023/index.html",
    "href": "talks/positconf2023/index.html",
    "title": "Big Data in R with Arrow",
    "section": "",
    "text": "A workshop on Arrow that Stephanie Hazlitt and I wrote and presented at posit::conf 2023.\nCourse homepage"
  },
  {
    "objectID": "talks/positconf2023/index.html#overview",
    "href": "talks/positconf2023/index.html#overview",
    "title": "Big Data in R with Arrow",
    "section": "Overview",
    "text": "Overview\nData analysis pipelines with larger-than-memory data are becoming more and more commonplace. In this workshop you will learn how to use Apache Arrow, a multi-language toolbox for working with larger-than-memory tabular data, to create seamless “big” data analysis pipelines with R.\nThe workshop will focus on using the the arrow R package—a mature R interface to Apache Arrow—to process larger-than-memory files and multi-file datasets with arrow using familiar dplyr syntax. You’ll learn to create and use interoperable data file formats like Parquet for efficient data storage and access, with data stored both on disk and in the cloud, and also how to exercise fine control over data types to avoid common large data pipeline problems. This workshop will provide a foundation for using Arrow, giving you access to a powerful suite of tools for performant analysis of larger-than-memory data in R.\nThis course is for you if you:\n\nwant to learn how to work with tabular data that is too large to fit in memory using existing R and tidyverse syntax implemented in Arrow\nwant to learn about Parquet and other file formats that are powerful alternatives to CSV files\nwant to learn how to engineer your tabular data storage for more performant access and analysis with Apache Arrow"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "PhD Thesis - Debiasing Reasoning: A Signal Detection Analysis\n\n\n\n\n\n\nNic Crane\n\n\nJul 22, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nic Crane, PhD",
    "section": "",
    "text": "Hi, I’m Nic! I’m a data scientist, software engineer, and R enthusiast, currently working at Voltron Data on Apache Arrow."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Debugging\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nNic Crane\n\n\n\n\n\n\n  \n\n\n\n\nThree Goats in a Raincoat\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nNic Crane\n\n\n\n\n\n\n  \n\n\n\n\nR package documentation - what makes a good example?\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\nNic Crane\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/phd/index.html",
    "href": "projects/phd/index.html",
    "title": "PhD Thesis - Debiasing Reasoning: A Signal Detection Analysis",
    "section": "",
    "text": "Download here: PDF, 2.1MB\nThis thesis focuses on deductive reasoning and how the belief bias effect can be reduced or ameliorated.\nBelief bias is a phenomenon whereby the evaluation of the logical validity of an argument is skewed by the degree to which the reasoner believes the conclusion. There has been little research examining ways of reducing such bias and whether there is some sort of effective intervention which makes people reason more on the basis of logic. Traditional analyses of this data has focussed on simple measures of accuracy, typically deducting the number of incorrect answers from the number of correct answers to give an accuracy score. However, recent theoretical developments have shown that this approach fails to separate reasoning biases and response biases. A reasoning bias, is one which affects individuals’ ability to discriminate between valid and invalid arguments, whereas a response bias is simply the individual’s tendency to give a particular answer, independent of reasoning. A Signal Detection Theory (SDT) approach is used to calculate measures of reasoning accuracy and response bias. These measures are then analysed using mixed effects models.\nChapter 1 gives a general introduction to the topic, and outlines the content of subsequent chapters. In Chapter 2, I review the psychological literature around belief bias, the growth of the use of SDT models, and approaches to reducing bias. Chapter 3 covers the methodology, and includes a a thorough description of the calculation of the SDT measures, and an explanation of the mixed effects models I used to analyse these. Chapter 4 presents an experiment in which the effects of feedback on reducing belief bias is examined. In Chapter 5, the focus shifts in the direction of individual differences, and looks at the effect of different instructions given to participants, and Chapter 6 examines the effects of both feedback and specific training. Chapter 7 provides a general discussion of the implications of the previous three chapters."
  },
  {
    "objectID": "talks/rstudioconf2022/index.html",
    "href": "talks/rstudioconf2022/index.html",
    "title": "What they forgot to teach you about becoming an open source contributor",
    "section": "",
    "text": "Getting involved in open source is an amazing learning experience and helps you grow your skills as a developer, but to a new contributor there are so many unknown factors about open source projects. In this talk, I’m going to discuss my journey from occasional open source contributor to full time project maintainer, and answer questions such as: what does it look like from the inside of an open-source project? What’s a good way to get involved, and really learn the internals of an R package? How can I navigate the social dynamics of an open source project? How do contributions look entirely different from the point of view of a contributor versus a maintainer?"
  },
  {
    "objectID": "talks/rstudioconf2019/index.html",
    "href": "talks/rstudioconf2019/index.html",
    "title": "The future’s Shiny: Pioneering genomic medicine in R",
    "section": "",
    "text": "Shiny's expanding capabilities are rapidly transforming how it is used in an enterprise. This talk details the creation of a large-scale application, supporting hundreds of concurrent users, making use of the future and promises packages. The 100,000 genomes project is an ambitious exercise that follows on from the Human Genome Project - aiming to put the UK at the forefront of genomic medicine, with the NHS as the first health service in the world to offer precision medicine to patients with rare diseases and cancer. Data is at the heart of this project; not only the outputs of the genomic sequencing, but vast amounts of metadata used to track progress against the 100,000 genome target and the status and path of each case through the sample tracking pipeline. In order to make this data readily available to stakeholders, Shiny was used to create an application containing multiple interactive dashboards. A scaled-up version of the app is being rolled out in early 2019 to a much larger audience to support the National Genomics Informatics Service, with the challenge of creating a complex app capable of supporting so many users without grinding to a halt. In this talk, I will explain why Shiny was the obvious technology choice for this task, and discuss the design decisions which enabled this project’s success."
  },
  {
    "objectID": "talks/nyhackr/index.html",
    "href": "talks/nyhackr/index.html",
    "title": "Efficiently Engineering Bigger Data with Arrow",
    "section": "",
    "text": "Data analysis pipelines with larger-than-memory data are becoming more and more commonplace. There are often blurred lines between data science and data engineering, and knowing a bit of both is a sure-fire way to make your life easier when working with big datasets. In this talk, I will give an overview of the arrow R package and best practices for getting the most out of your data when working with bigger datasets. I’ll demo the dplyr interface to arrow, and give you some tips and tricks for getting the most out of arrow’s functionality, as well as applying data engineering principles to speed things up even more."
  },
  {
    "objectID": "draft_posts/udfs/index.html",
    "href": "draft_posts/udfs/index.html",
    "title": "User-defined functions in arrow",
    "section": "",
    "text": "library(arrow)\nlibrary(dplyr)\n\nregister_scalar_function(\n  name = \"add_one\",\n  function(context, trip_distance) {\n    trip_distance + 1\n  },\n  in_type = schema(\n    trip_distance = float64()\n  ),\n  out_type = float64(),\n  auto_convert = TRUE\n)\n\n\nresults &lt;- bench::mark(\n  compute = open_dataset(\"~/data/nyc-taxi\") |&gt;\n    filter(year == 2019, month == 9) |&gt;\n    transmute(x = trip_distance + 1) |&gt;\n    collect(),\n  udf = open_dataset(\"~/data/nyc-taxi\") |&gt;\n    filter(year == 2019, month == 9) |&gt;\n    transmute(x = add_one(trip_distance)) |&gt;\n    collect(),\n  iterations = 10,\n  check = FALSE\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n\nresults\n\n# A tibble: 2 × 13\n  expression min        median     `itr/sec` mem_alloc  `gc/sec` n_itr  n_gc\n  &lt;bnch_xpr&gt; &lt;bench_tm&gt; &lt;bench_tm&gt;     &lt;dbl&gt; &lt;bnch_byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 &lt;language&gt; 0.6321745  0.6625475      1.49   6372712      0.744    10     5\n2 &lt;language&gt; 1.3555642  1.4408504      0.690 55933824      7.11     10   103\n# ℹ 5 more variables: total_time &lt;bench_tm&gt;, result &lt;list&gt;, memory &lt;list&gt;,\n#   time &lt;list&gt;, gc &lt;list&gt;\n\n\nThe time it took when using Arrow’s in-built compute function was about hald the time it took using a UDF. Crucially, there was significantly more memory allocated by R when using the UDF, as well as more garbage collections performed, which leads me to conclude that the UDF is being run after the results have been pulled back into R."
  },
  {
    "objectID": "posts/debugging/index.html",
    "href": "posts/debugging/index.html",
    "title": "Debugging",
    "section": "",
    "text": "As a package maintainer, I’m constantly disappointed when folks mention Arrow bugs they’re aware of but haven’t reported. Not disappointed with the individual in question, but disappointed with the fact that we’re not at the point where we’ve created an environment in which folks are happy to just report bugs immediately. This is not an Arrow-specific problem, and I find myself behaving in exactly the same way with other open source projects. If I’m not entirely sure something is a bug, I’m not going to risk looking foolish publicly by complaining, but the irony of this is that I don’t judge users who make those mistakes with Arrow, as usually it means we need to improve our docs to be more clear, and that in itself is valuable feedback.\nI really love interacting with people who use the package, as without that interaction, development and maintenance can feel like shouting into the void. I like being able to solve problems that make other people’s lives easier, and I thrive off that social energy. I implemented bindings for dplyr::across() because someone commented on Twitter that they’d love to see it as a feature. Last night I got home from a friend’s birthday drinks, saw a user question on Stack Overflow which had an easy win, and within an hour had a pull request up which implemented a new function and fixed the particular issue. I am not promising this level of responsiveness in perpetuity, but I’m still at the point where I find this kind of thing exciting and energising.\nOne particular bug which has haunted me for the past 6 months, is a particularly irritating one whereby when working with a partitioned CSV dataset in arrow (I’m using lowercase to denote the R package, rather than Arrow the overall project), if the user manually specified a schema, the variable used for partitioning did not appear in the resulting dataset. This is a huge problem IMO - while we can sit here all day talking about the virtues of Parquet, in reality, a lot of our users will be using CSVs, and it’s issues like these that can rule out being able to use arrow at all in some cases.\nWhen I opened the original issue based on another user issue, I knew it was important, but felt a bit stuck. It wasn’t immediately obvious to anyone what the source of the error was. I’d assumed it must be a C++ error and flagged it as such, but nobody had taken a look at it, and I’m always hesitant to mindlessly tag folks on issues when I don’t feel like I’ve done the work to investigate (though to be fair, didn’t really know what “the work” should be in this case).\nI’d ended up assuming that this bit of functionality just didn’t work with CSV datasets, and had been working around it, until I was presenting about arrow at New York Open Statistical Programming Meetup, and someone asked about it again. I take 1 user question as representative of 99 other people with the same issue who aren’t being so vocal about it, and felt like it needed to be fixed. I am unashamed to admit that I occasionally have the taste for a bit of melodrama, and publicly declared it to a few of my fellow contributors as “my white whale”, and so set out to find the source, even if it required me to delve deep into the guts of Arrow’s C++ libraries, a task which can often send me down endless rabbit holes and chasing red herrings (this sentence has become quite the nature park…)\nMy original exploration didn’t result in much useful - the arrow package does some cool things with R6 objects to binding them to Arrow C++ objects, but accessing the inner properties of these bound objects would mean manually creating print methods for every single one of them, and when you don’t know in which class the problem lies, this becomes, frankly, a massive pain in the arse. I still didn’t have enough to go on to take it to an Arrow C++ contributor and ask for their help, but showing I’d done some of “the work” to at least make an effort myself.\nAnd then collaborative debugging saved me! I had a catchup with the fantastic Dane Pitkin, and I asked for his help just walking through the problem. Dane’s main contributions to Arrow have been to Python, though he has a ton of previous C++ experience, even if he isn’t a regular contributor to the Arrow C++ library. I walked through the problem with him, and the steps I’d taken so far to try to figure things out, and the fact that I still needed to figure out if the problem was in R or C++. Dane commented that the object bindings we’d been looking at had little surface area for the problem to be in R - most of them were straight-up mappings from the C++ object to an R6 object with no extension. This was my first big clue! I remembered that there’s a bit of open_dataset() where we do some manual reconfiguration of specified options, which involves a whole load of R code - something I’ll come back to later. Dane also suggested I check out Stack Overflow to see if people were complaining about the issue in C++ there. I was sceptical that I’d find anything - lots of these bugs are more often surfaced in the R and Python libraries - but realised that this wasn’t the dead end that I’d thought. It suddenly occurred to me that if I could reproduce the bug in PyArrow, then the problem must lie in the C++ library, but if I couldn’t, then the problem lay in the R code.\nFifteen minutes later, and I had confirmed it was an R problem. I happened to mention on Slack the problem I was having, steps I’d taken so far to investigate, and potential ideas to look at next, and ended up engaging in a bit more collaborative debugging, this time with the wonderful Dewey Dunnington, who mentioned more disparities between PyArrow and R in terms of how we construct schemas, which put me on the path of testing the schema values at different points in Dataset creation and able to rule that out. At that point, with a smaller problem space to explore, the only logical thing left to look into was the R code which sets up the options for the various Arrow classes, and I ended up spotting the rogue instantiation of CSVReadOptions which just needed to have the partitioning column excluded (it relates to the reading in of the individual files which make up the dataset, and so has no “knowledge” of the partitions, and so previously raised an error as it treated them as an extraneous column).\nOne pull request later, and the problem that I’d given myself a week to look at had been solved in less than a day! This is probably one of the most gratifying bugs I’d worked on all year; there was a user with a problem to solve, a bug which had been annoying me for ages, the chance to fall into the puzzle-like aspects of debugging, and some great opportunities for collaboration with folks whose help here I really appreciated. This is one of the things I enjoy most about being a software engineer; this process of starting off feeling entirely clueless about something, and having to work out where I need to be and how I’m going to get there, and then doing it. Actually, in the abstract, that’s probably one of the things I enjoy most about being a human :)"
  }
]